{
  "config": {
    "id": "36a3b0b5-bad0-4a04-b79b-441c7cef77db",
    "label": "BetterBibTeX JSON",
    "preferences": {
      "ascii": "",
      "asciiBibLaTeX": false,
      "asciiBibTeX": true,
      "autoAbbrev": false,
      "autoAbbrevStyle": "",
      "autoExport": "immediate",
      "autoExportDelay": 2,
      "autoExportIdleWait": 10,
      "autoExportPathReplaceDiacritics": false,
      "autoExportPathReplaceDirSep": "-",
      "autoExportPathReplaceSpace": " ",
      "automaticTags": true,
      "autoPinDelay": 0,
      "auxImport": false,
      "baseAttachmentPath": "",
      "biblatexExtendedDateFormat": true,
      "biblatexExtendedNameFormat": true,
      "biblatexExtractEprint": true,
      "bibtexParticleNoOp": false,
      "bibtexURL": "off",
      "cache": true,
      "cacheFlushInterval": 5,
      "charmap": "",
      "citeCommand": "cite",
      "citekeyFold": true,
      "citekeyFormat": "auth.lower + shorttitle(3,3) + year",
      "citekeySearch": true,
      "citekeyUnsafeChars": "\\\"#%'(),={}~",
      "csquotes": "",
      "DOIandURL": "both",
      "exportBibTeXStrings": "off",
      "exportBraceProtection": true,
      "exportTitleCase": true,
      "extraMergeCitekeys": false,
      "extraMergeCSL": false,
      "extraMergeTeX": false,
      "git": "config",
      "import": true,
      "importBibTeXStrings": true,
      "importCaseProtection": "as-needed",
      "importCitationKey": true,
      "importExtra": true,
      "importJabRefAbbreviations": true,
      "importJabRefStrings": true,
      "importNoteToExtra": "",
      "importSentenceCase": "on+guess",
      "importUnknownTexCommand": "ignore",
      "itemObserverDelay": 5,
      "jabrefFormat": 0,
      "jieba": false,
      "keyConflictPolicy": "keep",
      "keyScope": "library",
      "kuroshiro": false,
      "language": "langid",
      "mapMath": "",
      "mapText": "",
      "mapUnicode": "conservative",
      "parseParticles": true,
      "patchDates": "dateadded=dateAdded, date-added=dateAdded, datemodified=dateModified, date-modified=dateModified",
      "postscript": "",
      "postscriptOverride": "",
      "preferencesOverride": "",
      "qualityReport": false,
      "quickCopyEta": "",
      "quickCopyMode": "latex",
      "quickCopyOrgMode": "zotero",
      "quickCopyPandocBrackets": false,
      "quickCopySelectLink": "zotero",
      "rawImports": false,
      "rawLaTag": "#LaTeX",
      "relativeFilePaths": false,
      "retainCache": false,
      "separatorList": "and",
      "separatorNames": "and",
      "skipFields": "",
      "skipWords": "a,ab,aboard,about,above,across,after,against,al,along,amid,among,an,and,anti,around,as,at,before,behind,below,beneath,beside,besides,between,beyond,but,by,d,da,das,de,del,dell,dello,dei,degli,della,dell,delle,dem,den,der,des,despite,die,do,down,du,during,ein,eine,einem,einen,einer,eines,el,en,et,except,for,from,gli,i,il,in,inside,into,is,l,la,las,le,les,like,lo,los,near,nor,of,off,on,onto,or,over,past,per,plus,round,save,since,so,some,sur,than,the,through,to,toward,towards,un,una,unas,under,underneath,une,unlike,uno,unos,until,up,upon,versus,via,von,while,with,within,without,yet,zu,zum",
      "startupProgress": "popup",
      "strings": "",
      "stringsOverride": "",
      "verbatimFields": "url,doi,file,pdf,ids,eprint,/^verb[a-z]$/,groups,/^citeulike-linkout-[0-9]+$/, /^bdsk-url-[0-9]+$/",
      "warnBulkModify": 10,
      "warnTitleCased": false,
      "texmap": {}
    },
    "options": {
      "exportNotes": true,
      "exportFileData": false,
      "keepUpdated": true,
      "worker": true,
      "Normalize": false
    }
  },
  "version": {
    "zotero": "6.0.26",
    "bbt": "6.7.75"
  },
  "collections": {
    "B9BDNXQ3": {
      "key": "B9BDNXQ3",
      "parent": "",
      "name": "Backdoor",
      "collections": [],
      "items": [
        2253,
        2971,
        3061
      ]
    },
    "ZHLXVZD8": {
      "key": "ZHLXVZD8",
      "parent": "",
      "name": "Black Box",
      "collections": [],
      "items": []
    },
    "SFGF7J9D": {
      "key": "SFGF7J9D",
      "parent": "",
      "name": "Classic",
      "collections": [],
      "items": [
        1799,
        1801,
        1807,
        1813,
        1844,
        1847,
        1849,
        1859,
        2457,
        2471,
        2472,
        2473,
        2479
      ]
    },
    "9GDH6RIA": {
      "key": "9GDH6RIA",
      "parent": "",
      "name": "David Wagner",
      "collections": [],
      "items": [
        1752,
        1753,
        1759,
        1761,
        1763,
        1766,
        1768,
        1770,
        1772,
        1774,
        1776,
        1778,
        1780,
        1782,
        1784,
        1787,
        1789,
        1791,
        1793
      ]
    },
    "6NL6J23I": {
      "key": "6NL6J23I",
      "parent": "",
      "name": "Distillation",
      "collections": [],
      "items": []
    },
    "E8GRLXAS": {
      "key": "E8GRLXAS",
      "parent": "",
      "name": "Graph",
      "collections": [],
      "items": [
        2253,
        2267,
        2477,
        2478
      ]
    },
    "BVYAHC9U": {
      "key": "BVYAHC9U",
      "parent": "",
      "name": "IP protect",
      "collections": [],
      "items": [
        3116,
        3310,
        3421,
        3424,
        3428,
        3481,
        3566,
        3578
      ]
    },
    "2TZ4EX8H": {
      "key": "2TZ4EX8H",
      "parent": "",
      "name": "NTU",
      "collections": [],
      "items": [
        3116,
        3296,
        3310,
        3315,
        3353
      ]
    },
    "A8ND2ME2": {
      "key": "A8ND2ME2",
      "parent": "",
      "name": "Privacy",
      "collections": [],
      "items": [
        3057,
        3058,
        3059,
        3060
      ]
    },
    "LQL26C5Q": {
      "key": "LQL26C5Q",
      "parent": "",
      "name": "Recent",
      "collections": [],
      "items": [
        1816,
        1835,
        1839,
        1843,
        2880,
        3056
      ]
    },
    "QEBL2CYY": {
      "key": "QEBL2CYY",
      "parent": "",
      "name": "Tao",
      "collections": [],
      "items": [
        2232,
        2467,
        2867
      ]
    },
    "7YX43XPN": {
      "key": "7YX43XPN",
      "parent": "",
      "name": "Theory",
      "collections": [],
      "items": [
        2474
      ]
    },
    "4FBQ34NZ": {
      "key": "4FBQ34NZ",
      "parent": "",
      "name": "White box",
      "collections": [],
      "items": [
        1847,
        1849,
        2475,
        2476,
        2479,
        3062
      ]
    }
  },
  "items": [
    {
      "key": "ZYZBC4TW",
      "version": 27,
      "itemType": "preprint",
      "title": "Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples",
      "abstractNote": "We identify obfuscated gradients, a kind of gradient masking, as a phenomenon that leads to a false sense of security in defenses against adversarial examples. While defenses that cause obfuscated gradients appear to defeat iterative optimizationbased attacks, we ﬁnd defenses relying on this effect can be circumvented. We describe characteristic behaviors of defenses exhibiting the effect, and for each of the three types of obfuscated gradients we discover, we develop attack techniques to overcome it. In a case study, examining noncertiﬁed white-box-secure defenses at ICLR 2018, we ﬁnd obfuscated gradients are a common occurrence, with 7 of 9 defenses relying on obfuscated gradients. Our new attacks successfully circumvent 6 completely, and 1 partially, in the original threat model each paper considers.",
      "date": "2018-07-30",
      "language": "en",
      "shortTitle": "Obfuscated Gradients Give a False Sense of Security",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/1802.00420",
      "accessDate": "2023-01-24T11:01:22Z",
      "extra": "arXiv:1802.00420 [cs]",
      "repository": "arXiv",
      "archiveID": "arXiv:1802.00420",
      "creators": [
        {
          "firstName": "Anish",
          "lastName": "Athalye",
          "creatorType": "author"
        },
        {
          "firstName": "Nicholas",
          "lastName": "Carlini",
          "creatorType": "author"
        },
        {
          "firstName": "David",
          "lastName": "Wagner",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Machine Learning",
          "type": 1
        },
        {
          "tag": "Computer Science - Cryptography and Security",
          "type": 1
        },
        {
          "tag": "Computer Science - Artificial Intelligence",
          "type": 1
        }
      ],
      "relations": [],
      "dateAdded": "2023-01-24T11:01:22Z",
      "dateModified": "2023-01-24T11:01:22Z",
      "uri": "http://zotero.org/groups/4922950/items/ZYZBC4TW",
      "itemID": 1763,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "obfgrad-icml18.pdf",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-01-24T11:01:17Z",
          "dateModified": "2023-01-24T11:01:22Z",
          "uri": "http://zotero.org/groups/4922950/items/SZ5BBFDF",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\SZ5BBFDF\\obfgrad-icml18.pdf",
          "select": "zotero://select/groups/4922950/items/SZ5BBFDF"
        }
      ],
      "notes": [
        {
          "key": "G92V99RB",
          "version": 27,
          "itemType": "note",
          "parentItem": "ZYZBC4TW",
          "note": "Comment: ICML 2018. Source code at https://github.com/anishathalye/obfuscated-gradients",
          "tags": [],
          "relations": {},
          "dateAdded": "2023-01-24T11:01:22Z",
          "dateModified": "2023-01-24T11:01:22Z",
          "uri": "http://zotero.org/groups/4922950/items/G92V99RB"
        }
      ],
      "citationKey": "athalyeObfuscatedGradientsGive2018",
      "itemKey": "ZYZBC4TW",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/ZYZBC4TW"
    },
    {
      "key": "6G7B8T3G",
      "version": 108,
      "itemType": "preprint",
      "title": "Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples",
      "abstractNote": "We identify obfuscated gradients, a kind of gradient masking, as a phenomenon that leads to a false sense of security in defenses against adversarial examples. While defenses that cause obfuscated gradients appear to defeat iterative optimization-based attacks, we find defenses relying on this effect can be circumvented. We describe characteristic behaviors of defenses exhibiting the effect, and for each of the three types of obfuscated gradients we discover, we develop attack techniques to overcome it. In a case study, examining non-certified white-box-secure defenses at ICLR 2018, we find obfuscated gradients are a common occurrence, with 7 of 9 defenses relying on obfuscated gradients. Our new attacks successfully circumvent 6 completely, and 1 partially, in the original threat model each paper considers.",
      "date": "2018-07-30",
      "shortTitle": "Obfuscated Gradients Give a False Sense of Security",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/1802.00420",
      "accessDate": "2023-01-29T15:02:26Z",
      "extra": "arXiv:1802.00420 [cs]",
      "DOI": "10.48550/arXiv.1802.00420",
      "repository": "arXiv",
      "archiveID": "arXiv:1802.00420",
      "creators": [
        {
          "firstName": "Anish",
          "lastName": "Athalye",
          "creatorType": "author"
        },
        {
          "firstName": "Nicholas",
          "lastName": "Carlini",
          "creatorType": "author"
        },
        {
          "firstName": "David",
          "lastName": "Wagner",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Machine Learning",
          "type": 1
        },
        {
          "tag": "Computer Science - Cryptography and Security",
          "type": 1
        },
        {
          "tag": "Computer Science - Artificial Intelligence",
          "type": 1
        }
      ],
      "relations": [],
      "dateAdded": "2023-01-29T15:02:26Z",
      "dateModified": "2023-01-29T15:02:26Z",
      "uri": "http://zotero.org/groups/4922950/items/6G7B8T3G",
      "itemID": 1813,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "arXiv Fulltext PDF",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-01-29T15:02:30Z",
          "dateModified": "2023-01-29T15:02:30Z",
          "uri": "http://zotero.org/groups/4922950/items/NX59VWMW",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\NX59VWMW\\Athalye 等 - 2018 - Obfuscated Gradients Give a False Sense of Securit.pdf",
          "select": "zotero://select/groups/4922950/items/NX59VWMW"
        },
        {
          "itemType": "attachment",
          "title": "arXiv.org Snapshot",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-01-29T15:02:39Z",
          "dateModified": "2023-01-29T15:02:39Z",
          "uri": "http://zotero.org/groups/4922950/items/DFCEQZ6H",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\DFCEQZ6H\\1802.html",
          "select": "zotero://select/groups/4922950/items/DFCEQZ6H"
        }
      ],
      "notes": [
        {
          "key": "6YMARHYL",
          "version": 108,
          "itemType": "note",
          "parentItem": "6G7B8T3G",
          "note": "Comment: ICML 2018. Source code at https://github.com/anishathalye/obfuscated-gradients",
          "tags": [],
          "relations": {},
          "dateAdded": "2023-01-29T15:02:26Z",
          "dateModified": "2023-01-29T15:02:26Z",
          "uri": "http://zotero.org/groups/4922950/items/6YMARHYL"
        }
      ],
      "citationKey": "athalyeObfuscatedGradientsGive2018a",
      "itemKey": "6G7B8T3G",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/6G7B8T3G"
    },
    {
      "key": "PCA6EYDD",
      "version": 210,
      "itemType": "attachment",
      "title": "arXiv Fulltext PDF",
      "url": "https://arxiv.org/pdf/2103.00020.pdf",
      "accessDate": "2023-02-20T07:30:19Z",
      "linkMode": "imported_url",
      "contentType": "application/pdf",
      "charset": "",
      "filename": "Radford 等 - 2021 - Learning Transferable Visual Models From Natural L.pdf",
      "tags": [],
      "collections": [],
      "relations": {},
      "dateAdded": "2023-02-20T07:30:19Z",
      "dateModified": "2023-02-20T07:30:19Z",
      "uri": "http://zotero.org/groups/4922950/items/PCA6EYDD",
      "itemID": 1885,
      "localPath": "C:\\Users\\AKK87\\Zotero\\storage\\PCA6EYDD\\Radford 等 - 2021 - Learning Transferable Visual Models From Natural L.pdf",
      "defaultPath": "files/1885/Radford 等 - 2021 - Learning Transferable Visual Models From Natural L.pdf",
      "itemKey": "PCA6EYDD",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/PCA6EYDD"
    },
    {
      "key": "VL8DC2QA",
      "version": 228,
      "itemType": "attachment",
      "title": "arXiv.org Snapshot",
      "url": "https://arxiv.org/abs/2003.01332",
      "accessDate": "2023-03-04T05:38:35Z",
      "linkMode": "imported_url",
      "contentType": "text/html",
      "charset": "utf-8",
      "filename": "2003.html",
      "tags": [],
      "collections": [],
      "relations": {},
      "dateAdded": "2023-03-04T05:38:35Z",
      "dateModified": "2023-03-04T05:38:35Z",
      "uri": "http://zotero.org/groups/4922950/items/VL8DC2QA",
      "itemID": 2263,
      "localPath": "C:\\Users\\AKK87\\Zotero\\storage\\VL8DC2QA\\2003.html",
      "defaultPath": "files/2263/2003.html",
      "itemKey": "VL8DC2QA",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/VL8DC2QA"
    },
    {
      "key": "NKXSWMLT",
      "version": 608,
      "itemType": "attachment",
      "title": "arXiv Fulltext PDF",
      "url": "https://arxiv.org/pdf/1805.07984.pdf",
      "accessDate": "2023-03-04T05:41:25Z",
      "linkMode": "imported_url",
      "contentType": "application/pdf",
      "charset": "",
      "filename": "Zügner 等 - 2018 - Adversarial Attacks on Neural Networks for Graph D.pdf",
      "tags": [],
      "collections": [
        "E8GRLXAS"
      ],
      "relations": {
        "owl:sameAs": [
          "http://zotero.org/groups/4848365/items/AANSQBM4"
        ]
      },
      "dateAdded": "2023-03-15T08:25:59Z",
      "dateModified": "2023-03-15T08:25:59Z",
      "uri": "http://zotero.org/groups/4922950/items/NKXSWMLT",
      "itemID": 2478,
      "localPath": "C:\\Users\\AKK87\\Zotero\\storage\\NKXSWMLT\\Zügner 等 - 2018 - Adversarial Attacks on Neural Networks for Graph D.pdf",
      "defaultPath": "files/2478/Zügner 等 - 2018 - Adversarial Attacks on Neural Networks for Graph D.pdf",
      "itemKey": "NKXSWMLT",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/NKXSWMLT"
    },
    {
      "key": "XNAPFVV9",
      "version": 725,
      "itemType": "attachment",
      "title": "arXiv Fulltext PDF",
      "url": "https://arxiv.org/pdf/2303.01263.pdf",
      "accessDate": "2023-03-04T03:16:03Z",
      "linkMode": "imported_url",
      "contentType": "application/pdf",
      "charset": "",
      "filename": "Dai 等 - 2023 - Unnoticeable Backdoor Attacks on Graph Neural Netw.pdf",
      "tags": [],
      "collections": [
        "E8GRLXAS"
      ],
      "relations": {
        "owl:sameAs": [
          "http://zotero.org/groups/4848365/items/FQIV275F"
        ]
      },
      "dateAdded": "2023-03-15T08:26:04Z",
      "dateModified": "2023-03-15T08:26:04Z",
      "uri": "http://zotero.org/groups/4922950/items/XNAPFVV9",
      "itemID": 2477,
      "localPath": "C:\\Users\\AKK87\\Zotero\\storage\\XNAPFVV9\\Dai 等 - 2023 - Unnoticeable Backdoor Attacks on Graph Neural Netw.pdf",
      "defaultPath": "files/2477/Dai 等 - 2023 - Unnoticeable Backdoor Attacks on Graph Neural Netw.pdf",
      "itemKey": "XNAPFVV9",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/XNAPFVV9"
    },
    {
      "key": "5SJ9TB5E",
      "version": 1075,
      "itemType": "attachment",
      "title": "arXiv Fulltext PDF",
      "url": "https://arxiv.org/pdf/2303.16199.pdf",
      "accessDate": "2023-04-04T09:24:08Z",
      "linkMode": "imported_url",
      "contentType": "application/pdf",
      "charset": "",
      "filename": "Zhang 等 - 2023 - LLaMA-Adapter Efficient Fine-tuning of Language M.pdf",
      "tags": [],
      "collections": [],
      "relations": {},
      "dateAdded": "2023-04-04T09:24:08Z",
      "dateModified": "2023-04-04T09:24:08Z",
      "uri": "http://zotero.org/groups/4922950/items/5SJ9TB5E",
      "itemID": 2915,
      "localPath": "C:\\Users\\AKK87\\Zotero\\storage\\5SJ9TB5E\\Zhang 等 - 2023 - LLaMA-Adapter Efficient Fine-tuning of Language M.pdf",
      "defaultPath": "files/2915/Zhang 等 - 2023 - LLaMA-Adapter Efficient Fine-tuning of Language M.pdf",
      "itemKey": "5SJ9TB5E",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/5SJ9TB5E"
    },
    {
      "key": "XNXTX648",
      "version": 1614,
      "itemType": "attachment",
      "title": "arXiv Fulltext PDF",
      "url": "https://arxiv.org/pdf/2303.01469.pdf",
      "accessDate": "2023-04-16T15:36:09Z",
      "linkMode": "imported_url",
      "contentType": "application/pdf",
      "charset": "",
      "filename": "Song 等 - 2023 - Consistency Models.pdf",
      "tags": [],
      "collections": [],
      "relations": {},
      "dateAdded": "2023-04-16T15:36:09Z",
      "dateModified": "2023-04-16T15:36:09Z",
      "uri": "http://zotero.org/groups/4922950/items/XNXTX648",
      "itemID": 3438,
      "localPath": "C:\\Users\\AKK87\\Zotero\\storage\\XNXTX648\\Song 等 - 2023 - Consistency Models.pdf",
      "defaultPath": "files/3438/Song 等 - 2023 - Consistency Models.pdf",
      "itemKey": "XNXTX648",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/XNXTX648"
    },
    {
      "key": "Z5U8NDLN",
      "version": 1623,
      "itemType": "attachment",
      "title": "Full Text PDF",
      "url": "https://arxiv.org/pdf/2209.02646",
      "accessDate": "2023-04-17T08:49:39Z",
      "linkMode": "imported_url",
      "contentType": "application/pdf",
      "charset": "",
      "filename": "Cao 等 - 2022 - A Survey on Generative Diffusion Model.pdf",
      "tags": [],
      "collections": [],
      "relations": {},
      "dateAdded": "2023-04-17T08:49:39Z",
      "dateModified": "2023-04-17T08:49:39Z",
      "uri": "http://zotero.org/groups/4922950/items/Z5U8NDLN",
      "itemID": 3465,
      "localPath": "C:\\Users\\AKK87\\Zotero\\storage\\Z5U8NDLN\\Cao 等 - 2022 - A Survey on Generative Diffusion Model.pdf",
      "defaultPath": "files/3465/Cao 等 - 2022 - A Survey on Generative Diffusion Model.pdf",
      "itemKey": "Z5U8NDLN",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/Z5U8NDLN"
    },
    {
      "key": "NZGSEN6W",
      "version": 153,
      "itemType": "bookSection",
      "title": "Evasion Attacks against Machine Learning at Test Time",
      "abstractNote": "In security-sensitive applications, the success of machine learning depends on a thorough vetting of their resistance to adversarial data. In one pertinent, well-motivated attack scenario, an adversary may attempt to evade a deployed system at test time by carefully manipulating attack samples. In this work, we present a simple but effective gradient-based approach that can be exploited to systematically assess the security of several, widely-used classification algorithms against evasion attacks. Following a recently proposed framework for security evaluation, we simulate attack scenarios that exhibit different risk levels for the classifier by increasing the attacker's knowledge of the system and her ability to manipulate attack samples. This gives the classifier designer a better picture of the classifier performance under evasion attacks, and allows him to perform a more informed model selection (or parameter setting). We evaluate our approach on the relevant security task of malware detection in PDF files, and show that such systems can be easily evaded. We also sketch some countermeasures suggested by our analysis.",
      "date": "2013",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/1708.06131",
      "accessDate": "2023-01-31T12:15:37Z",
      "extra": "DOI: 10.1007/978-3-642-40994-3_25\narXiv:1708.06131 [cs]",
      "volume": "7908",
      "pages": "387-402",
      "creators": [
        {
          "firstName": "Battista",
          "lastName": "Biggio",
          "creatorType": "author"
        },
        {
          "firstName": "Igino",
          "lastName": "Corona",
          "creatorType": "author"
        },
        {
          "firstName": "Davide",
          "lastName": "Maiorca",
          "creatorType": "author"
        },
        {
          "firstName": "Blaine",
          "lastName": "Nelson",
          "creatorType": "author"
        },
        {
          "firstName": "Nedim",
          "lastName": "Srndic",
          "creatorType": "author"
        },
        {
          "firstName": "Pavel",
          "lastName": "Laskov",
          "creatorType": "author"
        },
        {
          "firstName": "Giorgio",
          "lastName": "Giacinto",
          "creatorType": "author"
        },
        {
          "firstName": "Fabio",
          "lastName": "Roli",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Machine Learning",
          "type": 1
        },
        {
          "tag": "Computer Science - Cryptography and Security",
          "type": 1
        }
      ],
      "relations": [],
      "dateAdded": "2023-01-31T12:15:37Z",
      "dateModified": "2023-01-31T12:15:38Z",
      "uri": "http://zotero.org/groups/4922950/items/NZGSEN6W",
      "itemID": 1844,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "arXiv Fulltext PDF",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-01-31T12:15:51Z",
          "dateModified": "2023-01-31T12:15:51Z",
          "uri": "http://zotero.org/groups/4922950/items/Y4HXKY5I",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\Y4HXKY5I\\Biggio 等 - 2013 - Evasion Attacks against Machine Learning at Test T.pdf",
          "select": "zotero://select/groups/4922950/items/Y4HXKY5I"
        },
        {
          "itemType": "attachment",
          "title": "arXiv.org Snapshot",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-01-31T12:16:17Z",
          "dateModified": "2023-01-31T12:16:17Z",
          "uri": "http://zotero.org/groups/4922950/items/JF3C2X68",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\JF3C2X68\\1708.html",
          "select": "zotero://select/groups/4922950/items/JF3C2X68"
        }
      ],
      "notes": [
        {
          "key": "P82BH65V",
          "version": 288,
          "itemType": "note",
          "parentItem": "NZGSEN6W",
          "note": "<div data-citation-items=\"%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FNZGSEN6W%22%5D%2C%22itemData%22%3A%7B%22id%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FNZGSEN6W%22%2C%22type%22%3A%22chapter%22%2C%22abstract%22%3A%22In%20security-sensitive%20applications%2C%20the%20success%20of%20machine%20learning%20depends%20on%20a%20thorough%20vetting%20of%20their%20resistance%20to%20adversarial%20data.%20In%20one%20pertinent%2C%20well-motivated%20attack%20scenario%2C%20an%20adversary%20may%20attempt%20to%20evade%20a%20deployed%20system%20at%20test%20time%20by%20carefully%20manipulating%20attack%20samples.%20In%20this%20work%2C%20we%20present%20a%20simple%20but%20effective%20gradient-based%20approach%20that%20can%20be%20exploited%20to%20systematically%20assess%20the%20security%20of%20several%2C%20widely-used%20classification%20algorithms%20against%20evasion%20attacks.%20Following%20a%20recently%20proposed%20framework%20for%20security%20evaluation%2C%20we%20simulate%20attack%20scenarios%20that%20exhibit%20different%20risk%20levels%20for%20the%20classifier%20by%20increasing%20the%20attacker's%20knowledge%20of%20the%20system%20and%20her%20ability%20to%20manipulate%20attack%20samples.%20This%20gives%20the%20classifier%20designer%20a%20better%20picture%20of%20the%20classifier%20performance%20under%20evasion%20attacks%2C%20and%20allows%20him%20to%20perform%20a%20more%20informed%20model%20selection%20(or%20parameter%20setting).%20We%20evaluate%20our%20approach%20on%20the%20relevant%20security%20task%20of%20malware%20detection%20in%20PDF%20files%2C%20and%20show%20that%20such%20systems%20can%20be%20easily%20evaded.%20We%20also%20sketch%20some%20countermeasures%20suggested%20by%20our%20analysis.%22%2C%22note%22%3A%22DOI%3A%2010.1007%2F978-3-642-40994-3_25%5CnarXiv%3A1708.06131%20%5Bcs%5D%22%2C%22page%22%3A%22387-402%22%2C%22source%22%3A%22arXiv.org%22%2C%22title%22%3A%22Evasion%20Attacks%20against%20Machine%20Learning%20at%20Test%20Time%22%2C%22URL%22%3A%22http%3A%2F%2Farxiv.org%2Fabs%2F1708.06131%22%2C%22volume%22%3A%227908%22%2C%22author%22%3A%5B%7B%22family%22%3A%22Biggio%22%2C%22given%22%3A%22Battista%22%7D%2C%7B%22family%22%3A%22Corona%22%2C%22given%22%3A%22Igino%22%7D%2C%7B%22family%22%3A%22Maiorca%22%2C%22given%22%3A%22Davide%22%7D%2C%7B%22family%22%3A%22Nelson%22%2C%22given%22%3A%22Blaine%22%7D%2C%7B%22family%22%3A%22Srndic%22%2C%22given%22%3A%22Nedim%22%7D%2C%7B%22family%22%3A%22Laskov%22%2C%22given%22%3A%22Pavel%22%7D%2C%7B%22family%22%3A%22Giacinto%22%2C%22given%22%3A%22Giorgio%22%7D%2C%7B%22family%22%3A%22Roli%22%2C%22given%22%3A%22Fabio%22%7D%5D%2C%22accessed%22%3A%7B%22date-parts%22%3A%5B%5B%222023%22%2C1%2C31%5D%5D%7D%2C%22issued%22%3A%7B%22date-parts%22%3A%5B%5B%222013%22%5D%5D%7D%7D%7D%5D\" data-schema-version=\"8\"><h1>Abstract</h1>\n<p>在测试阶段进行攻击</p>\n<p>提出了一种通过梯度来评估安全性的算法</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FY4HXKY5I%22%2C%22annotationKey%22%3A%2253MUD736%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%221%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B295.724%2C385.615%2C452.246%2C393.577%5D%2C%5B163.111%2C374.656%2C452.247%2C382.618%5D%2C%5B163.111%2C363.697%2C449.687%2C371.659%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FNZGSEN6W%22%5D%2C%22locator%22%3A%221%22%7D%7D\">“present a simple but effective gradientbased approach that can be exploited to systematically assess the security of several, widely-used classification algorithms against evasion attacks”</span></p>\n<p>本文通过攻击者能力的建模，分析了各种风险等级</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FY4HXKY5I%22%2C%22annotationKey%22%3A%22WNMJPUMD%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%221%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B306.954%2C341.779%2C452.246%2C349.741%5D%2C%5B163.111%2C330.82%2C452.242%2C338.782%5D%2C%5B163.111%2C319.861%2C269.439%2C327.823%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FNZGSEN6W%22%5D%2C%22locator%22%3A%221%22%7D%7D\">“different risk levels for the classifier by increasing the attacker’s knowledge of the system and her ability to manipulate attack samples”</span></p>\n<p>然后在各种对安全性要求高的例子中进行了实验</p>\n<p></p>\n<h1>Intro</h1>\n<p>Motivation在于攻击者和设计者之间的博弈带来的越来越复杂的设计。需要一种更高效的方式来评估机器学习算法的安全性</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FY4HXKY5I%22%2C%22annotationKey%22%3A%227N7S5YAQ%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B445.615%2C604.493%2C480.594%2C613.34%5D%2C%5B134.765%2C592.538%2C480.591%2C601.385%5D%2C%5B134.765%2C580.583%2C480.587%2C589.43%5D%2C%5B134.765%2C568.627%2C199.333%2C577.474%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FNZGSEN6W%22%5D%2C%22locator%22%3A%222%22%7D%7D\">“classical performance evaluation techniques are not suitable to reliably assess the security of learning algorithms, i.e., the performance degradation caused by carefully crafted attacks”</span></p>\n<p>然后作者通过安全软件设计以及密码学领域来分析机制</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FY4HXKY5I%22%2C%22annotationKey%22%3A%22QIUZVAMD%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B238.334%2C542.538%2C480.591%2C551.385%5D%2C%5B134.765%2C530.583%2C344.08%2C539.43%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FNZGSEN6W%22%5D%2C%22locator%22%3A%222%22%7D%7D\">“paradigms from security engineering and cryptography have been adapted to the machine learning field”</span></p>\n<p></p>\n<p></p>\n<p></p>\n<p></p>\n<p></p>\n<h1>Gradient descent attack</h1>\n</div>",
          "tags": [],
          "relations": {},
          "dateAdded": "2023-03-07T00:42:48Z",
          "dateModified": "2023-03-07T10:28:12Z",
          "uri": "http://zotero.org/groups/4922950/items/P82BH65V"
        },
        {
          "key": "SK8E8GDW",
          "version": 152,
          "itemType": "note",
          "parentItem": "NZGSEN6W",
          "note": "Comment: In this paper, in 2013, we were the first to introduce the notion of evasion attacks (adversarial examples) created with high confidence (instead of minimum-distance misclassifications), and the notion of surrogate learners (substitute models). These two concepts are now widely re-used in developing attacks against deep networks (even if not always referring to the ideas reported in this work). arXiv admin note: text overlap with arXiv:1401.7727",
          "tags": [],
          "relations": {},
          "dateAdded": "2023-01-31T12:15:37Z",
          "dateModified": "2023-01-31T12:15:37Z",
          "uri": "http://zotero.org/groups/4922950/items/SK8E8GDW"
        }
      ],
      "citationKey": "biggioEvasionAttacksMachine2013",
      "itemKey": "NZGSEN6W",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/NZGSEN6W"
    },
    {
      "key": "LNSATXVT",
      "version": 55,
      "itemType": "conferencePaper",
      "title": "Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods",
      "abstractNote": "While Adversarial Training remains the standard in improving robustness to adversarial attack, it often sacriﬁces accuracy on natural (clean) samples to a signiﬁcant extent. Dual-domain training, optimizing on both clean and adversarial objectives, can help realize a better trade-off between clean accuracy and robustness. In this paper, we develop methods to improve dual-domain training for large adversarial perturbations and complex datasets. We ﬁrst demonstrate that existing methods suffer from poor performance in this setting, due to a poor training procedure and overﬁtting to a particular attack. Then, we develop novel methods to address these issues. First, we show that adding KLD regularization to the dual training objective mitigates this overﬁtting and achieves a better trade-off, on CIFAR-10 and a 10-class subset of ImageNet. Then, inspired by domain adaptation, we develop a new normalization technique, Dual Batch Normalization, to further improve accuracy. Combining these two strategies, our model sets a new state of the art in trade-off performance for dual-domain adversarial training.",
      "date": "2017-11-03",
      "language": "en",
      "shortTitle": "Adversarial Examples Are Not Easily Detected",
      "libraryCatalog": "DOI.org (Crossref)",
      "url": "https://dl.acm.org/doi/10.1145/3128572.3140444",
      "accessDate": "2023-01-24T11:30:08Z",
      "place": "Dallas Texas USA",
      "publisher": "ACM",
      "ISBN": "978-1-4503-5202-4",
      "pages": "3-14",
      "proceedingsTitle": "Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security",
      "conferenceName": "CCS '17: 2017 ACM SIGSAC Conference on Computer and Communications Security",
      "DOI": "10.1145/3128572.3140444",
      "creators": [
        {
          "firstName": "Nicholas",
          "lastName": "Carlini",
          "creatorType": "author"
        },
        {
          "firstName": "David",
          "lastName": "Wagner",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": [],
      "dateAdded": "2023-01-24T11:30:08Z",
      "dateModified": "2023-01-24T11:30:08Z",
      "uri": "http://zotero.org/groups/4922950/items/LNSATXVT",
      "itemID": 1782,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "Carlini 和 Wagner - 2017 - Adversarial Examples Are Not Easily Detected Bypa.pdf",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-01-24T11:30:04Z",
          "dateModified": "2023-01-24T11:30:09Z",
          "uri": "http://zotero.org/groups/4922950/items/4J6LDMH6",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\4J6LDMH6\\Carlini 和 Wagner - 2017 - Adversarial Examples Are Not Easily Detected Bypa.pdf",
          "select": "zotero://select/groups/4922950/items/4J6LDMH6"
        }
      ],
      "notes": [],
      "citationKey": "carliniAdversarialExamplesAre2017",
      "itemKey": "LNSATXVT",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/LNSATXVT"
    },
    {
      "key": "2329HR98",
      "version": 22,
      "itemType": "conferencePaper",
      "title": "Audio Adversarial Examples: Targeted Attacks on Speech-to-Text",
      "abstractNote": "We construct targeted audio adversarial examples on automatic speech recognition. Given any audio waveform, we can produce another that is over 99.9% similar, but transcribes as any phrase we choose (recognizing up to 50 characters per second). We apply our white-box iterative optimization-based attack to Mozilla’s implementation DeepSpeech end-to-end, and show it has a 100% success rate. The feasibility of this attack introduce a new domain to study adversarial examples.",
      "date": "5/2018",
      "language": "en",
      "shortTitle": "Audio Adversarial Examples",
      "libraryCatalog": "DOI.org (Crossref)",
      "url": "https://ieeexplore.ieee.org/document/8424625/",
      "accessDate": "2023-01-24T11:00:56Z",
      "place": "San Francisco, CA",
      "publisher": "IEEE",
      "ISBN": "978-1-5386-8276-0",
      "pages": "1-7",
      "proceedingsTitle": "2018 IEEE Security and Privacy Workshops (SPW)",
      "conferenceName": "2018 IEEE Security and Privacy Workshops (SPW)",
      "DOI": "10.1109/SPW.2018.00009",
      "creators": [
        {
          "firstName": "Nicholas",
          "lastName": "Carlini",
          "creatorType": "author"
        },
        {
          "firstName": "David",
          "lastName": "Wagner",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": [],
      "dateAdded": "2023-01-24T11:00:56Z",
      "dateModified": "2023-01-24T11:00:56Z",
      "uri": "http://zotero.org/groups/4922950/items/2329HR98",
      "itemID": 1761,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "Carlini 和 Wagner - 2018 - Audio Adversarial Examples Targeted Attacks on Sp.pdf",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-01-24T11:00:52Z",
          "dateModified": "2023-01-24T11:00:57Z",
          "uri": "http://zotero.org/groups/4922950/items/7I9C26YR",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\7I9C26YR\\Carlini 和 Wagner - 2018 - Audio Adversarial Examples Targeted Attacks on Sp.pdf",
          "select": "zotero://select/groups/4922950/items/7I9C26YR"
        }
      ],
      "notes": [],
      "citationKey": "carliniAudioAdversarialExamples2018",
      "itemKey": "2329HR98",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/2329HR98"
    },
    {
      "key": "SUDRUK3U",
      "version": 1206,
      "itemType": "preprint",
      "title": "Extracting Training Data from Large Language Models",
      "abstractNote": "It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model. We demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data. We comprehensively evaluate our extraction attack to understand the factors that contribute to its success. Worryingly, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models.",
      "date": "2021-06-15",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/2012.07805",
      "accessDate": "2023-04-11T03:59:15Z",
      "extra": "arXiv:2012.07805 [cs]",
      "DOI": "10.48550/arXiv.2012.07805",
      "repository": "arXiv",
      "archiveID": "arXiv:2012.07805",
      "creators": [
        {
          "firstName": "Nicholas",
          "lastName": "Carlini",
          "creatorType": "author"
        },
        {
          "firstName": "Florian",
          "lastName": "Tramer",
          "creatorType": "author"
        },
        {
          "firstName": "Eric",
          "lastName": "Wallace",
          "creatorType": "author"
        },
        {
          "firstName": "Matthew",
          "lastName": "Jagielski",
          "creatorType": "author"
        },
        {
          "firstName": "Ariel",
          "lastName": "Herbert-Voss",
          "creatorType": "author"
        },
        {
          "firstName": "Katherine",
          "lastName": "Lee",
          "creatorType": "author"
        },
        {
          "firstName": "Adam",
          "lastName": "Roberts",
          "creatorType": "author"
        },
        {
          "firstName": "Tom",
          "lastName": "Brown",
          "creatorType": "author"
        },
        {
          "firstName": "Dawn",
          "lastName": "Song",
          "creatorType": "author"
        },
        {
          "firstName": "Ulfar",
          "lastName": "Erlingsson",
          "creatorType": "author"
        },
        {
          "firstName": "Alina",
          "lastName": "Oprea",
          "creatorType": "author"
        },
        {
          "firstName": "Colin",
          "lastName": "Raffel",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Machine Learning",
          "type": 1
        },
        {
          "tag": "Computer Science - Computation and Language",
          "type": 1
        },
        {
          "tag": "Computer Science - Cryptography and Security",
          "type": 1
        }
      ],
      "relations": [],
      "dateAdded": "2023-04-11T03:59:15Z",
      "dateModified": "2023-04-11T03:59:15Z",
      "uri": "http://zotero.org/groups/4922950/items/SUDRUK3U",
      "itemID": 3060,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "arXiv Fulltext PDF",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-04-11T04:01:02Z",
          "dateModified": "2023-04-11T04:01:02Z",
          "uri": "http://zotero.org/groups/4922950/items/2C48SIVZ",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\2C48SIVZ\\Carlini 等 - 2021 - Extracting Training Data from Large Language Model.pdf",
          "select": "zotero://select/groups/4922950/items/2C48SIVZ"
        },
        {
          "itemType": "attachment",
          "title": "arXiv.org Snapshot",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-04-11T04:01:16Z",
          "dateModified": "2023-04-11T04:01:16Z",
          "uri": "http://zotero.org/groups/4922950/items/RU7QGLP4",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\RU7QGLP4\\2012.html",
          "select": "zotero://select/groups/4922950/items/RU7QGLP4"
        }
      ],
      "notes": [],
      "citationKey": "carliniExtractingTrainingData2021",
      "itemKey": "SUDRUK3U",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/SUDRUK3U"
    },
    {
      "key": "SW84BDSG",
      "version": 1207,
      "itemType": "preprint",
      "title": "Extracting Training Data from Diffusion Models",
      "abstractNote": "Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training.",
      "date": "2023-01-30",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/2301.13188",
      "accessDate": "2023-04-11T03:59:24Z",
      "extra": "arXiv:2301.13188 [cs]",
      "DOI": "10.48550/arXiv.2301.13188",
      "repository": "arXiv",
      "archiveID": "arXiv:2301.13188",
      "creators": [
        {
          "firstName": "Nicholas",
          "lastName": "Carlini",
          "creatorType": "author"
        },
        {
          "firstName": "Jamie",
          "lastName": "Hayes",
          "creatorType": "author"
        },
        {
          "firstName": "Milad",
          "lastName": "Nasr",
          "creatorType": "author"
        },
        {
          "firstName": "Matthew",
          "lastName": "Jagielski",
          "creatorType": "author"
        },
        {
          "firstName": "Vikash",
          "lastName": "Sehwag",
          "creatorType": "author"
        },
        {
          "firstName": "Florian",
          "lastName": "Tramèr",
          "creatorType": "author"
        },
        {
          "firstName": "Borja",
          "lastName": "Balle",
          "creatorType": "author"
        },
        {
          "firstName": "Daphne",
          "lastName": "Ippolito",
          "creatorType": "author"
        },
        {
          "firstName": "Eric",
          "lastName": "Wallace",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Machine Learning",
          "type": 1
        },
        {
          "tag": "Computer Science - Cryptography and Security",
          "type": 1
        },
        {
          "tag": "Computer Science - Computer Vision and Pattern Recognition",
          "type": 1
        }
      ],
      "relations": [],
      "dateAdded": "2023-04-11T03:59:24Z",
      "dateModified": "2023-04-11T03:59:24Z",
      "uri": "http://zotero.org/groups/4922950/items/SW84BDSG",
      "itemID": 3059,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "arXiv Fulltext PDF",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-04-11T04:03:49Z",
          "dateModified": "2023-04-11T04:03:49Z",
          "uri": "http://zotero.org/groups/4922950/items/KCAQU2WJ",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\KCAQU2WJ\\Carlini 等 - 2023 - Extracting Training Data from Diffusion Models.pdf",
          "select": "zotero://select/groups/4922950/items/KCAQU2WJ"
        },
        {
          "itemType": "attachment",
          "title": "arXiv.org Snapshot",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-04-11T04:04:04Z",
          "dateModified": "2023-04-11T04:04:04Z",
          "uri": "http://zotero.org/groups/4922950/items/NVZPQP2H",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\NVZPQP2H\\2301.html",
          "select": "zotero://select/groups/4922950/items/NVZPQP2H"
        }
      ],
      "notes": [],
      "citationKey": "carliniExtractingTrainingData2023",
      "itemKey": "SW84BDSG",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/SW84BDSG"
    },
    {
      "key": "85QW2GAE",
      "version": 11,
      "itemType": "preprint",
      "title": "MagNet and \"Efficient Defenses Against Adversarial Attacks\" are Not Robust to Adversarial Examples",
      "abstractNote": "MagNet and \"Efficient Defenses...\" were recently proposed as a defense to adversarial examples. We find that we can construct adversarial examples that defeat these defenses with only a slight increase in distortion.",
      "date": "2017-11-22",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/1711.08478",
      "accessDate": "2023-01-24T10:59:50Z",
      "extra": "arXiv:1711.08478 [cs]",
      "repository": "arXiv",
      "archiveID": "arXiv:1711.08478",
      "creators": [
        {
          "firstName": "Nicholas",
          "lastName": "Carlini",
          "creatorType": "author"
        },
        {
          "firstName": "David",
          "lastName": "Wagner",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Machine Learning",
          "type": 1
        },
        {
          "tag": "Computer Science - Cryptography and Security",
          "type": 1
        },
        {
          "tag": "Computer Science - Artificial Intelligence",
          "type": 1
        }
      ],
      "relations": [],
      "dateAdded": "2023-01-24T10:59:50Z",
      "dateModified": "2023-01-24T10:59:50Z",
      "uri": "http://zotero.org/groups/4922950/items/85QW2GAE",
      "itemID": 1753,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "arXiv Fulltext PDF",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-01-24T11:00:01Z",
          "dateModified": "2023-01-24T11:00:01Z",
          "uri": "http://zotero.org/groups/4922950/items/L3KJB65M",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\L3KJB65M\\Carlini 和 Wagner - 2017 - MagNet and Efficient Defenses Against Adversarial.pdf",
          "select": "zotero://select/groups/4922950/items/L3KJB65M"
        },
        {
          "itemType": "attachment",
          "title": "arXiv.org Snapshot",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-01-24T11:00:08Z",
          "dateModified": "2023-01-24T11:00:08Z",
          "uri": "http://zotero.org/groups/4922950/items/GUXUSLG7",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\GUXUSLG7\\1711.html",
          "select": "zotero://select/groups/4922950/items/GUXUSLG7"
        }
      ],
      "notes": [],
      "citationKey": "carliniMagNetEfficientDefenses2017",
      "itemKey": "85QW2GAE",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/85QW2GAE"
    },
    {
      "key": "PKP5U5BW",
      "version": 144,
      "itemType": "preprint",
      "title": "Poisoning and Backdooring Contrastive Learning",
      "abstractNote": "Multimodal contrastive learning methods like CLIP train on noisy and uncurated training datasets. This is cheaper than labeling datasets manually, and even improves out-of-distribution robustness. We show that this practice makes backdoor and poisoning attacks a significant threat. By poisoning just 0.01% of a dataset (e.g., just 300 images of the 3 million-example Conceptual Captions dataset), we can cause the model to misclassify test images by overlaying a small patch. Targeted poisoning attacks, whereby the model misclassifies a particular test input with an adversarially-desired label, are even easier requiring control of 0.0001% of the dataset (e.g., just three out of the 3 million images). Our attacks call into question whether training on noisy and uncurated Internet scrapes is desirable.",
      "date": "2022-03-28",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/2106.09667",
      "accessDate": "2023-01-31T12:05:33Z",
      "extra": "arXiv:2106.09667 [cs]",
      "DOI": "10.48550/arXiv.2106.09667",
      "repository": "arXiv",
      "archiveID": "arXiv:2106.09667",
      "creators": [
        {
          "firstName": "Nicholas",
          "lastName": "Carlini",
          "creatorType": "author"
        },
        {
          "firstName": "Andreas",
          "lastName": "Terzis",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Machine Learning",
          "type": 1
        }
      ],
      "relations": [],
      "dateAdded": "2023-01-31T12:05:33Z",
      "dateModified": "2023-01-31T12:05:33Z",
      "uri": "http://zotero.org/groups/4922950/items/PKP5U5BW",
      "itemID": 1839,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "arXiv Fulltext PDF",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-01-31T12:05:38Z",
          "dateModified": "2023-01-31T12:05:38Z",
          "uri": "http://zotero.org/groups/4922950/items/4J3TL475",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\4J3TL475\\Carlini 和 Terzis - 2022 - Poisoning and Backdooring Contrastive Learning.pdf",
          "select": "zotero://select/groups/4922950/items/4J3TL475"
        },
        {
          "itemType": "attachment",
          "title": "arXiv.org Snapshot",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-01-31T12:05:45Z",
          "dateModified": "2023-01-31T12:05:45Z",
          "uri": "http://zotero.org/groups/4922950/items/YA7K3N6E",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\YA7K3N6E\\2106.html",
          "select": "zotero://select/groups/4922950/items/YA7K3N6E"
        }
      ],
      "notes": [],
      "citationKey": "carliniPoisoningBackdooringContrastive2022",
      "itemKey": "PKP5U5BW",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/PKP5U5BW"
    },
    {
      "key": "R2IXSIIB",
      "version": 149,
      "itemType": "journalArticle",
      "title": "Poisoning the Unlabeled Dataset of Semi-Supervised Learning",
      "abstractNote": "Semi-supervised machine learning models learn from a (small) set of labeled training examples, and a (large) set of unlabeled training examples. State-of-the-art models can reach within a few percentage points of fully-supervised training, while requiring 100× less labeled data. We study a new class of vulnerabilities: poisoning attacks that modify the unlabeled dataset. In order to be useful, unlabeled datasets are given strictly less review than labeled datasets, and adversaries can therefore poison them easily. By inserting maliciously-crafted unlabeled examples totaling just 0.1% of the dataset size, we can manipulate a model trained on this poisoned dataset to misclassify arbitrary examples at test time (as any desired label). Our attacks are highly effective across datasets and semi-supervised learning methods. We ﬁnd that more accurate methods (thus more likely to be used) are signiﬁcantly more vulnerable to poisoning attacks, and as such better training methods are unlikely to prevent this attack. To counter this we explore the space of defenses, and propose two methods that mitigate our attack.",
      "language": "en",
      "libraryCatalog": "Zotero",
      "creators": [
        {
          "firstName": "Nicholas",
          "lastName": "Carlini",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": [],
      "dateAdded": "2023-01-31T12:10:54Z",
      "dateModified": "2023-01-31T12:10:54Z",
      "uri": "http://zotero.org/groups/4922950/items/R2IXSIIB",
      "itemID": 1843,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "Carlini - Poisoning the Unlabeled Dataset of Semi-Supervised.pdf",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-01-31T12:10:51Z",
          "dateModified": "2023-01-31T12:10:55Z",
          "uri": "http://zotero.org/groups/4922950/items/2K3Z3YXL",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\2K3Z3YXL\\Carlini - Poisoning the Unlabeled Dataset of Semi-Supervised.pdf",
          "select": "zotero://select/groups/4922950/items/2K3Z3YXL"
        }
      ],
      "notes": [],
      "citationKey": "carliniPoisoningUnlabeledDataset",
      "itemKey": "R2IXSIIB",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/R2IXSIIB"
    },
    {
      "key": "S86UYSSS",
      "version": 1216,
      "itemType": "preprint",
      "title": "Quantifying Memorization Across Neural Language Models",
      "abstractNote": "Large language models (LMs) have been shown to memorize parts of their training data, and when prompted appropriately, they will emit the memorized training data verbatim. This is undesirable because memorization violates privacy (exposing user data), degrades utility (repeated easy-to-memorize text is often low quality), and hurts fairness (some texts are memorized over others). We describe three log-linear relationships that quantify the degree to which LMs emit memorized training data. Memorization significantly grows as we increase (1) the capacity of a model, (2) the number of times an example has been duplicated, and (3) the number of tokens of context used to prompt the model. Surprisingly, we find the situation becomes more complicated when generalizing these results across model families. On the whole, we find that memorization in LMs is more prevalent than previously believed and will likely get worse as models continues to scale, at least without active mitigations.",
      "date": "2023-03-06",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/2202.07646",
      "accessDate": "2023-04-11T04:06:10Z",
      "extra": "arXiv:2202.07646 [cs]",
      "DOI": "10.48550/arXiv.2202.07646",
      "repository": "arXiv",
      "archiveID": "arXiv:2202.07646",
      "creators": [
        {
          "firstName": "Nicholas",
          "lastName": "Carlini",
          "creatorType": "author"
        },
        {
          "firstName": "Daphne",
          "lastName": "Ippolito",
          "creatorType": "author"
        },
        {
          "firstName": "Matthew",
          "lastName": "Jagielski",
          "creatorType": "author"
        },
        {
          "firstName": "Katherine",
          "lastName": "Lee",
          "creatorType": "author"
        },
        {
          "firstName": "Florian",
          "lastName": "Tramer",
          "creatorType": "author"
        },
        {
          "firstName": "Chiyuan",
          "lastName": "Zhang",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Machine Learning",
          "type": 1
        },
        {
          "tag": "Computer Science - Computation and Language",
          "type": 1
        }
      ],
      "relations": [],
      "dateAdded": "2023-04-11T04:06:10Z",
      "dateModified": "2023-04-11T04:06:10Z",
      "uri": "http://zotero.org/groups/4922950/items/S86UYSSS",
      "itemID": 3058,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "arXiv Fulltext PDF",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-04-11T04:06:15Z",
          "dateModified": "2023-04-11T04:06:15Z",
          "uri": "http://zotero.org/groups/4922950/items/F6SJ6EJM",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\F6SJ6EJM\\Carlini 等 - 2023 - Quantifying Memorization Across Neural Language Mo.pdf",
          "select": "zotero://select/groups/4922950/items/F6SJ6EJM"
        },
        {
          "itemType": "attachment",
          "title": "arXiv.org Snapshot",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-04-11T04:06:28Z",
          "dateModified": "2023-04-11T04:06:28Z",
          "uri": "http://zotero.org/groups/4922950/items/9VUISGUH",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\9VUISGUH\\2202.html",
          "select": "zotero://select/groups/4922950/items/9VUISGUH"
        }
      ],
      "notes": [],
      "citationKey": "carliniQuantifyingMemorizationNeural2023",
      "itemKey": "S86UYSSS",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/S86UYSSS"
    },
    {
      "key": "ANC6D3RF",
      "version": 1197,
      "itemType": "preprint",
      "title": "BadPre: Task-agnostic Backdoor Attacks to Pre-trained NLP Foundation Models",
      "abstractNote": "Pre-trained Natural Language Processing (NLP) models can be easily adapted to a variety of downstream language tasks. This significantly accelerates the development of language models. However, NLP models have been shown to be vulnerable to backdoor attacks, where a pre-defined trigger word in the input text causes model misprediction. Previous NLP backdoor attacks mainly focus on some specific tasks. This makes those attacks less general and applicable to other kinds of NLP models and tasks. In this work, we propose \\Name, the first task-agnostic backdoor attack against the pre-trained NLP models. The key feature of our attack is that the adversary does not need prior information about the downstream tasks when implanting the backdoor to the pre-trained model. When this malicious model is released, any downstream models transferred from it will also inherit the backdoor, even after the extensive transfer learning process. We further design a simple yet effective strategy to bypass a state-of-the-art defense. Experimental results indicate that our approach can compromise a wide range of downstream NLP tasks in an effective and stealthy way.",
      "date": "2021-10-05",
      "shortTitle": "BadPre",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/2110.02467",
      "accessDate": "2023-04-10T09:57:10Z",
      "extra": "arXiv:2110.02467 [cs]",
      "DOI": "10.48550/arXiv.2110.02467",
      "repository": "arXiv",
      "archiveID": "arXiv:2110.02467",
      "creators": [
        {
          "firstName": "Kangjie",
          "lastName": "Chen",
          "creatorType": "author"
        },
        {
          "firstName": "Yuxian",
          "lastName": "Meng",
          "creatorType": "author"
        },
        {
          "firstName": "Xiaofei",
          "lastName": "Sun",
          "creatorType": "author"
        },
        {
          "firstName": "Shangwei",
          "lastName": "Guo",
          "creatorType": "author"
        },
        {
          "firstName": "Tianwei",
          "lastName": "Zhang",
          "creatorType": "author"
        },
        {
          "firstName": "Jiwei",
          "lastName": "Li",
          "creatorType": "author"
        },
        {
          "firstName": "Chun",
          "lastName": "Fan",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Computation and Language",
          "type": 1
        },
        {
          "tag": "Computer Science - Artificial Intelligence",
          "type": 1
        }
      ],
      "relations": [],
      "dateAdded": "2023-04-10T09:57:10Z",
      "dateModified": "2023-04-10T09:57:14Z",
      "uri": "http://zotero.org/groups/4922950/items/ANC6D3RF",
      "itemID": 3061,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "arXiv Fulltext PDF",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-04-10T09:57:27Z",
          "dateModified": "2023-04-10T09:57:27Z",
          "uri": "http://zotero.org/groups/4922950/items/SJCEB4P6",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\SJCEB4P6\\Chen 等 - 2021 - BadPre Task-agnostic Backdoor Attacks to Pre-trai.pdf",
          "select": "zotero://select/groups/4922950/items/SJCEB4P6"
        },
        {
          "itemType": "attachment",
          "title": "arXiv.org Snapshot",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-04-10T09:57:47Z",
          "dateModified": "2023-04-10T09:57:47Z",
          "uri": "http://zotero.org/groups/4922950/items/2ZNDH9RY",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\2ZNDH9RY\\2110.html",
          "select": "zotero://select/groups/4922950/items/2ZNDH9RY"
        }
      ],
      "notes": [],
      "citationKey": "chenBadPreTaskagnosticBackdoor2021",
      "itemKey": "ANC6D3RF",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/ANC6D3RF"
    },
    {
      "key": "QVYUSFJ6",
      "version": 67,
      "itemType": "conferencePaper",
      "title": "Learning Security Classifiers with Verified Global Robustness Properties",
      "abstractNote": "Many recent works have proposed methods to train classi�ers with local robustness properties, which can provably eliminate classes of evasion attacks for most inputs, but not all inputs. Since data distribution shift is very common in security applications, e.g., often observed for malware detection, local robustness cannot guarantee that the property holds for unseen inputs at the time of deploying the classi�er. Therefore, it is more desirable to enforce global robustness properties that hold for all inputs, which is strictly stronger than local robustness. In this paper, we present a framework and tools for training classi�ers that satisfy global robustness properties. We de�ne new notions of global robustness that are more suitable for security classi�ers. We design a novel booster-�xer training framework to enforce global robustness properties. We structure our classi�er as an ensemble of logic rules and design a new veri�er to verify the properties. In our training algorithm, the booster increases the classi�er’s capacity, and the �xer enforces veri�ed global robustness properties following counterexample guided inductive synthesis. We show that we can train classi�ers to satisfy di�erent global robustness properties for three security datasets, and even multiple properties at the same time, with modest impact on the classi�er’s performance. For example, we train a Twitter spam account classi�er to satisfy �ve global robustness properties, with 5.4% decrease in true positive rate, and 0.1% increase in false positive rate, compared to a baseline XGBoost model that doesn’t satisfy any property.",
      "date": "2021-11-12",
      "language": "en",
      "libraryCatalog": "DOI.org (Crossref)",
      "url": "https://dl.acm.org/doi/10.1145/3460120.3484776",
      "accessDate": "2023-01-24T11:35:26Z",
      "place": "Virtual Event Republic of Korea",
      "publisher": "ACM",
      "ISBN": "978-1-4503-8454-4",
      "pages": "477-494",
      "proceedingsTitle": "Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security",
      "conferenceName": "CCS '21: 2021 ACM SIGSAC Conference on Computer and Communications Security",
      "DOI": "10.1145/3460120.3484776",
      "creators": [
        {
          "firstName": "Yizheng",
          "lastName": "Chen",
          "creatorType": "author"
        },
        {
          "firstName": "Shiqi",
          "lastName": "Wang",
          "creatorType": "author"
        },
        {
          "firstName": "Yue",
          "lastName": "Qin",
          "creatorType": "author"
        },
        {
          "firstName": "Xiaojing",
          "lastName": "Liao",
          "creatorType": "author"
        },
        {
          "firstName": "Suman",
          "lastName": "Jana",
          "creatorType": "author"
        },
        {
          "firstName": "David",
          "lastName": "Wagner",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": [],
      "dateAdded": "2023-01-24T11:35:26Z",
      "dateModified": "2023-01-24T11:35:26Z",
      "uri": "http://zotero.org/groups/4922950/items/QVYUSFJ6",
      "itemID": 1789,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "Chen 等 - 2021 - Learning Security Classifiers with Verified Global.pdf",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-01-24T11:35:21Z",
          "dateModified": "2023-01-24T11:35:27Z",
          "uri": "http://zotero.org/groups/4922950/items/I9ESPMP3",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\I9ESPMP3\\Chen 等 - 2021 - Learning Security Classifiers with Verified Global.pdf",
          "select": "zotero://select/groups/4922950/items/I9ESPMP3"
        }
      ],
      "notes": [],
      "citationKey": "chenLearningSecurityClassifiers2021",
      "itemKey": "QVYUSFJ6",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/QVYUSFJ6"
    },
    {
      "key": "9P2P8J3Z",
      "version": 36,
      "itemType": "conferencePaper",
      "title": "Stateful Detection of Black-Box Adversarial Attacks",
      "abstractNote": "The problem of adversarial examples, evasion attacks on machine learning classifiers, has proven extremely difficult to solve. This is true even in the black-box threat model, as is the case in many practical settings. Here, the classifier is hosted as a remote service and the adversary does not have direct access to the model parameters. This paper argues that in such settings, defenders have a larger space of actions than previously studied. Specifically, we deviate from the implicit assumption made by prior work that a defense must be a stateless function that operates on individual examples, and evaluate the space of stateful defenses.",
      "date": "2020-10-06",
      "language": "en",
      "libraryCatalog": "DOI.org (Crossref)",
      "url": "https://dl.acm.org/doi/10.1145/3385003.3410925",
      "accessDate": "2023-01-24T11:04:05Z",
      "place": "Taipei Taiwan",
      "publisher": "ACM",
      "ISBN": "978-1-4503-7611-2",
      "pages": "30-39",
      "proceedingsTitle": "Proceedings of the 1st ACM Workshop on Security and Privacy on Artificial Intelligence",
      "conferenceName": "ASIA CCS '20: The 15th ACM Asia Conference on Computer and Communications Security",
      "DOI": "10.1145/3385003.3410925",
      "creators": [
        {
          "firstName": "Steven",
          "lastName": "Chen",
          "creatorType": "author"
        },
        {
          "firstName": "Nicholas",
          "lastName": "Carlini",
          "creatorType": "author"
        },
        {
          "firstName": "David",
          "lastName": "Wagner",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": [],
      "dateAdded": "2023-01-24T11:04:05Z",
      "dateModified": "2023-01-24T11:04:05Z",
      "uri": "http://zotero.org/groups/4922950/items/9P2P8J3Z",
      "itemID": 1770,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "Chen 等 - 2020 - Stateful Detection of Black-Box Adversarial Attack.pdf",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-01-24T11:04:01Z",
          "dateModified": "2023-01-24T11:04:05Z",
          "uri": "http://zotero.org/groups/4922950/items/4HHNK458",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\4HHNK458\\Chen 等 - 2020 - Stateful Detection of Black-Box Adversarial Attack.pdf",
          "select": "zotero://select/groups/4922950/items/4HHNK458"
        }
      ],
      "notes": [],
      "citationKey": "chenStatefulDetectionBlackBox2020",
      "itemKey": "9P2P8J3Z",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/9P2P8J3Z"
    },
    {
      "key": "ID9T9QZI",
      "version": 1083,
      "itemType": "preprint",
      "title": "Unnoticeable Backdoor Attacks on Graph Neural Networks",
      "abstractNote": "Graph Neural Networks (GNNs) have achieved promising results in various tasks such as node classification and graph classification. Recent studies find that GNNs are vulnerable to adversarial attacks. However, effective backdoor attacks on graphs are still an open problem. In particular, backdoor attack poisons the graph by attaching triggers and the target class label to a set of nodes in the training graph. The backdoored GNNs trained on the poisoned graph will then be misled to predict test nodes to target class once attached with triggers. Though there are some initial efforts in graph backdoor attacks, our empirical analysis shows that they may require a large attack budget for effective backdoor attacks and the injected triggers can be easily detected and pruned. Therefore, in this paper, we study a novel problem of unnoticeable graph backdoor attacks with limited attack budget. To fully utilize the attack budget, we propose to deliberately select the nodes to inject triggers and target class labels in the poisoning phase. An adaptive trigger generator is deployed to obtain effective triggers that are difficult to be noticed. Extensive experiments on real-world datasets against various defense strategies demonstrate the effectiveness of our proposed method in conducting effective unnoticeable backdoor attacks.",
      "date": "2023-02-10",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/2303.01263",
      "accessDate": "2023-03-04T03:28:15Z",
      "extra": "arXiv:2303.01263 [cs]",
      "DOI": "10.1145/3543507.3583392",
      "creators": [
        {
          "firstName": "Enyan",
          "lastName": "Dai",
          "creatorType": "author"
        },
        {
          "firstName": "Minhua",
          "lastName": "Lin",
          "creatorType": "author"
        },
        {
          "firstName": "Xiang",
          "lastName": "Zhang",
          "creatorType": "author"
        },
        {
          "firstName": "Suhang",
          "lastName": "Wang",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Machine Learning",
          "type": 1
        },
        {
          "tag": "Computer Science - Cryptography and Security",
          "type": 1
        }
      ],
      "relations": [],
      "dateAdded": "2023-03-04T03:28:15Z",
      "dateModified": "2023-03-04T03:28:15Z",
      "uri": "http://zotero.org/groups/4922950/items/ID9T9QZI",
      "itemID": 2253,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "arXiv Fulltext PDF",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-03-04T03:28:19Z",
          "dateModified": "2023-03-04T03:28:19Z",
          "uri": "http://zotero.org/groups/4922950/items/X5FIC5VU",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\X5FIC5VU\\Dai 等 - 2023 - Unnoticeable Backdoor Attacks on Graph Neural Netw.pdf",
          "select": "zotero://select/groups/4922950/items/X5FIC5VU"
        },
        {
          "itemType": "attachment",
          "title": "arXiv.org Snapshot",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-03-04T03:28:27Z",
          "dateModified": "2023-03-04T03:28:27Z",
          "uri": "http://zotero.org/groups/4922950/items/VQR9A6S5",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\VQR9A6S5\\2303.html",
          "select": "zotero://select/groups/4922950/items/VQR9A6S5"
        }
      ],
      "notes": [],
      "citationKey": "daiUnnoticeableBackdoorAttacks2023",
      "itemKey": "ID9T9QZI",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/ID9T9QZI"
    },
    {
      "key": "SZPQNA5R",
      "version": 1460,
      "itemType": "preprint",
      "title": "Maximum-Entropy Fine-Grained Classification",
      "abstractNote": "Fine-Grained Visual Classification (FGVC) is an important computer vision problem that involves small diversity within the different classes, and often requires expert annotators to collect data. Utilizing this notion of small visual diversity, we revisit Maximum-Entropy learning in the context of fine-grained classification, and provide a training routine that maximizes the entropy of the output probability distribution for training convolutional neural networks on FGVC tasks. We provide a theoretical as well as empirical justification of our approach, and achieve state-of-the-art performance across a variety of classification tasks in FGVC, that can potentially be extended to any fine-tuning task. Our method is robust to different hyperparameter values, amount of training data and amount of training label noise and can hence be a valuable tool in many similar problems.",
      "date": "2018-09-16",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/1809.05934",
      "accessDate": "2023-04-14T03:30:12Z",
      "extra": "arXiv:1809.05934 [cs]\nversion: 1",
      "repository": "arXiv",
      "archiveID": "arXiv:1809.05934",
      "creators": [
        {
          "firstName": "Abhimanyu",
          "lastName": "Dubey",
          "creatorType": "author"
        },
        {
          "firstName": "Otkrist",
          "lastName": "Gupta",
          "creatorType": "author"
        },
        {
          "firstName": "Ramesh",
          "lastName": "Raskar",
          "creatorType": "author"
        },
        {
          "firstName": "Nikhil",
          "lastName": "Naik",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Machine Learning",
          "type": 1
        },
        {
          "tag": "Computer Science - Computer Vision and Pattern Recognition",
          "type": 1
        }
      ],
      "relations": [],
      "dateAdded": "2023-04-14T03:30:12Z",
      "dateModified": "2023-04-14T03:30:12Z",
      "uri": "http://zotero.org/groups/4922950/items/SZPQNA5R",
      "itemID": 3296,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "arXiv Fulltext PDF",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-04-14T03:30:18Z",
          "dateModified": "2023-04-14T03:30:18Z",
          "uri": "http://zotero.org/groups/4922950/items/NKB5F2LJ",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\NKB5F2LJ\\Dubey 等 - 2018 - Maximum-Entropy Fine-Grained Classification.pdf",
          "select": "zotero://select/groups/4922950/items/NKB5F2LJ"
        },
        {
          "itemType": "attachment",
          "title": "arXiv.org Snapshot",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-04-14T03:30:26Z",
          "dateModified": "2023-04-14T03:30:26Z",
          "uri": "http://zotero.org/groups/4922950/items/CTXAEFRC",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\CTXAEFRC\\1809.html",
          "select": "zotero://select/groups/4922950/items/CTXAEFRC"
        }
      ],
      "notes": [
        {
          "key": "W7D5XZRU",
          "version": 1459,
          "itemType": "note",
          "parentItem": "SZPQNA5R",
          "note": "Comment: Camera-ready, accepted to NIPS 2018, v2 has minor typo updates and small changes in text",
          "tags": [],
          "relations": {},
          "dateAdded": "2023-04-14T03:30:12Z",
          "dateModified": "2023-04-14T03:30:12Z",
          "uri": "http://zotero.org/groups/4922950/items/W7D5XZRU"
        }
      ],
      "citationKey": "dubeyMaximumEntropyFineGrainedClassification2018",
      "itemKey": "SZPQNA5R",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/SZPQNA5R"
    },
    {
      "key": "VGP4STRZ",
      "version": 106,
      "itemType": "preprint",
      "title": "Adversarial Reprogramming of Neural Networks",
      "abstractNote": "Deep neural networks are susceptible to \\emph{adversarial} attacks. In computer vision, well-crafted perturbations to images can cause neural networks to make mistakes such as confusing a cat with a computer. Previous adversarial attacks have been designed to degrade performance of models or cause machine learning models to produce specific outputs chosen ahead of time by the attacker. We introduce attacks that instead {\\em reprogram} the target model to perform a task chosen by the attacker---without the attacker needing to specify or compute the desired output for each test-time input. This attack finds a single adversarial perturbation, that can be added to all test-time inputs to a machine learning model in order to cause the model to perform a task chosen by the adversary---even if the model was not trained to do this task. These perturbations can thus be considered a program for the new task. We demonstrate adversarial reprogramming on six ImageNet classification models, repurposing these models to perform a counting task, as well as classification tasks: classification of MNIST and CIFAR-10 examples presented as inputs to the ImageNet model.",
      "date": "2018-11-29",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/1806.11146",
      "accessDate": "2023-01-29T14:45:15Z",
      "extra": "arXiv:1806.11146 [cs, stat]",
      "DOI": "10.48550/arXiv.1806.11146",
      "repository": "arXiv",
      "archiveID": "arXiv:1806.11146",
      "creators": [
        {
          "firstName": "Gamaleldin F.",
          "lastName": "Elsayed",
          "creatorType": "author"
        },
        {
          "firstName": "Ian",
          "lastName": "Goodfellow",
          "creatorType": "author"
        },
        {
          "firstName": "Jascha",
          "lastName": "Sohl-Dickstein",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Machine Learning",
          "type": 1
        },
        {
          "tag": "Statistics - Machine Learning",
          "type": 1
        },
        {
          "tag": "Computer Science - Cryptography and Security",
          "type": 1
        },
        {
          "tag": "Computer Science - Computer Vision and Pattern Recognition",
          "type": 1
        }
      ],
      "relations": [],
      "dateAdded": "2023-01-29T14:45:15Z",
      "dateModified": "2023-01-29T14:45:15Z",
      "uri": "http://zotero.org/groups/4922950/items/VGP4STRZ",
      "itemID": 1807,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "arXiv Fulltext PDF",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-01-29T14:48:59Z",
          "dateModified": "2023-01-29T14:48:59Z",
          "uri": "http://zotero.org/groups/4922950/items/9697QQNQ",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\9697QQNQ\\Elsayed 等 - 2018 - Adversarial Reprogramming of Neural Networks.pdf",
          "select": "zotero://select/groups/4922950/items/9697QQNQ"
        },
        {
          "itemType": "attachment",
          "title": "arXiv.org Snapshot",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-01-29T14:49:05Z",
          "dateModified": "2023-01-29T14:49:05Z",
          "uri": "http://zotero.org/groups/4922950/items/48D37EJF",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\48D37EJF\\1806.html",
          "select": "zotero://select/groups/4922950/items/48D37EJF"
        }
      ],
      "notes": [],
      "citationKey": "elsayedAdversarialReprogrammingNeural2018",
      "itemKey": "VGP4STRZ",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/VGP4STRZ"
    },
    {
      "key": "YD8NR5DF",
      "version": 1640,
      "itemType": "preprint",
      "title": "[Extended version] Rethinking Deep Neural Network Ownership Verification: Embedding Passports to Defeat Ambiguity Attacks",
      "abstractNote": "With substantial amount of time, resources and human (team) efforts invested to explore and develop successful deep neural networks (DNN), there emerges an urgent need to protect these inventions from being illegally copied, redistributed, or abused without respecting the intellectual properties of legitimate owners. Following recent progresses along this line, we investigate a number of watermark-based DNN ownership verification methods in the face of ambiguity attacks, which aim to cast doubts on the ownership verification by forging counterfeit watermarks. It is shown that ambiguity attacks pose serious threats to existing DNN watermarking methods. As remedies to the above-mentioned loophole, this paper proposes novel passport-based DNN ownership verification schemes which are both robust to network modifications and resilient to ambiguity attacks. The gist of embedding digital passports is to design and train DNN models in a way such that, the DNN inference performance of an original task will be significantly deteriorated due to forged passports. In other words, genuine passports are not only verified by looking for the predefined signatures, but also reasserted by the unyielding DNN model inference performances. Extensive experimental results justify the effectiveness of the proposed passport-based DNN ownership verification schemes. Code and models are available at https://github.com/kamwoh/DeepIPR",
      "date": "2019-11-02",
      "shortTitle": "[Extended version] Rethinking Deep Neural Network Ownership Verification",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/1909.07830",
      "accessDate": "2023-04-19T05:33:38Z",
      "extra": "arXiv:1909.07830 [cs]",
      "DOI": "10.48550/arXiv.1909.07830",
      "repository": "arXiv",
      "archiveID": "arXiv:1909.07830",
      "creators": [
        {
          "firstName": "Lixin",
          "lastName": "Fan",
          "creatorType": "author"
        },
        {
          "firstName": "Kam Woh",
          "lastName": "Ng",
          "creatorType": "author"
        },
        {
          "firstName": "Chee Seng",
          "lastName": "Chan",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Machine Learning",
          "type": 1
        },
        {
          "tag": "Computer Science - Cryptography and Security",
          "type": 1
        },
        {
          "tag": "Computer Science - Computer Vision and Pattern Recognition",
          "type": 1
        }
      ],
      "relations": [],
      "dateAdded": "2023-04-19T05:33:38Z",
      "dateModified": "2023-04-19T05:33:38Z",
      "uri": "http://zotero.org/groups/4922950/items/YD8NR5DF",
      "itemID": 3481,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "全文",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-04-19T05:36:20Z",
          "dateModified": "2023-04-19T05:36:20Z",
          "uri": "http://zotero.org/groups/4922950/items/H9EHWLZF",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\H9EHWLZF\\Fan 等 - 2019 - [Extended version] Rethinking Deep Neural Network .pdf",
          "select": "zotero://select/groups/4922950/items/H9EHWLZF"
        },
        {
          "itemType": "attachment",
          "title": "arXiv Fulltext PDF",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-04-19T05:35:49Z",
          "dateModified": "2023-04-19T05:35:49Z",
          "uri": "http://zotero.org/groups/4922950/items/I2MSNHNR",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\I2MSNHNR\\Fan 等 - 2019 - [Extended version] Rethinking Deep Neural Network .pdf",
          "select": "zotero://select/groups/4922950/items/I2MSNHNR"
        },
        {
          "itemType": "attachment",
          "title": "arXiv.org Snapshot",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-04-19T05:38:58Z",
          "dateModified": "2023-04-19T05:38:58Z",
          "uri": "http://zotero.org/groups/4922950/items/LPT3LNQD",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\LPT3LNQD\\1909.html",
          "select": "zotero://select/groups/4922950/items/LPT3LNQD"
        }
      ],
      "notes": [
        {
          "key": "PSV2Z2CM",
          "version": 1640,
          "itemType": "note",
          "parentItem": "YD8NR5DF",
          "note": "Comment: This paper is accepted by NeurIPS 2019; Our code is available at https://github.com/kamwoh/DeepIPR. This is the extended version",
          "tags": [],
          "relations": {},
          "dateAdded": "2023-04-19T05:33:38Z",
          "dateModified": "2023-04-19T05:33:38Z",
          "uri": "http://zotero.org/groups/4922950/items/PSV2Z2CM"
        }
      ],
      "citationKey": "fanExtendedVersionRethinking2019",
      "itemKey": "YD8NR5DF",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/YD8NR5DF"
    },
    {
      "key": "XUQHMAUI",
      "version": 1598,
      "itemType": "preprint",
      "title": "The Stable Signature: Rooting Watermarks in Latent Diffusion Models",
      "abstractNote": "Generative image modeling enables a wide range of applications but raises ethical concerns about responsible deployment. This paper introduces an active strategy combining image watermarking and Latent Diffusion Models. The goal is for all generated images to conceal an invisible watermark allowing for future detection and/or identification. The method quickly fine-tunes the latent decoder of the image generator, conditioned on a binary signature. A pre-trained watermark extractor recovers the hidden signature from any generated image and a statistical test then determines whether it comes from the generative model. We evaluate the invisibility and robustness of the watermarks on a variety of generation tasks, showing that Stable Signature works even after the images are modified. For instance, it detects the origin of an image generated from a text prompt, then cropped to keep $10\\%$ of the content, with $90$+$\\%$ accuracy at a false positive rate below 10$^{-6}$.",
      "date": "2023-03-27",
      "shortTitle": "The Stable Signature",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/2303.15435",
      "accessDate": "2023-04-14T08:32:02Z",
      "extra": "arXiv:2303.15435 [cs]",
      "DOI": "10.48550/arXiv.2303.15435",
      "repository": "arXiv",
      "archiveID": "arXiv:2303.15435",
      "creators": [
        {
          "firstName": "Pierre",
          "lastName": "Fernandez",
          "creatorType": "author"
        },
        {
          "firstName": "Guillaume",
          "lastName": "Couairon",
          "creatorType": "author"
        },
        {
          "firstName": "Hervé",
          "lastName": "Jégou",
          "creatorType": "author"
        },
        {
          "firstName": "Matthijs",
          "lastName": "Douze",
          "creatorType": "author"
        },
        {
          "firstName": "Teddy",
          "lastName": "Furon",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Artificial Intelligence",
          "type": 1
        },
        {
          "tag": "Computer Science - Computer Vision and Pattern Recognition",
          "type": 1
        }
      ],
      "relations": [],
      "dateAdded": "2023-04-14T08:32:02Z",
      "dateModified": "2023-04-14T08:32:02Z",
      "uri": "http://zotero.org/groups/4922950/items/XUQHMAUI",
      "itemID": 3310,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "arXiv Fulltext PDF",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-04-14T08:32:14Z",
          "dateModified": "2023-04-14T08:32:14Z",
          "uri": "http://zotero.org/groups/4922950/items/CVW53IEF",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\CVW53IEF\\Fernandez 等 - 2023 - The Stable Signature Rooting Watermarks in Latent.pdf",
          "select": "zotero://select/groups/4922950/items/CVW53IEF"
        },
        {
          "itemType": "attachment",
          "title": "arXiv.org Snapshot",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-04-14T08:32:23Z",
          "dateModified": "2023-04-14T08:32:23Z",
          "uri": "http://zotero.org/groups/4922950/items/EHCWBRGH",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\EHCWBRGH\\2303.html",
          "select": "zotero://select/groups/4922950/items/EHCWBRGH"
        }
      ],
      "notes": [
        {
          "key": "VQB3BURK",
          "version": 1475,
          "itemType": "note",
          "parentItem": "XUQHMAUI",
          "note": "Comment: Website at https://pierrefdz.github.io/publications/stablesignature",
          "tags": [],
          "relations": {},
          "dateAdded": "2023-04-14T08:32:02Z",
          "dateModified": "2023-04-14T08:32:02Z",
          "uri": "http://zotero.org/groups/4922950/items/VQB3BURK"
        },
        {
          "key": "FUD2ILJY",
          "version": 1812,
          "itemType": "note",
          "parentItem": "XUQHMAUI",
          "note": "<div data-citation-items=\"%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FXUQHMAUI%22%5D%2C%22itemData%22%3A%7B%22id%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FXUQHMAUI%22%2C%22type%22%3A%22article%22%2C%22abstract%22%3A%22Generative%20image%20modeling%20enables%20a%20wide%20range%20of%20applications%20but%20raises%20ethical%20concerns%20about%20responsible%20deployment.%20This%20paper%20introduces%20an%20active%20strategy%20combining%20image%20watermarking%20and%20Latent%20Diffusion%20Models.%20The%20goal%20is%20for%20all%20generated%20images%20to%20conceal%20an%20invisible%20watermark%20allowing%20for%20future%20detection%20and%2For%20identification.%20The%20method%20quickly%20fine-tunes%20the%20latent%20decoder%20of%20the%20image%20generator%2C%20conditioned%20on%20a%20binary%20signature.%20A%20pre-trained%20watermark%20extractor%20recovers%20the%20hidden%20signature%20from%20any%20generated%20image%20and%20a%20statistical%20test%20then%20determines%20whether%20it%20comes%20from%20the%20generative%20model.%20We%20evaluate%20the%20invisibility%20and%20robustness%20of%20the%20watermarks%20on%20a%20variety%20of%20generation%20tasks%2C%20showing%20that%20Stable%20Signature%20works%20even%20after%20the%20images%20are%20modified.%20For%20instance%2C%20it%20detects%20the%20origin%20of%20an%20image%20generated%20from%20a%20text%20prompt%2C%20then%20cropped%20to%20keep%20%2410%5C%5C%25%24%20of%20the%20content%2C%20with%20%2490%24%2B%24%5C%5C%25%24%20accuracy%20at%20a%20false%20positive%20rate%20below%2010%24%5E%7B-6%7D%24.%22%2C%22DOI%22%3A%2210.48550%2FarXiv.2303.15435%22%2C%22note%22%3A%22arXiv%3A2303.15435%20%5Bcs%5D%22%2C%22number%22%3A%22arXiv%3A2303.15435%22%2C%22publisher%22%3A%22arXiv%22%2C%22source%22%3A%22arXiv.org%22%2C%22title%22%3A%22The%20Stable%20Signature%3A%20Rooting%20Watermarks%20in%20Latent%20Diffusion%20Models%22%2C%22title-short%22%3A%22The%20Stable%20Signature%22%2C%22URL%22%3A%22http%3A%2F%2Farxiv.org%2Fabs%2F2303.15435%22%2C%22author%22%3A%5B%7B%22family%22%3A%22Fernandez%22%2C%22given%22%3A%22Pierre%22%7D%2C%7B%22family%22%3A%22Couairon%22%2C%22given%22%3A%22Guillaume%22%7D%2C%7B%22family%22%3A%22J%C3%A9gou%22%2C%22given%22%3A%22Herv%C3%A9%22%7D%2C%7B%22family%22%3A%22Douze%22%2C%22given%22%3A%22Matthijs%22%7D%2C%7B%22family%22%3A%22Furon%22%2C%22given%22%3A%22Teddy%22%7D%5D%2C%22accessed%22%3A%7B%22date-parts%22%3A%5B%5B%222023%22%2C4%2C14%5D%5D%7D%2C%22issued%22%3A%7B%22date-parts%22%3A%5B%5B%222023%22%2C3%2C27%5D%5D%7D%2C%22citation-key%22%3A%22fernandezStableSignatureRooting2023%22%7D%7D%5D\" data-schema-version=\"9\"><h1>Method</h1>\n<p>分为两个步骤，训练水印提取器以及finetuning</p>\n<h2>Part1</h2>\n<p></p>\n<p></p>\n<p></p>\n<p>基于一个基本的watermark框架，名称为：</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FCVW53IEF%22%2C%22annotationKey%22%3A%22N928PATA%22%2C%22color%22%3A%22%235fb236%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B95.252%2C588.906%2C131.217%2C597.813%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FXUQHMAUI%22%5D%2C%22locator%22%3A%223%22%7D%7D\">“HiDDeN”</span> <span class=\"citation\" data-citation=\"%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FXUQHMAUI%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D\">(<span class=\"citation-item\">Fernandez 等, 2023, p. 3</span>)</span></p>\n<p></p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FCVW53IEF%22%2C%22annotationKey%22%3A%226HV2BNC4%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B158.136%2C576.951%2C286.365%2C585.858%5D%2C%5B50.112%2C564.301%2C257.577%2C574.62%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FXUQHMAUI%22%5D%2C%22locator%22%3A%223%22%7D%7D\">“jointly optimizes the parameters of watermark encoder WE and extractor network W”</span> <span class=\"citation\" data-citation=\"%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FXUQHMAUI%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D\">(<span class=\"citation-item\">Fernandez 等, 2023, p. 3</span>)</span> 同时训练产生水印的 encoder 以及提取水印的神经网络</p>\n<p><img alt=\"\" data-attachment-key=\"5MTX7ZMH\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FCVW53IEF%22%2C%22annotationKey%22%3A%22RMMUVFCX%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B42%2C379%2C293%2C528%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FXUQHMAUI%22%5D%2C%22locator%22%3A%223%22%7D%7D\" width=\"418\" height=\"248\"><br><span class=\"citation\" data-citation=\"%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FXUQHMAUI%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D\">(<span class=\"citation-item\">Fernandez 等, 2023, p. 3</span>)</span></p>\n<p>首先对于encoder，输入一张图片x以及签名m，输出一个一样的小的带水印图片（实际上是产生residual然后再相加），然后对这个图片进行数据增强，再给extractor进行训练，提取出m’</p>\n<p><img alt=\"\" data-attachment-key=\"AF23WGHI\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FCVW53IEF%22%2C%22annotationKey%22%3A%22D638KKFB%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B50.5%2C104%2C246.5%2C202%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FXUQHMAUI%22%5D%2C%22locator%22%3A%223%22%7D%7D\" width=\"327\" height=\"164\"><br><span class=\"citation\" data-citation=\"%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FXUQHMAUI%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D\">(<span class=\"citation-item\">Fernandez 等, 2023, p. 3</span>)</span></p>\n<p>这里在m’上额外套用了一个<span class=\"math\">$\\sigma$</span>函数，意思可能是作为sign函数把m’变为标准的二进制01编码</p>\n<p>然后要求m与m’尽可能的接近，学习的损失函数如下：</p>\n<p><img alt=\"\" data-attachment-key=\"8ZGULYCP\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FCVW53IEF%22%2C%22annotationKey%22%3A%22WCB3V89M%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B40%2C341.5%2C294.5%2C385%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FXUQHMAUI%22%5D%2C%22locator%22%3A%223%22%7D%7D\" width=\"424\" height=\"72\"><br><span class=\"citation\" data-citation=\"%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FXUQHMAUI%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D\">(<span class=\"citation-item\">Fernandez 等, 2023, p. 3</span>)</span></p>\n<p></p>\n<hr>\n<h2>Part2</h2>\n<p>接下来是对Diffusion Model进行 fine-tuning，注意这里只是对stable diffusion过程中的decoder进行微调，对整体过程没什么印象，这样即方便又能保证性能</p>\n<p>这里的损失函数又分为两个部分</p>\n<p><img alt=\"\" data-attachment-key=\"QNVMPAZ6\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FCVW53IEF%22%2C%22annotationKey%22%3A%22YMBLEFWK%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B379%2C287%2C476.5%2C313.5%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FXUQHMAUI%22%5D%2C%22locator%22%3A%223%22%7D%7D\" width=\"163\" height=\"44\"><br><span class=\"citation\" data-citation=\"%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FXUQHMAUI%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D\">(<span class=\"citation-item\">Fernandez 等, 2023, p. 3</span>)</span></p>\n<p></p>\n<p><img alt=\"\" data-attachment-key=\"K66VREZG\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FCVW53IEF%22%2C%22annotationKey%22%3A%22VSGSKF6F%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B298.5%2C416.5%2C556%2C493%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FXUQHMAUI%22%5D%2C%22locator%22%3A%223%22%7D%7D\" width=\"429\" height=\"127\"><br><span class=\"citation\" data-citation=\"%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FXUQHMAUI%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D\">(<span class=\"citation-item\">Fernandez 等, 2023, p. 3</span>)</span></p>\n<p>第一个损失函数负责水印，</p>\n<ol>\n<li>\n<span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FCVW53IEF%22%2C%22annotationKey%22%3A%22DP46HYM9%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B506.538%2C479.866%2C545.113%2C491.202%5D%2C%5B308.862%2C467.91%2C494.503%2C479.247%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FXUQHMAUI%22%5D%2C%22locator%22%3A%223%22%7D%7D\">“LDM encoder E that outputs activation map z = E(x)”</span> <span class=\"citation\" data-citation=\"%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FXUQHMAUI%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D\">(<span class=\"citation-item\">Fernandez 等, 2023, p. 3</span>)</span> 图片被autoencoder放入latent space\n</li>\n<li>\ndecoder重新解压这张图片（其实这个过程只涉及autoencoder 和 diffusion过程无关？）\n</li>\n<li>\nextractor提取这个decoder生成的图中的签名，然后通过<span class=\"math\">$\\sigma$</span>函数\n</li>\n<li>\n损失函数要求这个签名与预先人为对这个模型设计的水印要约接近越好\n</li>\n</ol>\n<p>Decoder完全不需要之前的watermark encoder，只需要对这个损失函数进行微调就可以了！</p>\n<p></p>\n<p>第二个损失函数负责保证图片质量，其实也就是保证微调后的解码器要与解码器最初的效果接近</p>\n<p>这里第二个损失函数就用解码器的输出x’与原始解码器的输出进行比较</p>\n<p></p>\n<p><img alt=\"\" data-attachment-key=\"NB7E3YZI\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FCVW53IEF%22%2C%22annotationKey%22%3A%22E4UDQQNN%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B247.5%2C104.5%2C436%2C201.5%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FXUQHMAUI%22%5D%2C%22locator%22%3A%223%22%7D%7D\" width=\"314\" height=\"162\"><br><span class=\"citation\" data-citation=\"%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FXUQHMAUI%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D\">(<span class=\"citation-item\">Fernandez 等, 2023, p. 3</span>)</span></p>\n<hr>\n<p>特别注意，在fine-tuning的阶段没有通过变换来数据增强，但整个检测仍然具有鲁棒性，这是因为在之前的训练中 extractor就已经足够具有鲁棒性了？但如果这里再次把Decoder输出的图进行变换，然后再进入extractor然后计算损失函数，是否可以更有效？</p>\n<p></p>\n<p>还有个致命问题是，如果攻击者知道了W（白箱），对图片进行对抗攻击，那么W还是没有鲁棒性</p>\n<p>会不会是之前的数据增强只是普通的变换而不是更强的对抗训练？</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FCVW53IEF%22%2C%22annotationKey%22%3A%22TQQ3K6BP%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%227%22%2C%22position%22%3A%7B%22pageIndex%22%3A6%2C%22rects%22%3A%5B%5B134.216%2C174.489%2C286.365%2C183.396%5D%2C%5B50.112%2C162.534%2C286.365%2C171.441%5D%2C%5B50.112%2C150.579%2C230.734%2C159.486%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FXUQHMAUI%22%5D%2C%22locator%22%3A%227%22%7D%7D\">“watermark’s resistance to intentional tampering, as opposed to distortions that happen without bad intentions like crops or compression”</span> <span class=\"citation\" data-citation=\"%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FXUQHMAUI%22%5D%2C%22locator%22%3A%227%22%7D%5D%2C%22properties%22%3A%7B%7D%7D\">(<span class=\"citation-item\">Fernandez 等, 2023, p. 7</span>)</span></p>\n<p>作者自己也是把这种恶意的改造图片和无意的剪裁压缩分开，那会不会是之前transformation不够好？</p>\n<p></p>\n<p>而对于更改模型的攻击，作者讨论了两种</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FCVW53IEF%22%2C%22annotationKey%22%3A%22B5LDZQHU%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%228%22%2C%22position%22%3A%7B%22pageIndex%22%3A7%2C%22rects%22%3A%5B%5B280.826%2C370.463%2C286.365%2C379.37%5D%2C%5B50.112%2C358.507%2C286.365%2C367.414%5D%2C%5B50.112%2C346.552%2C286.365%2C355.459%5D%2C%5B50.112%2C334.597%2C139.217%2C343.504%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FXUQHMAUI%22%5D%2C%22locator%22%3A%228%22%7D%7D\">“it is difficult to significantly reduce the bit accuracy without compromising the image quality: artifacts start to appear during the purification”</span> <span class=\"citation\" data-citation=\"%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FXUQHMAUI%22%5D%2C%22locator%22%3A%228%22%7D%5D%2C%22properties%22%3A%7B%7D%7D\">(<span class=\"citation-item\">Fernandez 等, 2023, p. 8</span>)</span></p>\n<p>前者貌似还不错</p>\n<p>但这类文章，是不是都对于攻击只是用实验来简单讨论，似乎都没有更深入的探讨</p>\n<p></p>\n</div>",
          "tags": [],
          "relations": {},
          "dateAdded": "2023-04-16T08:47:58Z",
          "dateModified": "2023-04-23T15:46:07Z",
          "uri": "http://zotero.org/groups/4922950/items/FUD2ILJY"
        }
      ],
      "citationKey": "fernandezStableSignatureRooting2023",
      "itemKey": "XUQHMAUI",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/XUQHMAUI"
    },
    {
      "key": "JMVNRGV9",
      "version": 616,
      "itemType": "preprint",
      "title": "Explaining and Harnessing Adversarial Examples",
      "abstractNote": "Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",
      "date": "2015-03-20",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/1412.6572",
      "accessDate": "2023-01-31T12:16:22Z",
      "extra": "arXiv:1412.6572 [cs, stat]",
      "DOI": "10.48550/arXiv.1412.6572",
      "repository": "arXiv",
      "archiveID": "arXiv:1412.6572",
      "creators": [
        {
          "firstName": "Ian J.",
          "lastName": "Goodfellow",
          "creatorType": "author"
        },
        {
          "firstName": "Jonathon",
          "lastName": "Shlens",
          "creatorType": "author"
        },
        {
          "firstName": "Christian",
          "lastName": "Szegedy",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Machine Learning",
          "type": 1
        },
        {
          "tag": "Statistics - Machine Learning",
          "type": 1
        }
      ],
      "relations": [],
      "dateAdded": "2023-01-31T12:16:22Z",
      "dateModified": "2023-01-31T12:16:22Z",
      "uri": "http://zotero.org/groups/4922950/items/JMVNRGV9",
      "itemID": 1849,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "arXiv Fulltext PDF",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-01-31T12:16:34Z",
          "dateModified": "2023-01-31T12:16:34Z",
          "uri": "http://zotero.org/groups/4922950/items/6LYY4TFL",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\6LYY4TFL\\Goodfellow 等 - 2015 - Explaining and Harnessing Adversarial Examples.pdf",
          "select": "zotero://select/groups/4922950/items/6LYY4TFL"
        },
        {
          "itemType": "attachment",
          "title": "arXiv.org Snapshot",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-01-31T12:16:42Z",
          "dateModified": "2023-01-31T12:16:42Z",
          "uri": "http://zotero.org/groups/4922950/items/4YLIXWYG",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\4YLIXWYG\\1412.html",
          "select": "zotero://select/groups/4922950/items/4YLIXWYG"
        }
      ],
      "notes": [
        {
          "key": "25BLW6DN",
          "version": 445,
          "itemType": "note",
          "parentItem": "JMVNRGV9",
          "note": "<div data-citation-items=\"%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FJMVNRGV9%22%5D%2C%22itemData%22%3A%7B%22id%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FJMVNRGV9%22%2C%22type%22%3A%22article%22%2C%22abstract%22%3A%22Several%20machine%20learning%20models%2C%20including%20neural%20networks%2C%20consistently%20misclassify%20adversarial%20examples---inputs%20formed%20by%20applying%20small%20but%20intentionally%20worst-case%20perturbations%20to%20examples%20from%20the%20dataset%2C%20such%20that%20the%20perturbed%20input%20results%20in%20the%20model%20outputting%20an%20incorrect%20answer%20with%20high%20confidence.%20Early%20attempts%20at%20explaining%20this%20phenomenon%20focused%20on%20nonlinearity%20and%20overfitting.%20We%20argue%20instead%20that%20the%20primary%20cause%20of%20neural%20networks'%20vulnerability%20to%20adversarial%20perturbation%20is%20their%20linear%20nature.%20This%20explanation%20is%20supported%20by%20new%20quantitative%20results%20while%20giving%20the%20first%20explanation%20of%20the%20most%20intriguing%20fact%20about%20them%3A%20their%20generalization%20across%20architectures%20and%20training%20sets.%20Moreover%2C%20this%20view%20yields%20a%20simple%20and%20fast%20method%20of%20generating%20adversarial%20examples.%20Using%20this%20approach%20to%20provide%20examples%20for%20adversarial%20training%2C%20we%20reduce%20the%20test%20set%20error%20of%20a%20maxout%20network%20on%20the%20MNIST%20dataset.%22%2C%22DOI%22%3A%2210.48550%2FarXiv.1412.6572%22%2C%22note%22%3A%22arXiv%3A1412.6572%20%5Bcs%2C%20stat%5D%22%2C%22number%22%3A%22arXiv%3A1412.6572%22%2C%22publisher%22%3A%22arXiv%22%2C%22source%22%3A%22arXiv.org%22%2C%22title%22%3A%22Explaining%20and%20Harnessing%20Adversarial%20Examples%22%2C%22URL%22%3A%22http%3A%2F%2Farxiv.org%2Fabs%2F1412.6572%22%2C%22author%22%3A%5B%7B%22family%22%3A%22Goodfellow%22%2C%22given%22%3A%22Ian%20J.%22%7D%2C%7B%22family%22%3A%22Shlens%22%2C%22given%22%3A%22Jonathon%22%7D%2C%7B%22family%22%3A%22Szegedy%22%2C%22given%22%3A%22Christian%22%7D%5D%2C%22accessed%22%3A%7B%22date-parts%22%3A%5B%5B%222023%22%2C1%2C31%5D%5D%7D%2C%22issued%22%3A%7B%22date-parts%22%3A%5B%5B%222015%22%2C3%2C20%5D%5D%7D%7D%7D%5D\" data-schema-version=\"9\"><h1>abstract</h1>\n<p>原来关于对抗样本的讨论集中于非线性以及过拟合，而作者提出神经网络的脆弱性在于其线性的本质</p>\n<p><span style=\"background-color: #2ea8e580\">或许可以看看not bug features这篇文章</span></p>\n<h1>intro</h1>\n<p>开篇就承接<span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2F6LYY4TFL%22%2C%22pageLabel%22%3A%2210%22%2C%22position%22%3A%7B%22pageIndex%22%3A9%2C%22rects%22%3A%5B%5B186.888%2C346.965%2C333.865%2C354.981%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FJMVNRGV9%22%5D%2C%22locator%22%3A%2210%22%7D%7D\">“Intriguing properties of neural networks”</span>这篇论文。</p>\n<p>是有这项工作来发现神经网络对对抗样本非常脆弱，而且这种对抗样本的在多个模型上有效。这体现出了机器学习算法存在的基本性问题</p>\n<p><span style=\"background-color: #5fb23680\">开篇通过前人的论文来提出这个问题，然后对于这个问题有开始分析几种假说</span></p>\n<p>作者提出，这些假说都没啥必要。因为带线性的性质就足以产生对抗样本的问题了</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2F6LYY4TFL%22%2C%22annotationKey%22%3A%22NL92VXDW%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%221%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B298.196%2C300.729%2C504.003%2C309.636%5D%2C%5B108%2C289.77%2C258.963%2C298.677%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FJMVNRGV9%22%5D%2C%22locator%22%3A%221%22%7D%7D\">“Linear behavior in high-dimensional spaces is sufficient to cause adversarial examples.”</span></p>\n<p>通过这种思想，作者还设计了高效的adversarial training的方式</p>\n<p>作者还证明adversarial training可以起到非常好的正则化的作用（<span style=\"background-color: #ff666680\">regularization 比 dropout效果好？感觉没明白啥意思</span>）</p>\n<p>然后还提到了<span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2F6LYY4TFL%22%2C%22pageLabel%22%3A%221%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B194.824%2C256.893%2C289.937%2C265.8%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FJMVNRGV9%22%5D%2C%22locator%22%3A%221%22%7D%7D\">“regularization strategies”</span>并不能对脆弱性起到什么作用，而把模型换为非线性的模型就有非常好的效果<span style=\"background-color: #5fb23680\">（感觉这里上下文有点不通顺连贯啊）</span></p>\n<p><span style=\"background-color: #5fb23680\">然后作者略作总结</span></p>\n<p>线性特征的模型训练起来快，但是很容易被攻击，非线性模型效果比较好，但是容易被攻击</p>\n<p></p>\n<h1>3. Linear of Adversarial</h1>\n<p>模型输入的精度是限定的，首先模型不应该被小于精度的扰动所干扰.</p>\n<p>总之这里想强调的是，攻击的扰动要尽可能的小，以防止被发现</p>\n<p>sign函数，就是输入一个x，x&gt;0就输出1反之输出-1，这里<span class=\"math\">$\\eta = sign(w)$</span> 也就是对w向量中每个维度的值用sign函数，这样二者内积就尽可能的大（这里背后的逻辑其实就是在扰动下的情况下实现结果大的偏差，所以要把内积最大化，追求速度就把eta每个维度根据w的正负赋值为1和-1，不过最终还是保证扰动的范围在<span class=\"math\">$\\epsilon$</span> 之内）</p>\n<p>这个简单的模型展示了在线性性质下，模型非常容易受到攻击</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2F6LYY4TFL%22%2C%22annotationKey%22%3A%22AI3EFHPT%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B220.567%2C209.446%2C504.003%2C218.353%5D%2C%5B108%2C198.487%2C194.993%2C207.394%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FJMVNRGV9%22%5D%2C%22locator%22%3A%222%22%7D%7D\">“a simple linear model can have adversarial examples if its input has sufficient dimensionality”</span></p>\n<h1>4.Linear for non linear models</h1>\n<p>神经网络都非常线性（因为这样方便训练）</p>\n<p>对于扰动的计算，还是根据损失函数来找对抗样本</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2F6LYY4TFL%22%2C%22annotationKey%22%3A%22BH4CKLH5%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B288.161%2C478.935%2C501.507%2C488.001%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FJMVNRGV9%22%5D%2C%22locator%22%3A%223%22%7D%7D\">“J(θ, x, y) be the cost used to train the neural network”</span> J是模型训练的时候的损失函数</p>\n<p><img alt=\"\" data-attachment-key=\"ZIUUXUXN\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2F6LYY4TFL%22%2C%22annotationKey%22%3A%228TQ9XV56%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B241.324%2C438.618%2C366.176%2C461.118%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FJMVNRGV9%22%5D%2C%22locator%22%3A%223%22%7D%7D\" width=\"208\" height=\"37\"> 梯度是损失函数提高的最快方向，对这个方向使用sign函数，然后加上<span class=\"math\">$\\epsilon$</span>的约束，这是因为sign函数的无穷范数一定为1，乘以epsilon就将无穷范数约束在<span class=\"math\">$\\epsilon$</span>以内</p>\n<p>这就是</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2F6LYY4TFL%22%2C%22annotationKey%22%3A%22U73RIYRQ%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B201.22%2C427.692%2C301.812%2C436.599%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FJMVNRGV9%22%5D%2C%22locator%22%3A%223%22%7D%7D\">“fast gradient sign method”</span></p>\n<p>利用sign快速计算梯度的方向，<span style=\"background-color: #2ea8e580\">注意这里并没有指定攻击的方向（比如让图片识别为特定的标签）</span></p>\n<p>同时一个重要问题是，sign函数多大程度上提高了效率</p>\n<hr>\n<p>当然还有其他的扰动方式</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2F6LYY4TFL%22%2C%22annotationKey%22%3A%227T927DJM%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B108%2C312.126%2C496.372%2C321.192%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FJMVNRGV9%22%5D%2C%22locator%22%3A%223%22%7D%7D\">“rotating x by a small angle in the direction of the gradient reliably produces adversarial examples”</span> </p>\n<p>这个简单的算法不经可以用来找对抗样本，</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2F6LYY4TFL%22%2C%22annotationKey%22%3A%22XNI3PAYY%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B213.693%2C273.271%2C501.513%2C282.178%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FJMVNRGV9%22%5D%2C%22locator%22%3A%223%22%7D%7D\">“speeding up adversarial training or even just analysis of trained networks”</span></p>\n<p></p>\n<h1>5.Adversarial training (linear)</h1>\n<p>用基本的logistic regression来做，但是没太看懂公式原理</p>\n<p>然后讨论了对抗训练和正则化在本质上的联系，以及权重衰退的效果</p>\n<h1>6.Adversarial training (deep)</h1>\n<p></p>\n<h1>7.Model capacity</h1>\n<p></p>\n<h1>8.Generalization</h1>\n<p></p>\n<h1>9.Hypotheses</h1>\n<p></p>\n<p></p>\n<p></p>\n<p></p>\n<p></p>\n<p></p>\n</div>",
          "tags": [],
          "relations": {},
          "dateAdded": "2023-03-08T12:27:38Z",
          "dateModified": "2023-03-10T06:56:56Z",
          "uri": "http://zotero.org/groups/4922950/items/25BLW6DN"
        }
      ],
      "citationKey": "goodfellowExplainingHarnessingAdversarial2015",
      "itemKey": "JMVNRGV9",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/JMVNRGV9"
    },
    {
      "key": "B2MGNSGN",
      "version": 920,
      "itemType": "preprint",
      "title": "Fine-tuning Is Not Enough: A Simple yet Effective Watermark Removal Attack for DNN Models",
      "abstractNote": "Watermarking has become the tendency in protecting the intellectual property of DNN models. Recent works, from the adversary's perspective, attempted to subvert watermarking mechanisms by designing watermark removal attacks. However, these attacks mainly adopted sophisticated fine-tuning techniques, which have certain fatal drawbacks or unrealistic assumptions. In this paper, we propose a novel watermark removal attack from a different perspective. Instead of just fine-tuning the watermarked models, we design a simple yet powerful transformation algorithm by combining imperceptible pattern embedding and spatial-level transformations, which can effectively and blindly destroy the memorization of watermarked models to the watermark samples. We also introduce a lightweight fine-tuning strategy to preserve the model performance. Our solution requires much less resource or knowledge about the watermarking scheme than prior works. Extensive experimental results indicate that our attack can bypass state-of-the-art watermarking solutions with very high success rates. Based on our attack, we propose watermark augmentation techniques to enhance the robustness of existing watermarks.",
      "date": "2021-05-17",
      "shortTitle": "Fine-tuning Is Not Enough",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/2009.08697",
      "accessDate": "2023-03-29T15:58:54Z",
      "extra": "arXiv:2009.08697 [cs, stat]",
      "DOI": "10.48550/arXiv.2009.08697",
      "repository": "arXiv",
      "archiveID": "arXiv:2009.08697",
      "creators": [
        {
          "firstName": "Shangwei",
          "lastName": "Guo",
          "creatorType": "author"
        },
        {
          "firstName": "Tianwei",
          "lastName": "Zhang",
          "creatorType": "author"
        },
        {
          "firstName": "Han",
          "lastName": "Qiu",
          "creatorType": "author"
        },
        {
          "firstName": "Yi",
          "lastName": "Zeng",
          "creatorType": "author"
        },
        {
          "firstName": "Tao",
          "lastName": "Xiang",
          "creatorType": "author"
        },
        {
          "firstName": "Yang",
          "lastName": "Liu",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Machine Learning",
          "type": 1
        },
        {
          "tag": "Statistics - Machine Learning",
          "type": 1
        },
        {
          "tag": "Computer Science - Cryptography and Security",
          "type": 1
        }
      ],
      "relations": [],
      "dateAdded": "2023-03-29T15:58:57Z",
      "dateModified": "2023-03-29T15:58:57Z",
      "uri": "http://zotero.org/groups/4922950/items/B2MGNSGN",
      "itemID": 2467,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "全文",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-04-10T03:58:21Z",
          "dateModified": "2023-04-10T03:58:21Z",
          "uri": "http://zotero.org/groups/4922950/items/K2L79MRE",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\K2L79MRE\\Guo 等 - 2021 - Fine-tuning Is Not Enough A Simple yet Effective .pdf",
          "select": "zotero://select/groups/4922950/items/K2L79MRE"
        }
      ],
      "notes": [
        {
          "key": "UWI2YVNH",
          "version": 920,
          "itemType": "note",
          "parentItem": "B2MGNSGN",
          "note": "Comment: 7 pages, 4 figures, accpeted by IJCAI 2021",
          "tags": [],
          "relations": {},
          "dateAdded": "2023-03-29T15:58:57Z",
          "dateModified": "2023-03-29T15:58:57Z",
          "uri": "http://zotero.org/groups/4922950/items/UWI2YVNH"
        }
      ],
      "citationKey": "guoFinetuningNotEnough2021",
      "itemKey": "B2MGNSGN",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/B2MGNSGN"
    },
    {
      "key": "8P3F47LV",
      "version": 920,
      "itemType": "preprint",
      "title": "Fine-tuning Is Not Enough: A Simple yet Effective Watermark Removal Attack for DNN Models",
      "abstractNote": "Watermarking has become the tendency in protecting the intellectual property of DNN models. Recent works, from the adversary's perspective, attempted to subvert watermarking mechanisms by designing watermark removal attacks. However, these attacks mainly adopted sophisticated fine-tuning techniques, which have certain fatal drawbacks or unrealistic assumptions. In this paper, we propose a novel watermark removal attack from a different perspective. Instead of just fine-tuning the watermarked models, we design a simple yet powerful transformation algorithm by combining imperceptible pattern embedding and spatial-level transformations, which can effectively and blindly destroy the memorization of watermarked models to the watermark samples. We also introduce a lightweight fine-tuning strategy to preserve the model performance. Our solution requires much less resource or knowledge about the watermarking scheme than prior works. Extensive experimental results indicate that our attack can bypass state-of-the-art watermarking solutions with very high success rates. Based on our attack, we propose watermark augmentation techniques to enhance the robustness of existing watermarks.",
      "date": "2021-05-17",
      "shortTitle": "Fine-tuning Is Not Enough",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/2009.08697",
      "accessDate": "2023-03-29T15:58:54Z",
      "extra": "arXiv:2009.08697 [cs, stat]",
      "DOI": "10.48550/arXiv.2009.08697",
      "repository": "arXiv",
      "archiveID": "arXiv:2009.08697",
      "creators": [
        {
          "firstName": "Shangwei",
          "lastName": "Guo",
          "creatorType": "author"
        },
        {
          "firstName": "Tianwei",
          "lastName": "Zhang",
          "creatorType": "author"
        },
        {
          "firstName": "Han",
          "lastName": "Qiu",
          "creatorType": "author"
        },
        {
          "firstName": "Yi",
          "lastName": "Zeng",
          "creatorType": "author"
        },
        {
          "firstName": "Tao",
          "lastName": "Xiang",
          "creatorType": "author"
        },
        {
          "firstName": "Yang",
          "lastName": "Liu",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Machine Learning",
          "type": 1
        },
        {
          "tag": "Statistics - Machine Learning",
          "type": 1
        },
        {
          "tag": "Computer Science - Cryptography and Security",
          "type": 1
        }
      ],
      "relations": [],
      "dateAdded": "2023-03-29T15:59:01Z",
      "dateModified": "2023-03-29T15:59:01Z",
      "uri": "http://zotero.org/groups/4922950/items/8P3F47LV",
      "itemID": 2470,
      "attachments": [],
      "notes": [],
      "citationKey": "guoFinetuningNotEnough2021a",
      "itemKey": "8P3F47LV",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/8P3F47LV"
    },
    {
      "key": "F9HJZNDY",
      "version": 708,
      "itemType": "preprint",
      "title": "Adversarial Examples Are Not Bugs, They Are Features",
      "abstractNote": "Adversarial examples have attracted significant attention in machine learning, but the reasons for their existence and pervasiveness remain unclear. We demonstrate that adversarial examples can be directly attributed to the presence of non-robust features: features derived from patterns in the data distribution that are highly predictive, yet brittle and incomprehensible to humans. After capturing these features within a theoretical framework, we establish their widespread existence in standard datasets. Finally, we present a simple setting where we can rigorously tie the phenomena we observe in practice to a misalignment between the (human-specified) notion of robustness and the inherent geometry of the data.",
      "date": "2019-08-12",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/1905.02175",
      "accessDate": "2023-03-21T12:02:38Z",
      "extra": "arXiv:1905.02175 [cs, stat]",
      "DOI": "10.48550/arXiv.1905.02175",
      "repository": "arXiv",
      "archiveID": "arXiv:1905.02175",
      "creators": [
        {
          "firstName": "Andrew",
          "lastName": "Ilyas",
          "creatorType": "author"
        },
        {
          "firstName": "Shibani",
          "lastName": "Santurkar",
          "creatorType": "author"
        },
        {
          "firstName": "Dimitris",
          "lastName": "Tsipras",
          "creatorType": "author"
        },
        {
          "firstName": "Logan",
          "lastName": "Engstrom",
          "creatorType": "author"
        },
        {
          "firstName": "Brandon",
          "lastName": "Tran",
          "creatorType": "author"
        },
        {
          "firstName": "Aleksander",
          "lastName": "Madry",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Machine Learning",
          "type": 1
        },
        {
          "tag": "Statistics - Machine Learning",
          "type": 1
        },
        {
          "tag": "Computer Science - Cryptography and Security",
          "type": 1
        },
        {
          "tag": "Computer Science - Computer Vision and Pattern Recognition",
          "type": 1
        }
      ],
      "relations": [],
      "dateAdded": "2023-03-21T12:02:38Z",
      "dateModified": "2023-03-21T12:02:39Z",
      "uri": "http://zotero.org/groups/4922950/items/F9HJZNDY",
      "itemID": 2473,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "arXiv Fulltext PDF",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-03-21T12:06:12Z",
          "dateModified": "2023-03-21T12:06:12Z",
          "uri": "http://zotero.org/groups/4922950/items/LWQPQC2U",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\LWQPQC2U\\Ilyas 等 - 2019 - Adversarial Examples Are Not Bugs, They Are Featur.pdf",
          "select": "zotero://select/groups/4922950/items/LWQPQC2U"
        },
        {
          "itemType": "attachment",
          "title": "arXiv.org Snapshot",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-03-21T12:06:24Z",
          "dateModified": "2023-03-21T12:06:24Z",
          "uri": "http://zotero.org/groups/4922950/items/JEB59ZRC",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\JEB59ZRC\\1905.html",
          "select": "zotero://select/groups/4922950/items/JEB59ZRC"
        }
      ],
      "notes": [],
      "citationKey": "ilyasAdversarialExamplesAre2019",
      "itemKey": "F9HJZNDY",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/F9HJZNDY"
    },
    {
      "key": "652D8PW4",
      "version": 691,
      "itemType": "preprint",
      "title": "Adversarial Examples Are Not Bugs, They Are Features",
      "abstractNote": "Adversarial examples have attracted significant attention in machine learning, but the reasons for their existence and pervasiveness remain unclear. We demonstrate that adversarial examples can be directly attributed to the presence of non-robust features: features derived from patterns in the data distribution that are highly predictive, yet brittle and incomprehensible to humans. After capturing these features within a theoretical framework, we establish their widespread existence in standard datasets. Finally, we present a simple setting where we can rigorously tie the phenomena we observe in practice to a misalignment between the (human-specified) notion of robustness and the inherent geometry of the data.",
      "date": "2019-08-12",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/1905.02175",
      "accessDate": "2023-03-20T13:00:31Z",
      "extra": "arXiv:1905.02175 [cs, stat]",
      "DOI": "10.48550/arXiv.1905.02175",
      "repository": "arXiv",
      "archiveID": "arXiv:1905.02175",
      "creators": [
        {
          "firstName": "Andrew",
          "lastName": "Ilyas",
          "creatorType": "author"
        },
        {
          "firstName": "Shibani",
          "lastName": "Santurkar",
          "creatorType": "author"
        },
        {
          "firstName": "Dimitris",
          "lastName": "Tsipras",
          "creatorType": "author"
        },
        {
          "firstName": "Logan",
          "lastName": "Engstrom",
          "creatorType": "author"
        },
        {
          "firstName": "Brandon",
          "lastName": "Tran",
          "creatorType": "author"
        },
        {
          "firstName": "Aleksander",
          "lastName": "Madry",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Machine Learning",
          "type": 1
        },
        {
          "tag": "Statistics - Machine Learning",
          "type": 1
        },
        {
          "tag": "Computer Science - Cryptography and Security",
          "type": 1
        },
        {
          "tag": "Computer Science - Computer Vision and Pattern Recognition",
          "type": 1
        }
      ],
      "relations": [],
      "dateAdded": "2023-03-20T13:00:31Z",
      "dateModified": "2023-03-20T13:00:31Z",
      "uri": "http://zotero.org/groups/4922950/items/652D8PW4",
      "itemID": 2474,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "arXiv Fulltext PDF",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-03-20T13:00:38Z",
          "dateModified": "2023-03-20T13:00:38Z",
          "uri": "http://zotero.org/groups/4922950/items/CNFK76ZL",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\CNFK76ZL\\Ilyas 等 - 2019 - Adversarial Examples Are Not Bugs, They Are Featur.pdf",
          "select": "zotero://select/groups/4922950/items/CNFK76ZL"
        },
        {
          "itemType": "attachment",
          "title": "arXiv.org Snapshot",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-03-20T13:00:51Z",
          "dateModified": "2023-03-20T13:00:51Z",
          "uri": "http://zotero.org/groups/4922950/items/5BFFRQTJ",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\5BFFRQTJ\\1905.html",
          "select": "zotero://select/groups/4922950/items/5BFFRQTJ"
        }
      ],
      "notes": [],
      "citationKey": "ilyasAdversarialExamplesAre2019a",
      "itemKey": "652D8PW4",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/652D8PW4"
    },
    {
      "key": "BXCDMYHD",
      "version": 9,
      "itemType": "conferencePaper",
      "title": "Adversarially Robust Malware Detection Using Monotonic Classification",
      "abstractNote": "We propose monotonic classification with selection of monotonic features as a defense against evasion attacks on classifiers for malware detection. The monotonicity property of our classifier ensures that an adversary will not be able to evade the classifier by adding more features. We train and test our classifier on over one million executables collected from VirusTotal. Our secure classifier has 62% temporal detection rate at a 1% false positive rate. In comparison with a regular classifier with unrestricted features, the secure malware classifier results in a drop of approximately 13% in detection rate. Since this degradation in performance is a result of using a classifier that cannot be evaded, we interpret this performance hit as the cost of security in classifying malware.",
      "date": "2018-03-21",
      "language": "en",
      "libraryCatalog": "DOI.org (Crossref)",
      "url": "https://dl.acm.org/doi/10.1145/3180445.3180449",
      "accessDate": "2023-01-24T10:59:17Z",
      "place": "Tempe AZ USA",
      "publisher": "ACM",
      "ISBN": "978-1-4503-5634-3",
      "pages": "54-63",
      "proceedingsTitle": "Proceedings of the Fourth ACM International Workshop on Security and Privacy Analytics",
      "conferenceName": "CODASPY '18: Eighth ACM Conference on Data and Application Security and Privacy",
      "DOI": "10.1145/3180445.3180449",
      "creators": [
        {
          "firstName": "Íñigo",
          "lastName": "Íncer Romeo",
          "creatorType": "author"
        },
        {
          "firstName": "Michael",
          "lastName": "Theodorides",
          "creatorType": "author"
        },
        {
          "firstName": "Sadia",
          "lastName": "Afroz",
          "creatorType": "author"
        },
        {
          "firstName": "David",
          "lastName": "Wagner",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": [],
      "dateAdded": "2023-01-24T10:59:17Z",
      "dateModified": "2023-01-24T10:59:17Z",
      "uri": "http://zotero.org/groups/4922950/items/BXCDMYHD",
      "itemID": 1752,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "Íncer Romeo 等 - 2018 - Adversarially Robust Malware Detection Using Monot.pdf",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-01-24T10:59:12Z",
          "dateModified": "2023-01-24T10:59:18Z",
          "uri": "http://zotero.org/groups/4922950/items/532SHQM2",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\532SHQM2\\Íncer Romeo 等 - 2018 - Adversarially Robust Malware Detection Using Monot.pdf",
          "select": "zotero://select/groups/4922950/items/532SHQM2"
        }
      ],
      "notes": [],
      "citationKey": "incerromeoAdversariallyRobustMalware2018",
      "itemKey": "BXCDMYHD",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/BXCDMYHD"
    },
    {
      "key": "9BEYMTIU",
      "version": 1221,
      "itemType": "preprint",
      "title": "Measuring Forgetting of Memorized Training Examples",
      "abstractNote": "Machine learning models exhibit two seemingly contradictory phenomena: training data memorization and various forms of forgetting. In memorization, models overfit specific training examples and become susceptible to privacy attacks. In forgetting, examples which appeared early in training are forgotten by the end. In this work, we connect these phenomena. We propose a technique to measure to what extent models ``forget'' the specifics of training examples, becoming less susceptible to privacy attacks on examples they have not seen recently. We show that, while non-convexity can prevent forgetting from happening in the worst-case, standard image and speech models empirically do forget examples over time. We identify nondeterminism as a potential explanation, showing that deterministically trained models do not forget. Our results suggest that examples seen early when training with extremely large datasets -- for instance those examples used to pre-train a model -- may observe privacy benefits at the expense of examples seen later.",
      "date": "2022-06-30",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/2207.00099",
      "accessDate": "2023-04-11T04:07:03Z",
      "extra": "arXiv:2207.00099 [cs]",
      "DOI": "10.48550/arXiv.2207.00099",
      "repository": "arXiv",
      "archiveID": "arXiv:2207.00099",
      "creators": [
        {
          "firstName": "Matthew",
          "lastName": "Jagielski",
          "creatorType": "author"
        },
        {
          "firstName": "Om",
          "lastName": "Thakkar",
          "creatorType": "author"
        },
        {
          "firstName": "Florian",
          "lastName": "Tramèr",
          "creatorType": "author"
        },
        {
          "firstName": "Daphne",
          "lastName": "Ippolito",
          "creatorType": "author"
        },
        {
          "firstName": "Katherine",
          "lastName": "Lee",
          "creatorType": "author"
        },
        {
          "firstName": "Nicholas",
          "lastName": "Carlini",
          "creatorType": "author"
        },
        {
          "firstName": "Eric",
          "lastName": "Wallace",
          "creatorType": "author"
        },
        {
          "firstName": "Shuang",
          "lastName": "Song",
          "creatorType": "author"
        },
        {
          "firstName": "Abhradeep",
          "lastName": "Thakurta",
          "creatorType": "author"
        },
        {
          "firstName": "Nicolas",
          "lastName": "Papernot",
          "creatorType": "author"
        },
        {
          "firstName": "Chiyuan",
          "lastName": "Zhang",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Machine Learning",
          "type": 1
        }
      ],
      "relations": [],
      "dateAdded": "2023-04-11T04:07:03Z",
      "dateModified": "2023-04-11T04:07:03Z",
      "uri": "http://zotero.org/groups/4922950/items/9BEYMTIU",
      "itemID": 3057,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "arXiv Fulltext PDF",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-04-11T04:07:08Z",
          "dateModified": "2023-04-11T04:07:08Z",
          "uri": "http://zotero.org/groups/4922950/items/N3UJIFZT",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\N3UJIFZT\\Jagielski 等 - 2022 - Measuring Forgetting of Memorized Training Example.pdf",
          "select": "zotero://select/groups/4922950/items/N3UJIFZT"
        },
        {
          "itemType": "attachment",
          "title": "arXiv.org Snapshot",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-04-11T04:07:20Z",
          "dateModified": "2023-04-11T04:07:20Z",
          "uri": "http://zotero.org/groups/4922950/items/UHV526YB",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\UHV526YB\\2207.html",
          "select": "zotero://select/groups/4922950/items/UHV526YB"
        }
      ],
      "notes": [
        {
          "key": "Y5UA5CXU",
          "version": 1221,
          "itemType": "note",
          "parentItem": "9BEYMTIU",
          "note": "Comment: 19 pages, 7 figures",
          "tags": [],
          "relations": {},
          "dateAdded": "2023-04-11T04:07:03Z",
          "dateModified": "2023-04-11T04:07:03Z",
          "uri": "http://zotero.org/groups/4922950/items/Y5UA5CXU"
        }
      ],
      "citationKey": "jagielskiMeasuringForgettingMemorized2022",
      "itemKey": "9BEYMTIU",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/9BEYMTIU"
    },
    {
      "key": "QQNTHJ7U",
      "version": 41,
      "itemType": "conferencePaper",
      "title": "E-ABS: Extending the Analysis-By-Synthesis Robust Classification Model to More Complex Image Domains",
      "abstractNote": "Conditional generative models, such as Schott et al.’s Analysis-bySynthesis (ABS), have state-of-the-art robustness on MNIST, but fail in more challenging datasets. In this paper, we present E-ABS, an improvement on ABS that achieves state-of-the-art robustness on SVHN. E-ABS gives more reliable class-conditional likelihood estimations on both in-distribution and out-of-distribution samples than ABS. Theoretically, E-ABS preserves ABS’s key features for robustness; thus, we show that E-ABS has similar certified robustness as ABS. Empirically, E-ABS outperforms both ABS and adversarial training on SVHN and a traffic sign dataset, achieving state-of-the-art robustness on these two real-world tasks. Our work shows a connection between ABS-like models and some recent advances on generative models, suggesting that ABS-like models are a promising direction for defending adversarial examples.",
      "date": "2020-11-13",
      "language": "en",
      "shortTitle": "E-ABS",
      "libraryCatalog": "DOI.org (Crossref)",
      "url": "https://dl.acm.org/doi/10.1145/3411508.3421382",
      "accessDate": "2023-01-24T11:28:47Z",
      "place": "Virtual Event USA",
      "publisher": "ACM",
      "ISBN": "978-1-4503-8094-2",
      "pages": "25-36",
      "proceedingsTitle": "Proceedings of the 13th ACM Workshop on Artificial Intelligence and Security",
      "conferenceName": "CCS '20: 2020 ACM SIGSAC Conference on Computer and Communications Security",
      "DOI": "10.1145/3411508.3421382",
      "creators": [
        {
          "firstName": "An",
          "lastName": "Ju",
          "creatorType": "author"
        },
        {
          "firstName": "David",
          "lastName": "Wagner",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": [],
      "dateAdded": "2023-01-24T11:28:47Z",
      "dateModified": "2023-01-24T11:28:47Z",
      "uri": "http://zotero.org/groups/4922950/items/QQNTHJ7U",
      "itemID": 1774,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "Ju 和 Wagner - 2020 - E-ABS Extending the Analysis-By-Synthesis Robust .pdf",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-01-24T11:28:44Z",
          "dateModified": "2023-01-24T11:28:48Z",
          "uri": "http://zotero.org/groups/4922950/items/NSTXSKQC",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\NSTXSKQC\\Ju 和 Wagner - 2020 - E-ABS Extending the Analysis-By-Synthesis Robust .pdf",
          "select": "zotero://select/groups/4922950/items/NSTXSKQC"
        }
      ],
      "notes": [],
      "citationKey": "juEABSExtendingAnalysisBySynthesis2020",
      "itemKey": "QQNTHJ7U",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/QQNTHJ7U"
    },
    {
      "key": "ZVY8428J",
      "version": 645,
      "itemType": "preprint",
      "title": "Adversarial examples in the physical world",
      "abstractNote": "Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. Up to now, all previous work have assumed a threat model in which the adversary can feed data directly into the machine learning classifier. This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as an input. This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples. We demonstrate this by feeding adversarial images obtained from cell-phone camera to an ImageNet Inception classifier and measuring the classification accuracy of the system. We find that a large fraction of adversarial examples are classified incorrectly even when perceived through the camera.",
      "date": "2017-02-10",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/1607.02533",
      "accessDate": "2023-03-20T12:05:10Z",
      "extra": "arXiv:1607.02533 [cs, stat]",
      "DOI": "10.48550/arXiv.1607.02533",
      "repository": "arXiv",
      "archiveID": "arXiv:1607.02533",
      "creators": [
        {
          "firstName": "Alexey",
          "lastName": "Kurakin",
          "creatorType": "author"
        },
        {
          "firstName": "Ian",
          "lastName": "Goodfellow",
          "creatorType": "author"
        },
        {
          "firstName": "Samy",
          "lastName": "Bengio",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Machine Learning",
          "type": 1
        },
        {
          "tag": "Statistics - Machine Learning",
          "type": 1
        },
        {
          "tag": "Computer Science - Cryptography and Security",
          "type": 1
        },
        {
          "tag": "Computer Science - Computer Vision and Pattern Recognition",
          "type": 1
        }
      ],
      "relations": [],
      "dateAdded": "2023-03-20T12:05:10Z",
      "dateModified": "2023-03-20T12:05:14Z",
      "uri": "http://zotero.org/groups/4922950/items/ZVY8428J",
      "itemID": 2476,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "arXiv Fulltext PDF",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-03-20T12:05:18Z",
          "dateModified": "2023-03-20T12:05:18Z",
          "uri": "http://zotero.org/groups/4922950/items/DS3E44GG",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\DS3E44GG\\Kurakin 等 - 2017 - Adversarial examples in the physical world.pdf",
          "select": "zotero://select/groups/4922950/items/DS3E44GG"
        },
        {
          "itemType": "attachment",
          "title": "arXiv.org Snapshot",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-03-20T12:05:31Z",
          "dateModified": "2023-03-20T12:05:31Z",
          "uri": "http://zotero.org/groups/4922950/items/LTC7HC6X",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\LTC7HC6X\\1607.html",
          "select": "zotero://select/groups/4922950/items/LTC7HC6X"
        }
      ],
      "notes": [
        {
          "key": "XRZZKK3S",
          "version": 645,
          "itemType": "note",
          "parentItem": "ZVY8428J",
          "note": "Comment: 14 pages, 6 figures. Demo available at https://youtu.be/zQ_uMenoBCk",
          "tags": [],
          "relations": {},
          "dateAdded": "2023-03-20T12:05:10Z",
          "dateModified": "2023-03-20T12:05:10Z",
          "uri": "http://zotero.org/groups/4922950/items/XRZZKK3S"
        },
        {
          "key": "3SALIJTS",
          "version": 685,
          "itemType": "note",
          "parentItem": "ZVY8428J",
          "note": "<div data-citation-items=\"%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FZVY8428J%22%5D%2C%22itemData%22%3A%7B%22id%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FZVY8428J%22%2C%22type%22%3A%22article%22%2C%22abstract%22%3A%22Most%20existing%20machine%20learning%20classifiers%20are%20highly%20vulnerable%20to%20adversarial%20examples.%20An%20adversarial%20example%20is%20a%20sample%20of%20input%20data%20which%20has%20been%20modified%20very%20slightly%20in%20a%20way%20that%20is%20intended%20to%20cause%20a%20machine%20learning%20classifier%20to%20misclassify%20it.%20In%20many%20cases%2C%20these%20modifications%20can%20be%20so%20subtle%20that%20a%20human%20observer%20does%20not%20even%20notice%20the%20modification%20at%20all%2C%20yet%20the%20classifier%20still%20makes%20a%20mistake.%20Adversarial%20examples%20pose%20security%20concerns%20because%20they%20could%20be%20used%20to%20perform%20an%20attack%20on%20machine%20learning%20systems%2C%20even%20if%20the%20adversary%20has%20no%20access%20to%20the%20underlying%20model.%20Up%20to%20now%2C%20all%20previous%20work%20have%20assumed%20a%20threat%20model%20in%20which%20the%20adversary%20can%20feed%20data%20directly%20into%20the%20machine%20learning%20classifier.%20This%20is%20not%20always%20the%20case%20for%20systems%20operating%20in%20the%20physical%20world%2C%20for%20example%20those%20which%20are%20using%20signals%20from%20cameras%20and%20other%20sensors%20as%20an%20input.%20This%20paper%20shows%20that%20even%20in%20such%20physical%20world%20scenarios%2C%20machine%20learning%20systems%20are%20vulnerable%20to%20adversarial%20examples.%20We%20demonstrate%20this%20by%20feeding%20adversarial%20images%20obtained%20from%20cell-phone%20camera%20to%20an%20ImageNet%20Inception%20classifier%20and%20measuring%20the%20classification%20accuracy%20of%20the%20system.%20We%20find%20that%20a%20large%20fraction%20of%20adversarial%20examples%20are%20classified%20incorrectly%20even%20when%20perceived%20through%20the%20camera.%22%2C%22DOI%22%3A%2210.48550%2FarXiv.1607.02533%22%2C%22note%22%3A%22arXiv%3A1607.02533%20%5Bcs%2C%20stat%5D%22%2C%22number%22%3A%22arXiv%3A1607.02533%22%2C%22publisher%22%3A%22arXiv%22%2C%22source%22%3A%22arXiv.org%22%2C%22title%22%3A%22Adversarial%20examples%20in%20the%20physical%20world%22%2C%22URL%22%3A%22http%3A%2F%2Farxiv.org%2Fabs%2F1607.02533%22%2C%22author%22%3A%5B%7B%22family%22%3A%22Kurakin%22%2C%22given%22%3A%22Alexey%22%7D%2C%7B%22family%22%3A%22Goodfellow%22%2C%22given%22%3A%22Ian%22%7D%2C%7B%22family%22%3A%22Bengio%22%2C%22given%22%3A%22Samy%22%7D%5D%2C%22accessed%22%3A%7B%22date-parts%22%3A%5B%5B%222023%22%2C3%2C20%5D%5D%7D%2C%22issued%22%3A%7B%22date-parts%22%3A%5B%5B%222017%22%2C2%2C10%5D%5D%7D%7D%7D%5D\" data-schema-version=\"8\"><h1>Motivation</h1>\n<p>总的来说，还是从现实世界的角度出发来分析问题。无法直接控制数据应该是从物理检测后的数据无法修改，而不是从黑箱的角度入手</p>\n<p></p>\n<h1>Method</h1>\n<p>有意思的是提出了IFGSM，就是通过多次迭代来构造扰动，而且还提出了指定攻击方向的一种攻击。</p>\n<p>总的来说IFGSM可以更有效的攻击</p>\n<h1>Transformation</h1>\n<p>在实际拍摄中对构造的图片又会有改动，这个时候实验结果非常有趣，FGSM表现的更好</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FDS3E44GG%22%2C%22annotationKey%22%3A%225W2W9L4C%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%227%22%2C%22position%22%3A%7B%22pageIndex%22%3A6%2C%22rects%22%3A%5B%5B166.799%2C120.758%2C504.003%2C129.665%5D%2C%5B108%2C109.8%2C161.36%2C118.707%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FZVY8428J%22%5D%2C%22locator%22%3A%227%22%7D%7D\">““fast” adversarial images are more robust to photo transformation compared to iterative methods.”</span></p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FDS3E44GG%22%2C%22annotationKey%22%3A%22Q6YISEQA%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%227%22%2C%22position%22%3A%7B%22pageIndex%22%3A6%2C%22rects%22%3A%5B%5B324.836%2C109.8%2C504.003%2C118.707%5D%2C%5B108%2C98.841%2C504.003%2C107.748%5D%2C%5B108%2C87.882%2C125.992%2C96.789%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FZVY8428J%22%5D%2C%22locator%22%3A%227%22%7D%7D\">“iterative methods exploit more subtle kind of perturbations, and these subtle perturbations are more likely to be destroyed by photo transformation.”</span></p>\n<h1>conclusion</h1>\n<p>总的来说这项工作并没有什么太惊人的效果，知识从创新上展现了攻击在现实世界的可能性。未来很多工作都可以基于此来展开。</p>\n</div>",
          "tags": [],
          "relations": {},
          "dateAdded": "2023-03-20T12:08:09Z",
          "dateModified": "2023-03-20T12:53:20Z",
          "uri": "http://zotero.org/groups/4922950/items/3SALIJTS"
        }
      ],
      "citationKey": "kurakinAdversarialExamplesPhysical2017",
      "itemKey": "ZVY8428J",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/ZVY8428J"
    },
    {
      "key": "284YS5FJ",
      "version": 731,
      "itemType": "preprint",
      "title": "Adversarial Machine Learning at Scale",
      "abstractNote": "Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet. Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than single-step attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a \"label leaking\" effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.",
      "date": "2017-02-10",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/1611.01236",
      "accessDate": "2023-03-28T11:43:50Z",
      "extra": "arXiv:1611.01236 [cs, stat]",
      "DOI": "10.48550/arXiv.1611.01236",
      "repository": "arXiv",
      "archiveID": "arXiv:1611.01236",
      "creators": [
        {
          "firstName": "Alexey",
          "lastName": "Kurakin",
          "creatorType": "author"
        },
        {
          "firstName": "Ian",
          "lastName": "Goodfellow",
          "creatorType": "author"
        },
        {
          "firstName": "Samy",
          "lastName": "Bengio",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Machine Learning",
          "type": 1
        },
        {
          "tag": "Statistics - Machine Learning",
          "type": 1
        },
        {
          "tag": "Computer Science - Cryptography and Security",
          "type": 1
        },
        {
          "tag": "Computer Science - Computer Vision and Pattern Recognition",
          "type": 1
        }
      ],
      "relations": [],
      "dateAdded": "2023-03-28T11:43:50Z",
      "dateModified": "2023-03-28T11:43:50Z",
      "uri": "http://zotero.org/groups/4922950/items/284YS5FJ",
      "itemID": 2472,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "arXiv Fulltext PDF",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-03-28T11:43:59Z",
          "dateModified": "2023-03-28T11:43:59Z",
          "uri": "http://zotero.org/groups/4922950/items/V9BD7XY5",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\V9BD7XY5\\Kurakin 等 - 2017 - Adversarial Machine Learning at Scale.pdf",
          "select": "zotero://select/groups/4922950/items/V9BD7XY5"
        },
        {
          "itemType": "attachment",
          "title": "arXiv.org Snapshot",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-03-28T11:44:08Z",
          "dateModified": "2023-03-28T11:44:08Z",
          "uri": "http://zotero.org/groups/4922950/items/LT8G8ZJI",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\LT8G8ZJI\\1611.html",
          "select": "zotero://select/groups/4922950/items/LT8G8ZJI"
        }
      ],
      "notes": [
        {
          "key": "4FRRJET3",
          "version": 1040,
          "itemType": "note",
          "parentItem": "284YS5FJ",
          "note": "<div data-citation-items=\"%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2F284YS5FJ%22%5D%2C%22itemData%22%3A%7B%22id%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2F284YS5FJ%22%2C%22type%22%3A%22article%22%2C%22abstract%22%3A%22Adversarial%20examples%20are%20malicious%20inputs%20designed%20to%20fool%20machine%20learning%20models.%20They%20often%20transfer%20from%20one%20model%20to%20another%2C%20allowing%20attackers%20to%20mount%20black%20box%20attacks%20without%20knowledge%20of%20the%20target%20model's%20parameters.%20Adversarial%20training%20is%20the%20process%20of%20explicitly%20training%20a%20model%20on%20adversarial%20examples%2C%20in%20order%20to%20make%20it%20more%20robust%20to%20attack%20or%20to%20reduce%20its%20test%20error%20on%20clean%20inputs.%20So%20far%2C%20adversarial%20training%20has%20primarily%20been%20applied%20to%20small%20problems.%20In%20this%20research%2C%20we%20apply%20adversarial%20training%20to%20ImageNet.%20Our%20contributions%20include%3A%20(1)%20recommendations%20for%20how%20to%20succesfully%20scale%20adversarial%20training%20to%20large%20models%20and%20datasets%2C%20(2)%20the%20observation%20that%20adversarial%20training%20confers%20robustness%20to%20single-step%20attack%20methods%2C%20(3)%20the%20finding%20that%20multi-step%20attack%20methods%20are%20somewhat%20less%20transferable%20than%20single-step%20attack%20methods%2C%20so%20single-step%20attacks%20are%20the%20best%20for%20mounting%20black-box%20attacks%2C%20and%20(4)%20resolution%20of%20a%20%5C%22label%20leaking%5C%22%20effect%20that%20causes%20adversarially%20trained%20models%20to%20perform%20better%20on%20adversarial%20examples%20than%20on%20clean%20examples%2C%20because%20the%20adversarial%20example%20construction%20process%20uses%20the%20true%20label%20and%20the%20model%20can%20learn%20to%20exploit%20regularities%20in%20the%20construction%20process.%22%2C%22DOI%22%3A%2210.48550%2FarXiv.1611.01236%22%2C%22note%22%3A%22arXiv%3A1611.01236%20%5Bcs%2C%20stat%5D%22%2C%22number%22%3A%22arXiv%3A1611.01236%22%2C%22publisher%22%3A%22arXiv%22%2C%22source%22%3A%22arXiv.org%22%2C%22title%22%3A%22Adversarial%20Machine%20Learning%20at%20Scale%22%2C%22URL%22%3A%22http%3A%2F%2Farxiv.org%2Fabs%2F1611.01236%22%2C%22author%22%3A%5B%7B%22family%22%3A%22Kurakin%22%2C%22given%22%3A%22Alexey%22%7D%2C%7B%22family%22%3A%22Goodfellow%22%2C%22given%22%3A%22Ian%22%7D%2C%7B%22family%22%3A%22Bengio%22%2C%22given%22%3A%22Samy%22%7D%5D%2C%22accessed%22%3A%7B%22date-parts%22%3A%5B%5B%222023%22%2C3%2C28%5D%5D%7D%2C%22issued%22%3A%7B%22date-parts%22%3A%5B%5B%222017%22%2C2%2C10%5D%5D%7D%7D%7D%5D\" data-schema-version=\"8\"><p>这篇文章写于PGD之前</p>\n<p></p>\n<h1>Contribution</h1>\n<p><img alt=\"\" data-attachment-key=\"ZR7ZASTF\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FV9BD7XY5%22%2C%22annotationKey%22%3A%22SBAEZYSE%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%221%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B131.245%2C405.874%2C474.098%2C513.343%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2F284YS5FJ%22%5D%2C%22locator%22%3A%221%22%7D%7D\" width=\"571\" height=\"179\"> 在abstract中就详细列出了自己的贡献<br>1.如何在<span style=\"background-color: #ff666680\">大模型与数据集上对抗训练</span>，注意之前的工作基本都比较小<br>2.对抗训练对单步攻击有鲁棒性<br>3.多步攻击的泛化性弱于单步攻击 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <br>4.标签泄露的问题，模型只是学到了对正确标签的一些变换而没有真的学到东西</p>\n<p></p>\n<hr>\n<p>具体贡献：</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FV9BD7XY5%22%2C%22annotationKey%22%3A%22IHA467AL%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B160.085%2C666.028%2C311.138%2C675.652%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2F284YS5FJ%22%5D%2C%22locator%22%3A%222%22%7D%7D\">“successfully used adversarial training”</span> 作者是使用了adversarial training的方法，而且成功在大模型与数据集中训练，不过注意是用one-step method在大数据集上成功对抗训练</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FV9BD7XY5%22%2C%22annotationKey%22%3A%22FSJ9IVLJ%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B444.976%2C607.017%2C504.004%2C615.924%5D%2C%5B143.866%2C596.058%2C504.004%2C604.965%5D%2C%5B143.866%2C585.099%2C210.825%2C594.006%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2F284YS5FJ%22%5D%2C%22locator%22%3A%222%22%7D%7D\">“adversarial examples which are harder to resist using adversarial training are less likely to be transferrable between models.”</span> 意思是在对抗训练中容易被解决的样本 的迁移性也比较弱？非常有趣的结论</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FV9BD7XY5%22%2C%22annotationKey%22%3A%22KHYTI99C%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B293.604%2C569.924%2C504.004%2C579.548%5D%2C%5B143.866%2C558.965%2C305.798%2C567.872%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2F284YS5FJ%22%5D%2C%22locator%22%3A%222%22%7D%7D\">“higher capacity (i.e. number of parameters) tend to be more robust to adversarial examples”</span>容量与鲁棒性</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FV9BD7XY5%22%2C%22annotationKey%22%3A%22VKCEUUJU%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B351.825%2C532.831%2C403.95%2C542.455%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2F284YS5FJ%22%5D%2C%22locator%22%3A%222%22%7D%7D\">“label leaking”</span>标签泄露</p>\n<p></p>\n<h1>Method</h1>\n<h1>Problem</h1>\n<p></p>\n<h1>Inspiration</h1>\n<p>感觉这篇论文的transfer角度有很多启发的点</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FV9BD7XY5%22%2C%22annotationKey%22%3A%22FSJ9IVLJ%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B444.976%2C607.017%2C504.004%2C615.924%5D%2C%5B143.866%2C596.058%2C504.004%2C604.965%5D%2C%5B143.866%2C585.099%2C210.825%2C594.006%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2F284YS5FJ%22%5D%2C%22locator%22%3A%222%22%7D%7D\">“adversarial examples which are harder to resist using adversarial training are less likely to be transferrable between models.”</span> 意思是在对抗训练中容易被解决的样本 的迁移性也比较弱？非常有趣的结论</p>\n<p>同时还提到FGSM最好迁移</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FV9BD7XY5%22%2C%22annotationKey%22%3A%22TGYZPN97%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%2210%22%2C%22position%22%3A%7B%22pageIndex%22%3A9%2C%22rects%22%3A%5B%5B239.994%2C581.461%2C455.914%2C590.368%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2F284YS5FJ%22%5D%2C%22locator%22%3A%2210%22%7D%7D\">“FGSM adversarial examples are the most transferable”</span></p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FV9BD7XY5%22%2C%22annotationKey%22%3A%22R75R3L3T%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%2210%22%2C%22position%22%3A%7B%22pageIndex%22%3A9%2C%22rects%22%3A%5B%5B145.051%2C548.584%2C504.003%2C557.491%5D%2C%5B108%2C537.625%2C182.909%2C546.532%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2F284YS5FJ%22%5D%2C%22locator%22%3A%2210%22%7D%7D\">“an inverse relationship between transferability of specific method and ability of the method to fool the network”</span></p>\n<p>攻击越好迁移，相对的对特定模型的攻击能力就比较弱？</p>\n<p>而在对抗训练中，</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FV9BD7XY5%22%2C%22annotationKey%22%3A%22U97LHK4U%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%225%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B108%2C70.945%2C504.003%2C79.852%5D%2C%5B108%2C59.986%2C191.526%2C68.893%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2F284YS5FJ%22%5D%2C%22locator%22%3A%225%22%7D%7D\">“Unfortunately, training on one-step adversarial examples does not confer robustness to iterative adversarial examples”</span></p>\n<p>同时iterative method练不起来</p>\n<p><img alt=\"\" data-attachment-key=\"2DE8L3H3\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FV9BD7XY5%22%2C%22annotationKey%22%3A%22D6UCMUAE%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%226%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B102.63199999999999%2C451.3223672164113%2C511.97400000000005%2C516.868%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2F284YS5FJ%22%5D%2C%22locator%22%3A%226%22%7D%7D\" width=\"682\" height=\"109\"> 那么PGD为什么能够那么好的通过iterative method进行对抗训练???<br><br>或许PGD使用了更大的模型？</p>\n<h1>Compare to PGD</h1>\n<p></p>\n<p></p>\n<p></p>\n</div>",
          "tags": [],
          "relations": {},
          "dateAdded": "2023-04-04T07:47:11Z",
          "dateModified": "2023-04-04T08:16:01Z",
          "uri": "http://zotero.org/groups/4922950/items/4FRRJET3"
        },
        {
          "key": "G8BRD77P",
          "version": 731,
          "itemType": "note",
          "parentItem": "284YS5FJ",
          "note": "Comment: 17 pages, 5 figures",
          "tags": [],
          "relations": {},
          "dateAdded": "2023-03-28T11:43:50Z",
          "dateModified": "2023-03-28T11:43:50Z",
          "uri": "http://zotero.org/groups/4922950/items/G8BRD77P"
        }
      ],
      "citationKey": "kurakinAdversarialMachineLearning2017",
      "itemKey": "284YS5FJ",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/284YS5FJ"
    },
    {
      "key": "HXCPN928",
      "version": 935,
      "itemType": "conferencePaper",
      "title": "Finding Actual Descent Directions for Adversarial Training",
      "abstractNote": "Adversarial Training using a strong first-order adversary (PGD) is the gold standard for training Deep Neural Networks that are robust to adversarial examples. We show that, contrary to the general understanding of the method, the gradient at an optimal adversarial example may increase, rather than decrease, the adversarially robust loss. This holds independently of the learning rate. More precisely, we provide a counterexample to a corollary of Danskin's Theorem presented in the seminal paper of Madry et al. (2018) which states that a solution of the inner maximization problem can yield a descent direction for the adversarially robust loss. Based on a correct interpretation of Danskin's Theorem, we propose Danskin's Descent Direction (DDi) and we verify experimentally that it provides better directions than those obtained by a PGD adversary. Using the CIFAR10 dataset we further provide a real world example showing that our method achieves a steeper increase in robustness levels in the early stages of training, and is more stable than the PGD baseline. As a limitation, PGD training of ReLU+BatchNorm networks still performs better, but current theory is unable to explain this.",
      "date": "2023/02/01",
      "language": "en",
      "libraryCatalog": "openreview.net",
      "url": "https://openreview.net/forum?id=I3HCE7Ro78H",
      "accessDate": "2023-04-03T03:50:10Z",
      "conferenceName": "The Eleventh International Conference on Learning Representations",
      "creators": [
        {
          "firstName": "Fabian",
          "lastName": "Latorre",
          "creatorType": "author"
        },
        {
          "firstName": "Igor",
          "lastName": "Krawczuk",
          "creatorType": "author"
        },
        {
          "firstName": "Leello Tadesse",
          "lastName": "Dadi",
          "creatorType": "author"
        },
        {
          "firstName": "Thomas",
          "lastName": "Pethick",
          "creatorType": "author"
        },
        {
          "firstName": "Volkan",
          "lastName": "Cevher",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": [],
      "dateAdded": "2023-04-03T03:50:20Z",
      "dateModified": "2023-04-03T03:50:23Z",
      "uri": "http://zotero.org/groups/4922950/items/HXCPN928",
      "itemID": 2880,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "Full Text PDF",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-04-03T03:50:20Z",
          "dateModified": "2023-04-03T03:50:20Z",
          "uri": "http://zotero.org/groups/4922950/items/AHTKRX67",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\AHTKRX67\\Latorre 等 - 2023 - Finding Actual Descent Directions for Adversarial .pdf",
          "select": "zotero://select/groups/4922950/items/AHTKRX67"
        }
      ],
      "notes": [],
      "citationKey": "latorreFindingActualDescent2023",
      "itemKey": "HXCPN928",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/HXCPN928"
    },
    {
      "key": "YUAV99XU",
      "version": 1082,
      "itemType": "book",
      "title": "Backdoor Learning: A Survey",
      "abstractNote": "Backdoor attack intends to embed hidden backdoor into deep neural networks (DNNs), such that the attacked model performs well on benign samples, whereas its prediction will be maliciously changed if the hidden backdoor is activated by the attacker-defined trigger. This threat could happen when the training process is not fully controlled, such as training on third-party datasets or adopting third-party models, which poses a new and realistic threat. Although backdoor learning is an emerging and rapidly growing research area, its systematic review, however, remains blank. In this paper, we present the first comprehensive survey of this realm. We summarize and categorize existing backdoor attacks and defenses based on their characteristics, and provide a unified framework for analyzing poisoning-based backdoor attacks. Besides, we also analyze the relation between backdoor attacks and relevant fields ($i.e.,$ adversarial attacks and data poisoning), and summarize widely adopted benchmark datasets. Finally, we briefly outline certain future research directions relying upon reviewed works. A curated list of backdoor-related resources is also available at \\url{https://github.com/THUYimingLi/backdoor-learning-resources}.",
      "date": "2020-07-17",
      "shortTitle": "Backdoor Learning",
      "libraryCatalog": "ResearchGate",
      "creators": [
        {
          "firstName": "Yiming",
          "lastName": "Li",
          "creatorType": "author"
        },
        {
          "firstName": "Yong",
          "lastName": "Jiang",
          "creatorType": "author"
        },
        {
          "firstName": "Zhifeng",
          "lastName": "Li",
          "creatorType": "author"
        },
        {
          "firstName": "Shu-Tao",
          "lastName": "Xia",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": [],
      "dateAdded": "2023-04-05T07:03:11Z",
      "dateModified": "2023-04-05T07:03:19Z",
      "uri": "http://zotero.org/groups/4922950/items/YUAV99XU",
      "itemID": 2971,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "Full Text PDF",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-04-05T07:03:15Z",
          "dateModified": "2023-04-05T07:03:15Z",
          "uri": "http://zotero.org/groups/4922950/items/W55LI57A",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\W55LI57A\\Li 等 - 2020 - Backdoor Learning A Survey.pdf",
          "select": "zotero://select/groups/4922950/items/W55LI57A"
        },
        {
          "key": "BKV6P8W7",
          "version": 1077,
          "itemType": "attachment",
          "title": "ResearchGate Link",
          "url": "https://www.researchgate.net/publication/343006441_Backdoor_Learning_A_Survey",
          "accessDate": "2023-04-05T07:03:16Z",
          "parentItem": "YUAV99XU",
          "linkMode": "linked_url",
          "contentType": "",
          "charset": "",
          "tags": [],
          "relations": {},
          "dateAdded": "2023-04-05T07:03:16Z",
          "dateModified": "2023-04-05T07:03:16Z",
          "uri": "http://zotero.org/groups/4922950/items/BKV6P8W7"
        }
      ],
      "notes": [],
      "citationKey": "liBackdoorLearningSurvey2020",
      "itemKey": "YUAV99XU",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/YUAV99XU"
    },
    {
      "key": "94KNAXSS",
      "version": 1599,
      "itemType": "preprint",
      "title": "CycleGANWM: A CycleGAN watermarking method for ownership verification",
      "abstractNote": "Due to the proliferation and widespread use of deep neural networks (DNN), their Intellectual Property Rights (IPR) protection has become increasingly important. This paper presents a novel model watermarking method for an unsupervised image-to-image translation (I2IT) networks, named CycleGAN, which leverage the image translation visual quality and watermark embedding. In this method, a watermark decoder is trained initially. Then the decoder is frozen and used to extract the watermark bits when training the CycleGAN watermarking model. The CycleGAN watermarking (CycleGANWM) is trained with specific loss functions and optimized to get a good performance on both I2IT task and watermark embedding. For watermark verification, this work uses statistical significance test to identify the ownership of the model from the extract watermark bits. We evaluate the robustness of the model against image post-processing and improve it by fine-tuning the model with adding data augmentation on the output images before extracting the watermark bits. We also carry out surrogate model attack under black-box access of the model. The experimental results prove that the proposed method is effective and robust to some image post-processing, and it is able to resist surrogate model attack.",
      "date": "2022-12-09",
      "shortTitle": "CycleGANWM",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/2211.13737",
      "accessDate": "2023-04-16T14:39:04Z",
      "extra": "arXiv:2211.13737 [cs]",
      "DOI": "10.48550/arXiv.2211.13737",
      "repository": "arXiv",
      "archiveID": "arXiv:2211.13737",
      "creators": [
        {
          "firstName": "Dongdong",
          "lastName": "Lin",
          "creatorType": "author"
        },
        {
          "firstName": "Benedetta",
          "lastName": "Tondi",
          "creatorType": "author"
        },
        {
          "firstName": "Bin",
          "lastName": "Li",
          "creatorType": "author"
        },
        {
          "firstName": "Mauro",
          "lastName": "Barni",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Cryptography and Security",
          "type": 1
        }
      ],
      "relations": [],
      "dateAdded": "2023-04-16T14:39:04Z",
      "dateModified": "2023-04-16T14:39:04Z",
      "uri": "http://zotero.org/groups/4922950/items/94KNAXSS",
      "itemID": 3421,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "arXiv.org Snapshot",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-04-16T14:39:20Z",
          "dateModified": "2023-04-16T14:39:20Z",
          "uri": "http://zotero.org/groups/4922950/items/QHJGCHZ7",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\QHJGCHZ7\\2211.html",
          "select": "zotero://select/groups/4922950/items/QHJGCHZ7"
        }
      ],
      "notes": [
        {
          "key": "AWWHEVT2",
          "version": 1599,
          "itemType": "note",
          "parentItem": "94KNAXSS",
          "note": "Comment: There is an crucial error in Figure 1, where the \"watermark\" should be modified",
          "tags": [],
          "relations": {},
          "dateAdded": "2023-04-16T14:39:04Z",
          "dateModified": "2023-04-16T14:39:04Z",
          "uri": "http://zotero.org/groups/4922950/items/AWWHEVT2"
        }
      ],
      "citationKey": "linCycleGANWMCycleGANWatermarking2022",
      "itemKey": "94KNAXSS",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/94KNAXSS"
    },
    {
      "key": "WHCVGYI5",
      "version": 1608,
      "itemType": "preprint",
      "title": "CycleGANWM: A CycleGAN watermarking method for ownership verification",
      "abstractNote": "Due to the proliferation and widespread use of deep neural networks (DNN), their Intellectual Property Rights (IPR) protection has become increasingly important. This paper presents a novel model watermarking method for an unsupervised image-to-image translation (I2IT) networks, named CycleGAN, which leverage the image translation visual quality and watermark embedding. In this method, a watermark decoder is trained initially. Then the decoder is frozen and used to extract the watermark bits when training the CycleGAN watermarking model. The CycleGAN watermarking (CycleGANWM) is trained with speciﬁc loss functions and optimized to get a good performance on both I2IT task and watermark embedding. For watermark veriﬁcation, this work uses statistical signiﬁcance test to identify the ownership of the model from the extract watermark bits. We evaluate the robustness of the model against image post-processing and improve it by ﬁne-tuning the model with adding data augmentation on the output images before extracting the watermark bits. We also carry out surrogate model attack under black-box access of the model. The experimental results prove that the proposed method is effective and robust to some image post-processing, and it is able to resist surrogate model attack.",
      "date": "2022-12-09",
      "language": "en",
      "shortTitle": "CycleGANWM",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/2211.13737",
      "accessDate": "2023-04-16T15:32:51Z",
      "extra": "arXiv:2211.13737 [cs]",
      "repository": "arXiv",
      "archiveID": "arXiv:2211.13737",
      "creators": [
        {
          "firstName": "Dongdong",
          "lastName": "Lin",
          "creatorType": "author"
        },
        {
          "firstName": "Benedetta",
          "lastName": "Tondi",
          "creatorType": "author"
        },
        {
          "firstName": "Bin",
          "lastName": "Li",
          "creatorType": "author"
        },
        {
          "firstName": "Mauro",
          "lastName": "Barni",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Cryptography and Security",
          "type": 1
        }
      ],
      "relations": [],
      "dateAdded": "2023-04-16T15:32:51Z",
      "dateModified": "2023-04-16T15:32:52Z",
      "uri": "http://zotero.org/groups/4922950/items/WHCVGYI5",
      "itemID": 3433,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "Lin 等 - 2022 - CycleGANWM A CycleGAN watermarking method for own.pdf",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-04-16T15:32:43Z",
          "dateModified": "2023-04-16T15:32:53Z",
          "uri": "http://zotero.org/groups/4922950/items/BERH7TCE",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\BERH7TCE\\Lin 等 - 2022 - CycleGANWM A CycleGAN watermarking method for own.pdf",
          "select": "zotero://select/groups/4922950/items/BERH7TCE"
        }
      ],
      "notes": [
        {
          "key": "C2QATAYP",
          "version": 1608,
          "itemType": "note",
          "parentItem": "WHCVGYI5",
          "note": "Comment: There is an crucial error in Figure 1, where the \"watermark\" should be modified",
          "tags": [],
          "relations": {},
          "dateAdded": "2023-04-16T15:32:51Z",
          "dateModified": "2023-04-16T15:32:51Z",
          "uri": "http://zotero.org/groups/4922950/items/C2QATAYP"
        }
      ],
      "citationKey": "linCycleGANWMCycleGANWatermarking2022a",
      "itemKey": "WHCVGYI5",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/WHCVGYI5"
    },
    {
      "key": "3PVFJM7R",
      "version": 1598,
      "itemType": "preprint",
      "title": "A Novel Verifiable Fingerprinting Scheme for Generative Adversarial Networks",
      "abstractNote": "This paper presents a novel fingerprinting scheme for the Intellectual Property (IP) protection of Generative Adversarial Networks (GANs). Prior solutions for classification models adopt adversarial examples as the fingerprints, which can raise stealthiness and robustness problems when they are applied to the GAN models. Our scheme constructs a composite deep learning model from the target GAN and a classifier. Then we generate stealthy fingerprint samples from this composite model, and register them to the classifier for effective ownership verification. This scheme inspires three concrete methodologies to practically protect the modern GAN models. Theoretical analysis proves that these methods can satisfy different security requirements necessary for IP protection. We also conduct extensive experiments to show that our solutions outperform existing strategies in terms of stealthiness, functionality-preserving and unremovability.",
      "date": "2021-07-30",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/2106.11760",
      "accessDate": "2023-04-12T13:53:53Z",
      "extra": "arXiv:2106.11760 [cs]",
      "repository": "arXiv",
      "archiveID": "arXiv:2106.11760",
      "creators": [
        {
          "firstName": "Guanlin",
          "lastName": "Li",
          "creatorType": "author"
        },
        {
          "firstName": "Guowen",
          "lastName": "Xu",
          "creatorType": "author"
        },
        {
          "firstName": "Han",
          "lastName": "Qiu",
          "creatorType": "author"
        },
        {
          "firstName": "Shangwei",
          "lastName": "Guo",
          "creatorType": "author"
        },
        {
          "firstName": "Run",
          "lastName": "Wang",
          "creatorType": "author"
        },
        {
          "firstName": "Jiwei",
          "lastName": "Li",
          "creatorType": "author"
        },
        {
          "firstName": "Tianwei",
          "lastName": "Zhang",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Machine Learning",
          "type": 1
        },
        {
          "tag": "Computer Science - Cryptography and Security",
          "type": 1
        },
        {
          "tag": "Computer Science - Artificial Intelligence",
          "type": 1
        },
        {
          "tag": "Computer Science - Computer Vision and Pattern Recognition",
          "type": 1
        }
      ],
      "relations": [],
      "dateAdded": "2023-04-12T13:53:53Z",
      "dateModified": "2023-04-12T13:53:53Z",
      "uri": "http://zotero.org/groups/4922950/items/3PVFJM7R",
      "itemID": 3116,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "arXiv Fulltext PDF",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-04-12T13:54:03Z",
          "dateModified": "2023-04-12T13:54:03Z",
          "uri": "http://zotero.org/groups/4922950/items/YASRU9RG",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\YASRU9RG\\Li 等 - 2021 - A Novel Verifiable Fingerprinting Scheme for Gener.pdf",
          "select": "zotero://select/groups/4922950/items/YASRU9RG"
        },
        {
          "itemType": "attachment",
          "title": "arXiv.org Snapshot",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-04-12T13:54:10Z",
          "dateModified": "2023-04-12T13:54:10Z",
          "uri": "http://zotero.org/groups/4922950/items/TTEUUVBG",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\TTEUUVBG\\2106.html",
          "select": "zotero://select/groups/4922950/items/TTEUUVBG"
        }
      ],
      "notes": [
        {
          "key": "8UMMSCQD",
          "version": 1407,
          "itemType": "note",
          "parentItem": "3PVFJM7R",
          "note": "<div data-citation-items=\"%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2F3PVFJM7R%22%5D%2C%22itemData%22%3A%7B%22id%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2F3PVFJM7R%22%2C%22type%22%3A%22article%22%2C%22abstract%22%3A%22This%20paper%20presents%20a%20novel%20fingerprinting%20scheme%20for%20the%20Intellectual%20Property%20(IP)%20protection%20of%20Generative%20Adversarial%20Networks%20(GANs).%20Prior%20solutions%20for%20classification%20models%20adopt%20adversarial%20examples%20as%20the%20fingerprints%2C%20which%20can%20raise%20stealthiness%20and%20robustness%20problems%20when%20they%20are%20applied%20to%20the%20GAN%20models.%20Our%20scheme%20constructs%20a%20composite%20deep%20learning%20model%20from%20the%20target%20GAN%20and%20a%20classifier.%20Then%20we%20generate%20stealthy%20fingerprint%20samples%20from%20this%20composite%20model%2C%20and%20register%20them%20to%20the%20classifier%20for%20effective%20ownership%20verification.%20This%20scheme%20inspires%20three%20concrete%20methodologies%20to%20practically%20protect%20the%20modern%20GAN%20models.%20Theoretical%20analysis%20proves%20that%20these%20methods%20can%20satisfy%20different%20security%20requirements%20necessary%20for%20IP%20protection.%20We%20also%20conduct%20extensive%20experiments%20to%20show%20that%20our%20solutions%20outperform%20existing%20strategies%20in%20terms%20of%20stealthiness%2C%20functionality-preserving%20and%20unremovability.%22%2C%22note%22%3A%22arXiv%3A2106.11760%20%5Bcs%5D%22%2C%22number%22%3A%22arXiv%3A2106.11760%22%2C%22publisher%22%3A%22arXiv%22%2C%22source%22%3A%22arXiv.org%22%2C%22title%22%3A%22A%20Novel%20Verifiable%20Fingerprinting%20Scheme%20for%20Generative%20Adversarial%20Networks%22%2C%22URL%22%3A%22http%3A%2F%2Farxiv.org%2Fabs%2F2106.11760%22%2C%22author%22%3A%5B%7B%22family%22%3A%22Li%22%2C%22given%22%3A%22Guanlin%22%7D%2C%7B%22family%22%3A%22Xu%22%2C%22given%22%3A%22Guowen%22%7D%2C%7B%22family%22%3A%22Qiu%22%2C%22given%22%3A%22Han%22%7D%2C%7B%22family%22%3A%22Guo%22%2C%22given%22%3A%22Shangwei%22%7D%2C%7B%22family%22%3A%22Wang%22%2C%22given%22%3A%22Run%22%7D%2C%7B%22family%22%3A%22Li%22%2C%22given%22%3A%22Jiwei%22%7D%2C%7B%22family%22%3A%22Zhang%22%2C%22given%22%3A%22Tianwei%22%7D%5D%2C%22accessed%22%3A%7B%22date-parts%22%3A%5B%5B%222023%22%2C4%2C12%5D%5D%7D%2C%22issued%22%3A%7B%22date-parts%22%3A%5B%5B%222021%22%2C7%2C30%5D%5D%7D%2C%22citation-key%22%3A%22liNovelVerifiableFingerprinting2021%22%7D%7D%5D\" data-schema-version=\"8\"><h1>Composite</h1>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FYASRU9RG%22%2C%22annotationKey%22%3A%2226A8C28T%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B120.497%2C565.985%2C301.065%2C574.892%5D%2C%5B48.959%2C555.026%2C233.865%2C563.933%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2F3PVFJM7R%22%5D%2C%22locator%22%3A%222%22%7D%7D\">“composite deep learning model constructed from the target GAN model and a classifier”</span></p>\n<p>这里提到了conposite 的概念，其实从图中可以看到</p>\n<p><img alt=\"\" data-attachment-key=\"9TLG9HGA\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FYASRU9RG%22%2C%22annotationKey%22%3A%222N955XM4%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B312.273%2C487.682%2C579.773%2C628.136%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2F3PVFJM7R%22%5D%2C%22locator%22%3A%222%22%7D%7D\" width=\"446\" height=\"234\">f不同于一般针对于classifier的fingerprint，这里使用了GAN与classifer的复合结构来组成框架，对此，作者给出了一个比较直观的说明</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FYASRU9RG%22%2C%22annotationKey%22%3A%222BKZ9Z7X%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B238.398%2C544.067%2C301.063%2C552.974%5D%2C%5B48.959%2C533.108%2C301.063%2C542.015%5D%2C%5B48.959%2C522.149%2C301.063%2C531.056%5D%2C%5B48.959%2C511.19%2C102.169%2C520.097%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2F3PVFJM7R%22%5D%2C%22locator%22%3A%222%22%7D%7D\">“aim to design a set of fingerprints, whose input samples and output samples from the target model are visually indistinguishable from normal ones.”</span> 攻击的思想，差别要小，要隐蔽。</p>\n<p></p>\n<p><img alt=\"\" data-attachment-key=\"JWMI4792\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FYASRU9RG%22%2C%22annotationKey%22%3A%22TRYYNCRC%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B45.682%2C444.045%2C309.773%2C521.773%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2F3PVFJM7R%22%5D%2C%22locator%22%3A%222%22%7D%7D\" width=\"440\" height=\"130\">而相比于针对classifier的fingerprint，在要求了GAN的输出区别要与正常的输出差别很小的前提下，好像GAN的fingerprint无法检测，所以采用一个复合的结构，多一个额外的classifier来分辨输出</p>\n<hr>\n<p>同时作者还提出有了classifier不仅可以让差别很小来提高隐蔽性，还可以提高效率与鲁棒性，但我觉得这个说法有点牵强</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FYASRU9RG%22%2C%22annotationKey%22%3A%2237K5THQX%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B163.778%2C428.999%2C301.063%2C437.906%5D%2C%5B48.959%2C418.04%2C301.063%2C426.947%5D%2C%5B48.959%2C407.081%2C157.382%2C415.988%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2F3PVFJM7R%22%5D%2C%22locator%22%3A%222%22%7D%7D\">“classifier can provide stealthiness for the fingerprints. Besides, it can also enhance their effectiveness and robustness”</span></p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FYASRU9RG%22%2C%22annotationKey%22%3A%22PZL5IVJM%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B219.668%2C407.081%2C301.063%2C415.988%5D%2C%5B48.959%2C396.122%2C301.063%2C405.029%5D%2C%5B48.959%2C385.163%2C301.063%2C394.07%5D%2C%5B48.959%2C374.204%2C301.063%2C383.111%5D%2C%5B48.959%2C363.245%2C163.12%2C372.152%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2F3PVFJM7R%22%5D%2C%22locator%22%3A%222%22%7D%7D\">“model owner is not permitted to change the target model, he can freely modify the classifier to better recognize the fingerprint output even the adversary performs certain transformations over the target model or inference samples”</span></p>\n<p></p>\n<h1>Special IDEA1</h1>\n<p>首先这里要求的是composite中的classifier能分辨出fingerprint的sample，而这里并不是说让classifier去直接判断这个模型是否被fingerprint，而是说出于隐蔽性的考虑让fingerprint samples能够产生特殊的效果，让自己的classifer产生反应，又让其他的无法察觉。</p>\n<p>这就又与adversarial attack的思想对上了，adversarial中强调说无法分辨这个样本是否被改变，但是这个样本可以让模型出错</p>\n<p>这里也可以说是让其他人无法察觉我正在进行验证，而我的classifier可以对这个fingerprint做出特殊的反应</p>\n<hr>\n<p>而作者接下来提出的3个method也就是针对我们classifier设计的adversarial attack与backdoor attack（回想一下，在backdoor综述中，提到过可以用于知识产权保护）</p>\n<p></p>\n<h1>CFP-AE</h1>\n<p>核心思想是让GAN产生针对classifier的adversarial sample</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FYASRU9RG%22%2C%22annotationKey%22%3A%22BBQ4YDNU%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%226%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B528.071%2C592.183%2C571.1%2C601.09%5D%2C%5B318.996%2C581.224%2C571.1%2C590.131%5D%2C%5B318.996%2C570.266%2C405.74%2C579.173%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2F3PVFJM7R%22%5D%2C%22locator%22%3A%226%22%7D%7D\">“propose to make the target GAN model generate adversarial examples against the classifier.”</span></p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FYASRU9RG%22%2C%22annotationKey%22%3A%22P8CIZ8I5%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%226%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B430.268%2C570.266%2C571.1%2C579.173%5D%2C%5B318.996%2C559.307%2C571.1%2C568.214%5D%2C%5B318.996%2C548.348%2C456.948%2C557.255%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2F3PVFJM7R%22%5D%2C%22locator%22%3A%226%22%7D%7D\">“output sample of the target GAN model looks normal, while the output label of the classifier is unique as the ownership evidence”</span></p>\n<p></p>\n<p>首先训练一个 ground truth classifier f，然后再均匀的在D空间中进行随机采样得到x，这个空间足够大，因此攻击者不能推断出这些采样的样本</p>\n<p>然后通过优化算法（adversarial optimization？）得到v</p>\n<p>要求差别要小，因此还要最小化一下这个式子</p>\n<p><img alt=\"\" data-attachment-key=\"L69P463Z\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FYASRU9RG%22%2C%22annotationKey%22%3A%22BLFWRF8Z%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%225%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B322.273%2C308.591%2C570.909%2C338.136%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2F3PVFJM7R%22%5D%2C%22locator%22%3A%225%22%7D%7D\" width=\"414\" height=\"49\">同时攻击要最大化label的差别</p>\n<p>最终综合两个目标，构造了一个新的损失函数</p>\n<p></p>\n<p><img alt=\"\" data-attachment-key=\"SH6SQTY2\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FYASRU9RG%22%2C%22annotationKey%22%3A%222JJL3UWS%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%226%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B322.866%2C325.72%2C576.037%2C396.146%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2F3PVFJM7R%22%5D%2C%22locator%22%3A%226%22%7D%7D\" width=\"422\" height=\"117\"></p>\n<p></p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FYASRU9RG%22%2C%22annotationKey%22%3A%22IUGYVWJ9%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%226%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B507%2C101.671%2C571.099%2C110.578%5D%2C%5B318.997%2C90.016%2C553.023%2C99.778%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2F3PVFJM7R%22%5D%2C%22locator%22%3A%226%22%7D%7D\">“do not need to modify the classifier f after we perform the Fgen function”</span>这里通过adversarial attack找到v就结束了，不需要再进行register</p>\n<p>也就是说classifier这里并不需要特别区分？其实只需要注意目标GAN会产生对抗样本而其他模型不会产生反应？但对抗样本的鲁棒性会不会导致这个前提不存在？</p>\n<p></p>\n<h1>CEP</h1>\n</div>",
          "tags": [],
          "relations": {},
          "dateAdded": "2023-04-13T07:48:47Z",
          "dateModified": "2023-04-13T09:55:17Z",
          "uri": "http://zotero.org/groups/4922950/items/8UMMSCQD"
        }
      ],
      "citationKey": "liNovelVerifiableFingerprinting2021",
      "itemKey": "3PVFJM7R",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/3PVFJM7R"
    },
    {
      "key": "YDJ5SYJ6",
      "version": 1792,
      "itemType": "preprint",
      "title": "PTW: Pivotal Tuning Watermarking for Pre-Trained Image Generators",
      "abstractNote": "Deepfakes refer to content synthesized using deep generators, which, when \\emph{misused}, have the potential to erode trust in digital media. Synthesizing high-quality deepfakes requires access to large and complex generators only few entities can train and provide. The threat are malicious users that exploit access to the provided model and generate harmful deepfakes without risking detection. Watermarking makes deepfakes detectable by embedding an identifiable code into the generator that is later extractable from its generated images. We propose Pivotal Tuning Watermarking (PTW), a method for watermarking pre-trained generators (i) three orders of magnitude faster than watermarking from scratch and (ii) without the need for any training data. We improve existing watermarking methods and scale to generators $4 \\times$ larger than related work. PTW can embed longer codes than existing methods while better preserving the generator's image quality. We propose rigorous, game-based definitions for robustness and undetectability and our study reveals that watermarking is not robust against an adaptive white-box attacker who has control over the generator's parameters. We propose an adaptive attack that can successfully remove any watermarking with access to only $200$ non-watermarked images. Our work challenges the trustworthiness of watermarking for deepfake detection when the parameters of a generator are available.",
      "date": "2023-04-14",
      "shortTitle": "PTW",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/2304.07361",
      "accessDate": "2023-04-23T12:25:03Z",
      "extra": "arXiv:2304.07361 [cs]",
      "DOI": "10.48550/arXiv.2304.07361",
      "repository": "arXiv",
      "archiveID": "arXiv:2304.07361",
      "creators": [
        {
          "firstName": "Nils",
          "lastName": "Lukas",
          "creatorType": "author"
        },
        {
          "firstName": "Florian",
          "lastName": "Kerschbaum",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Machine Learning",
          "type": 1
        },
        {
          "tag": "Computer Science - Cryptography and Security",
          "type": 1
        }
      ],
      "relations": [],
      "dateAdded": "2023-04-23T12:25:03Z",
      "dateModified": "2023-04-23T12:25:05Z",
      "uri": "http://zotero.org/groups/4922950/items/YDJ5SYJ6",
      "itemID": 3578,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "arXiv Fulltext PDF",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-04-23T12:25:30Z",
          "dateModified": "2023-04-23T12:25:30Z",
          "uri": "http://zotero.org/groups/4922950/items/3428CSHQ",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\3428CSHQ\\Lukas 和 Kerschbaum - 2023 - PTW Pivotal Tuning Watermarking for Pre-Trained I.pdf",
          "select": "zotero://select/groups/4922950/items/3428CSHQ"
        },
        {
          "itemType": "attachment",
          "title": "arXiv.org Snapshot",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-04-23T12:25:45Z",
          "dateModified": "2023-04-23T12:25:45Z",
          "uri": "http://zotero.org/groups/4922950/items/VQ2FQ8D2",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\VQ2FQ8D2\\2304.html",
          "select": "zotero://select/groups/4922950/items/VQ2FQ8D2"
        }
      ],
      "notes": [],
      "citationKey": "lukasPTWPivotalTuning2023",
      "itemKey": "YDJ5SYJ6",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/YDJ5SYJ6"
    },
    {
      "key": "DTST2J6D",
      "version": 686,
      "itemType": "preprint",
      "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
      "abstractNote": "Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at https://github.com/MadryLab/mnist_challenge and https://github.com/MadryLab/cifar10_challenge.",
      "date": "2019-09-04",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/1706.06083",
      "accessDate": "2023-03-20T12:55:31Z",
      "extra": "arXiv:1706.06083 [cs, stat]",
      "DOI": "10.48550/arXiv.1706.06083",
      "repository": "arXiv",
      "archiveID": "arXiv:1706.06083",
      "creators": [
        {
          "firstName": "Aleksander",
          "lastName": "Madry",
          "creatorType": "author"
        },
        {
          "firstName": "Aleksandar",
          "lastName": "Makelov",
          "creatorType": "author"
        },
        {
          "firstName": "Ludwig",
          "lastName": "Schmidt",
          "creatorType": "author"
        },
        {
          "firstName": "Dimitris",
          "lastName": "Tsipras",
          "creatorType": "author"
        },
        {
          "firstName": "Adrian",
          "lastName": "Vladu",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Machine Learning",
          "type": 1
        },
        {
          "tag": "Statistics - Machine Learning",
          "type": 1
        },
        {
          "tag": "Computer Science - Neural and Evolutionary Computing",
          "type": 1
        }
      ],
      "relations": [],
      "dateAdded": "2023-03-20T12:55:31Z",
      "dateModified": "2023-03-20T12:55:31Z",
      "uri": "http://zotero.org/groups/4922950/items/DTST2J6D",
      "itemID": 2475,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "arXiv Fulltext PDF",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-03-20T12:55:36Z",
          "dateModified": "2023-03-20T12:55:36Z",
          "uri": "http://zotero.org/groups/4922950/items/AVERHSNP",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\AVERHSNP\\Madry 等 - 2019 - Towards Deep Learning Models Resistant to Adversar.pdf",
          "select": "zotero://select/groups/4922950/items/AVERHSNP"
        },
        {
          "itemType": "attachment",
          "title": "arXiv.org Snapshot",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-03-20T12:55:49Z",
          "dateModified": "2023-03-20T12:55:49Z",
          "uri": "http://zotero.org/groups/4922950/items/4LDQ6MUN",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\4LDQ6MUN\\1706.html",
          "select": "zotero://select/groups/4922950/items/4LDQ6MUN"
        }
      ],
      "notes": [
        {
          "key": "LBN4D7VU",
          "version": 686,
          "itemType": "note",
          "parentItem": "DTST2J6D",
          "note": "Comment: ICLR'18",
          "tags": [],
          "relations": {},
          "dateAdded": "2023-03-20T12:55:31Z",
          "dateModified": "2023-03-20T12:55:31Z",
          "uri": "http://zotero.org/groups/4922950/items/LBN4D7VU"
        },
        {
          "key": "VHA8VP5K",
          "version": 854,
          "itemType": "note",
          "parentItem": "DTST2J6D",
          "note": "<div data-citation-items=\"%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FDTST2J6D%22%5D%2C%22itemData%22%3A%7B%22id%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FDTST2J6D%22%2C%22type%22%3A%22article%22%2C%22abstract%22%3A%22Recent%20work%20has%20demonstrated%20that%20deep%20neural%20networks%20are%20vulnerable%20to%20adversarial%20examples---inputs%20that%20are%20almost%20indistinguishable%20from%20natural%20data%20and%20yet%20classified%20incorrectly%20by%20the%20network.%20In%20fact%2C%20some%20of%20the%20latest%20findings%20suggest%20that%20the%20existence%20of%20adversarial%20attacks%20may%20be%20an%20inherent%20weakness%20of%20deep%20learning%20models.%20To%20address%20this%20problem%2C%20we%20study%20the%20adversarial%20robustness%20of%20neural%20networks%20through%20the%20lens%20of%20robust%20optimization.%20This%20approach%20provides%20us%20with%20a%20broad%20and%20unifying%20view%20on%20much%20of%20the%20prior%20work%20on%20this%20topic.%20Its%20principled%20nature%20also%20enables%20us%20to%20identify%20methods%20for%20both%20training%20and%20attacking%20neural%20networks%20that%20are%20reliable%20and%2C%20in%20a%20certain%20sense%2C%20universal.%20In%20particular%2C%20they%20specify%20a%20concrete%20security%20guarantee%20that%20would%20protect%20against%20any%20adversary.%20These%20methods%20let%20us%20train%20networks%20with%20significantly%20improved%20resistance%20to%20a%20wide%20range%20of%20adversarial%20attacks.%20They%20also%20suggest%20the%20notion%20of%20security%20against%20a%20first-order%20adversary%20as%20a%20natural%20and%20broad%20security%20guarantee.%20We%20believe%20that%20robustness%20against%20such%20well-defined%20classes%20of%20adversaries%20is%20an%20important%20stepping%20stone%20towards%20fully%20resistant%20deep%20learning%20models.%20Code%20and%20pre-trained%20models%20are%20available%20at%20https%3A%2F%2Fgithub.com%2FMadryLab%2Fmnist_challenge%20and%20https%3A%2F%2Fgithub.com%2FMadryLab%2Fcifar10_challenge.%22%2C%22DOI%22%3A%2210.48550%2FarXiv.1706.06083%22%2C%22note%22%3A%22arXiv%3A1706.06083%20%5Bcs%2C%20stat%5D%22%2C%22number%22%3A%22arXiv%3A1706.06083%22%2C%22publisher%22%3A%22arXiv%22%2C%22source%22%3A%22arXiv.org%22%2C%22title%22%3A%22Towards%20Deep%20Learning%20Models%20Resistant%20to%20Adversarial%20Attacks%22%2C%22URL%22%3A%22http%3A%2F%2Farxiv.org%2Fabs%2F1706.06083%22%2C%22author%22%3A%5B%7B%22family%22%3A%22Madry%22%2C%22given%22%3A%22Aleksander%22%7D%2C%7B%22family%22%3A%22Makelov%22%2C%22given%22%3A%22Aleksandar%22%7D%2C%7B%22family%22%3A%22Schmidt%22%2C%22given%22%3A%22Ludwig%22%7D%2C%7B%22family%22%3A%22Tsipras%22%2C%22given%22%3A%22Dimitris%22%7D%2C%7B%22family%22%3A%22Vladu%22%2C%22given%22%3A%22Adrian%22%7D%5D%2C%22accessed%22%3A%7B%22date-parts%22%3A%5B%5B%222023%22%2C3%2C20%5D%5D%7D%2C%22issued%22%3A%7B%22date-parts%22%3A%5B%5B%222019%22%2C9%2C4%5D%5D%7D%7D%7D%5D\" data-schema-version=\"8\"><h1>Motivation</h1>\n<p>讲了领域中遇到的问题，以及前人方法的缺陷：<br>总结来说，模型容易被很小的扰动攻击（废话）。<br>然后其他的防御方式并不能提供一种安全性的保证（重点！）<br>原因在于无法预测攻击者是否找到最具对抗性的样本，或者用特定的攻击（设计出来的防御机制是争对特定的攻击）</p>\n<h1>Contribution</h1>\n<p>因此作者提出了一种</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FAVERHSNP%22%2C%22annotationKey%22%3A%22IRTQZBK9%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B218.733%2C493.545%2C388.558%2C504.421%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FDTST2J6D%22%5D%2C%22locator%22%3A%222%22%7D%7D\">“saddle point (min-max) formulation”</span></p>\n<p><img alt=\"\" data-attachment-key=\"H98LQWXL\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FAVERHSNP%22%2C%22annotationKey%22%3A%22L8ZUN3RG%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B151.154%2C85.846%2C465%2C130.269%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FDTST2J6D%22%5D%2C%22locator%22%3A%223%22%7D%7D\" width=\"523\" height=\"74\"></p>\n<p>作者的这套公式框架，相当于对模型的攻击与防御提供了一个理论体系。这套框架也对前人的工作adversarial example adversarial training做了很好的概括</p>\n<p></p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FAVERHSNP%22%2C%22annotationKey%22%3A%22ITXGUPL4%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B172.144%2C529.826%2C356.987%2C540.702%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FDTST2J6D%22%5D%2C%22locator%22%3A%224%22%7D%7D\">“quantitative measure of its robustness”</span> 这个新定义的目标函数可以作为量化鲁棒性的指标</p>\n<hr>\n<p>然后研究了这个优化问题的可解性（*SECTION 3*）</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FAVERHSNP%22%2C%22annotationKey%22%3A%22SXFDUAQL%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%225%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B312.124%2C468.669%2C540%2C479.621%5D%2C%5B72%2C455.12%2C387.983%2C466.072%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FDTST2J6D%22%5D%2C%22locator%22%3A%225%22%7D%7D\">“One of our key contributions is demonstrating that, in practice, one can solve the saddle point problem after all”</span> 文章的贡献就在于证明了这个式子能找到很好的解<br><br>所以为啥这个称为sadlle point problem</p>\n<p>提出了一种恶意攻击的方法 PGD</p>\n<p><img alt=\"\" data-attachment-key=\"ERJ2EZZD\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FAVERHSNP%22%2C%22annotationKey%22%3A%22YUP4Y6ET%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B196.731%2C143.53799999999995%2C408.46200000000005%2C169.7884615384615%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FDTST2J6D%22%5D%2C%22locator%22%3A%224%22%7D%7D\" width=\"353\" height=\"44\"><br></p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FAVERHSNP%22%2C%22annotationKey%22%3A%22YFYM333L%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%225%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B280.543%2C414.472%2C540.216%2C425.348%5D%2C%5B72%2C400.923%2C119.307%2C411.799%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FDTST2J6D%22%5D%2C%22locator%22%3A%225%22%7D%7D\">“projected gradient descent as the “ultimate” first-order adversary”</span> 同时证明了 PGD很强</p>\n<p></p>\n<hr>\n<p>研究了模型结构的影响：容量很重要</p>\n<p>对鲁棒性以及对抗训练过程都很重要（两个概念有点重复）</p>\n<p>*SECTION 4*</p>\n<p>顺带分析训练方法也很重要（FGSM不行）</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FAVERHSNP%22%2C%22annotationKey%22%3A%222MSXIALK%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%2210%22%2C%22position%22%3A%7B%22pageIndex%22%3A9%2C%22rects%22%3A%5B%5B72%2C675.788%2C361.398%2C686.664%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FDTST2J6D%22%5D%2C%22locator%22%3A%2210%22%7D%7D\">“FGSM adversaries don’t increase robustness (for large ε).”</span></p>\n<hr>\n<p>然后做了很多实验</p>\n<p></p>\n<p></p>\n<p></p>\n<p></p>\n<p></p>\n<p></p>\n<p></p>\n<p></p>\n<p></p>\n<p></p>\n<p></p>\n<p></p>\n<h1>问题</h1>\n<p><span class=\"citation\" data-citation=\"%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FDTST2J6D%22%5D%2C%22locator%22%3A%222%22%7D%5D%2C%22properties%22%3A%7B%7D%7D\">(<span class=\"citation-item\">Madry 等, 2019, p. 2</span>)</span> 以上讲了领域中遇到的问题，以及前人方法的缺陷：<br>总结来说，模型容易被很小的扰动攻击（废话）。<br>然后其他的防御方式并不能提供一种安全性的保证（重点！）<br>原因在于无法预测攻击者是否找到最具对抗性的样本，或者用特定的攻击（设计出来的防御机制是争对特定的攻击）</p>\n<p>关于核心概念的理解问题：</p>\n<p>首先对于PGD的具体算法，不是很清楚</p>\n<hr>\n<p>关于Linear的理解</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FAVERHSNP%22%2C%22annotationKey%22%3A%2278RMHG7Y%22%2C%22color%22%3A%22%235fb236%22%2C%22pageLabel%22%3A%225%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B182.238%2C294.927%2C444.134%2C305.803%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FDTST2J6D%22%5D%2C%22locator%22%3A%225%22%7D%7D\">“resorted to linearizing the inner maximization problem”</span> linearizing啥意思啊</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FAVERHSNP%22%2C%22annotationKey%22%3A%22E2ZCRJXI%22%2C%22color%22%3A%22%235fb236%22%2C%22pageLabel%22%3A%225%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B145.832%2C281.378%2C464.25%2C292.254%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FDTST2J6D%22%5D%2C%22locator%22%3A%225%22%7D%7D\">“linearization approach yields well-known methods such as FGSM”</span></p>\n<p>这里提到FGSM是linearization approach</p>\n<p></p>\n<p>以及最后对FGSMlinear缺点的论述：</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FAVERHSNP%22%2C%22annotationKey%22%3A%22JSPRXY2C%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%2210%22%2C%22position%22%3A%7B%22pageIndex%22%3A9%2C%22rects%22%3A%5B%5B250.943%2C608.042%2C540.001%2C618.918%5D%2C%5B72.136%2C593.939%2C540.427%2C605.458%5D%2C%5B72%2C580.944%2C337.506%2C591.82%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FDTST2J6D%22%5D%2C%22locator%22%3A%2210%22%7D%7D\">“For the case of smaller ε the loss is ofter linear enough in the `∞-ball around natural examples, that FGSM finds adversarial examples close to those found by PGD thus being a reasonable adversary to train against”</span> 因为FGSM是线性去找,而 epsilon小的时候要找的东西本来就比较线性，因此PGD和FGSM找到的东西差不多？？？</p>\n<p></p>\n<hr>\n<p>对于one-step概念的理解</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FAVERHSNP%22%2C%22annotationKey%22%3A%2255ESIYGA%22%2C%22color%22%3A%22%235fb236%22%2C%22pageLabel%22%3A%225%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B151.736%2C254.279%2C260.266%2C265.155%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FDTST2J6D%22%5D%2C%22locator%22%3A%225%22%7D%7D\">“this one-step approach”</span> 为什么称FGSM为 one-step approach？是相对于迭代版本来说的吗</p>\n<p></p>\n<hr>\n<p>对于first-order method的理解</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FAVERHSNP%22%2C%22annotationKey%22%3A%22HJEX5BRR%22%2C%22color%22%3A%22%235fb236%22%2C%22pageLabel%22%3A%225%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B162.164%2C145.886%2C257.781%2C156.762%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FDTST2J6D%22%5D%2C%22locator%22%3A%225%22%7D%7D\">“first-order methods”</span></p>\n<p>文中，FGSM PGD这些都称为first order method</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FAVERHSNP%22%2C%22annotationKey%22%3A%229JQ4RFXQ%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%227%22%2C%22position%22%3A%7B%22pageIndex%22%3A6%2C%22rects%22%3A%5B%5B72%2C109.822%2C398.808%2C120.698%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FDTST2J6D%22%5D%2C%22locator%22%3A%227%22%7D%7D\">“of optimization problems in ML is solved with first-order methods”</span></p>\n</div>",
          "tags": [],
          "relations": {},
          "dateAdded": "2023-03-28T13:17:47Z",
          "dateModified": "2023-03-28T13:34:33Z",
          "uri": "http://zotero.org/groups/4922950/items/VHA8VP5K"
        }
      ],
      "citationKey": "madryDeepLearningModels2019",
      "itemKey": "DTST2J6D",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/DTST2J6D"
    },
    {
      "key": "GJYIPW3J",
      "version": 19,
      "itemType": "conferencePaper",
      "title": "Background Class Defense Against Adversarial Examples",
      "abstractNote": "Adversarial examples allow crafted attacks against deep neural network classiﬁcation of images. We propose a defense of expanding the training set with a single, large, and diverse class of background images, striving to ‘ﬁll’ around the borders of the classiﬁcation boundary. We ﬁnd it aids detection of simple attacks on EMNIST, but not advanced attacks. We discuss several limitations of our examination.",
      "date": "5/2018",
      "language": "en",
      "libraryCatalog": "DOI.org (Crossref)",
      "url": "https://ieeexplore.ieee.org/document/8424639/",
      "accessDate": "2023-01-24T11:00:41Z",
      "place": "San Francisco, CA",
      "publisher": "IEEE",
      "ISBN": "978-1-5386-8276-0",
      "pages": "96-102",
      "proceedingsTitle": "2018 IEEE Security and Privacy Workshops (SPW)",
      "conferenceName": "2018 IEEE Security and Privacy Workshops (SPW)",
      "DOI": "10.1109/SPW.2018.00023",
      "creators": [
        {
          "firstName": "Michael",
          "lastName": "McCoyd",
          "creatorType": "author"
        },
        {
          "firstName": "David",
          "lastName": "Wagner",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": [],
      "dateAdded": "2023-01-24T11:00:41Z",
      "dateModified": "2023-01-24T11:00:41Z",
      "uri": "http://zotero.org/groups/4922950/items/GJYIPW3J",
      "itemID": 1759,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "McCoyd 和 Wagner - 2018 - Background Class Defense Against Adversarial Examp.pdf",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-01-24T11:00:37Z",
          "dateModified": "2023-01-24T11:00:41Z",
          "uri": "http://zotero.org/groups/4922950/items/Q5V4ADTP",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\Q5V4ADTP\\McCoyd 和 Wagner - 2018 - Background Class Defense Against Adversarial Examp.pdf",
          "select": "zotero://select/groups/4922950/items/Q5V4ADTP"
        }
      ],
      "notes": [],
      "citationKey": "mccoydBackgroundClassDefense2018",
      "itemKey": "GJYIPW3J",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/GJYIPW3J"
    },
    {
      "key": "BYYGT445",
      "version": 39,
      "itemType": "bookSection",
      "title": "Minority Reports Defense: Defending Against Adversarial Patches",
      "abstractNote": "Deep learning image classiﬁcation is widely used yet is vulnerable to adversarial attack, which can change the computer classiﬁcation without changing how humans classify the image. This is possible even if the attacker changes just a small patch of the image. We propose a defense against patch attacks based on partially occluding the image around each candidate patch location, so that a few occlusions each completely hide the patch. We demonstrate on CIFAR-10, Fashion MNIST, and MNIST that our defense provides certiﬁed security against patch attacks of a certain size. For CIFAR-10 and a 5 × 5 patch, we can provide certify accuracy for 43.8% of images, at a cost of only 1.6% in clean image accuracy compared to the architecture we defend or a cost of 0.1% compared to our training of that architecture, and a 0.2% false positive rate.",
      "date": "2020",
      "language": "en",
      "shortTitle": "Minority Reports Defense",
      "libraryCatalog": "DOI.org (Crossref)",
      "url": "http://link.springer.com/10.1007/978-3-030-61638-0_31",
      "accessDate": "2023-01-24T11:28:22Z",
      "extra": "Series Title: Lecture Notes in Computer Science\nDOI: 10.1007/978-3-030-61638-0_31",
      "volume": "12418",
      "place": "Cham",
      "publisher": "Springer International Publishing",
      "ISBN": "978-3-030-61637-3 978-3-030-61638-0",
      "pages": "564-582",
      "bookTitle": "Applied Cryptography and Network Security Workshops",
      "creators": [
        {
          "firstName": "Jianying",
          "lastName": "Zhou",
          "creatorType": "editor"
        },
        {
          "firstName": "Mauro",
          "lastName": "Conti",
          "creatorType": "editor"
        },
        {
          "firstName": "Chuadhry Mujeeb",
          "lastName": "Ahmed",
          "creatorType": "editor"
        },
        {
          "firstName": "Man Ho",
          "lastName": "Au",
          "creatorType": "editor"
        },
        {
          "firstName": "Lejla",
          "lastName": "Batina",
          "creatorType": "editor"
        },
        {
          "firstName": "Zhou",
          "lastName": "Li",
          "creatorType": "editor"
        },
        {
          "firstName": "Jingqiang",
          "lastName": "Lin",
          "creatorType": "editor"
        },
        {
          "firstName": "Eleonora",
          "lastName": "Losiouk",
          "creatorType": "editor"
        },
        {
          "firstName": "Bo",
          "lastName": "Luo",
          "creatorType": "editor"
        },
        {
          "firstName": "Suryadipta",
          "lastName": "Majumdar",
          "creatorType": "editor"
        },
        {
          "firstName": "Weizhi",
          "lastName": "Meng",
          "creatorType": "editor"
        },
        {
          "firstName": "Martín",
          "lastName": "Ochoa",
          "creatorType": "editor"
        },
        {
          "firstName": "Stjepan",
          "lastName": "Picek",
          "creatorType": "editor"
        },
        {
          "firstName": "Georgios",
          "lastName": "Portokalidis",
          "creatorType": "editor"
        },
        {
          "firstName": "Cong",
          "lastName": "Wang",
          "creatorType": "editor"
        },
        {
          "firstName": "Kehuan",
          "lastName": "Zhang",
          "creatorType": "editor"
        },
        {
          "firstName": "Michael",
          "lastName": "McCoyd",
          "creatorType": "author"
        },
        {
          "firstName": "Won",
          "lastName": "Park",
          "creatorType": "author"
        },
        {
          "firstName": "Steven",
          "lastName": "Chen",
          "creatorType": "author"
        },
        {
          "firstName": "Neil",
          "lastName": "Shah",
          "creatorType": "author"
        },
        {
          "firstName": "Ryan",
          "lastName": "Roggenkemper",
          "creatorType": "author"
        },
        {
          "firstName": "Minjune",
          "lastName": "Hwang",
          "creatorType": "author"
        },
        {
          "firstName": "Jason Xinyu",
          "lastName": "Liu",
          "creatorType": "author"
        },
        {
          "firstName": "David",
          "lastName": "Wagner",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": [],
      "dateAdded": "2023-01-24T11:28:22Z",
      "dateModified": "2023-01-24T11:28:22Z",
      "uri": "http://zotero.org/groups/4922950/items/BYYGT445",
      "itemID": 1772,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "McCoyd 等 - 2020 - Minority Reports Defense Defending Against Advers.pdf",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-01-24T11:28:17Z",
          "dateModified": "2023-01-24T11:28:23Z",
          "uri": "http://zotero.org/groups/4922950/items/YTMTHY76",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\YTMTHY76\\McCoyd 等 - 2020 - Minority Reports Defense Defending Against Advers.pdf",
          "select": "zotero://select/groups/4922950/items/YTMTHY76"
        }
      ],
      "notes": [],
      "citationKey": "mccoydMinorityReportsDefense2020",
      "itemKey": "BYYGT445",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/BYYGT445"
    },
    {
      "key": "H9NZQ58A",
      "version": 622,
      "itemType": "preprint",
      "title": "DeepFool: a simple and accurate method to fool deep neural networks",
      "abstractNote": "State-of-the-art deep neural networks have achieved impressive results on many image classification tasks. However, these same architectures have been shown to be unstable to small, well sought, perturbations of the images. Despite the importance of this phenomenon, no effective methods have been proposed to accurately compute the robustness of state-of-the-art deep classifiers to such perturbations on large-scale datasets. In this paper, we fill this gap and propose the DeepFool algorithm to efficiently compute perturbations that fool deep networks, and thus reliably quantify the robustness of these classifiers. Extensive experimental results show that our approach outperforms recent methods in the task of computing adversarial perturbations and making classifiers more robust.",
      "date": "2016-07-04",
      "shortTitle": "DeepFool",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/1511.04599",
      "accessDate": "2023-03-15T08:16:53Z",
      "extra": "arXiv:1511.04599 [cs]",
      "DOI": "10.48550/arXiv.1511.04599",
      "repository": "arXiv",
      "archiveID": "arXiv:1511.04599",
      "creators": [
        {
          "firstName": "Seyed-Mohsen",
          "lastName": "Moosavi-Dezfooli",
          "creatorType": "author"
        },
        {
          "firstName": "Alhussein",
          "lastName": "Fawzi",
          "creatorType": "author"
        },
        {
          "firstName": "Pascal",
          "lastName": "Frossard",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Machine Learning",
          "type": 1
        },
        {
          "tag": "Computer Science - Computer Vision and Pattern Recognition",
          "type": 1
        }
      ],
      "relations": [],
      "dateAdded": "2023-03-15T08:16:53Z",
      "dateModified": "2023-03-15T08:16:53Z",
      "uri": "http://zotero.org/groups/4922950/items/H9NZQ58A",
      "itemID": 2479,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "arXiv Fulltext PDF",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-03-15T08:17:06Z",
          "dateModified": "2023-03-15T08:17:06Z",
          "uri": "http://zotero.org/groups/4922950/items/T2L744X2",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\T2L744X2\\Moosavi-Dezfooli 等 - 2016 - DeepFool a simple and accurate method to fool dee.pdf",
          "select": "zotero://select/groups/4922950/items/T2L744X2"
        },
        {
          "itemType": "attachment",
          "title": "arXiv.org Snapshot",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-03-15T08:17:17Z",
          "dateModified": "2023-03-15T08:17:17Z",
          "uri": "http://zotero.org/groups/4922950/items/KAPL8LC3",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\KAPL8LC3\\1511.html",
          "select": "zotero://select/groups/4922950/items/KAPL8LC3"
        }
      ],
      "notes": [
        {
          "key": "ZXAL3M4A",
          "version": 569,
          "itemType": "note",
          "parentItem": "H9NZQ58A",
          "note": "Comment: In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016",
          "tags": [],
          "relations": {},
          "dateAdded": "2023-03-15T08:16:53Z",
          "dateModified": "2023-03-15T08:16:53Z",
          "uri": "http://zotero.org/groups/4922950/items/ZXAL3M4A"
        },
        {
          "key": "TNZE3FU5",
          "version": 644,
          "itemType": "note",
          "parentItem": "H9NZQ58A",
          "note": "<div data-citation-items=\"%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FH9NZQ58A%22%5D%2C%22itemData%22%3A%7B%22id%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FH9NZQ58A%22%2C%22type%22%3A%22article%22%2C%22abstract%22%3A%22State-of-the-art%20deep%20neural%20networks%20have%20achieved%20impressive%20results%20on%20many%20image%20classification%20tasks.%20However%2C%20these%20same%20architectures%20have%20been%20shown%20to%20be%20unstable%20to%20small%2C%20well%20sought%2C%20perturbations%20of%20the%20images.%20Despite%20the%20importance%20of%20this%20phenomenon%2C%20no%20effective%20methods%20have%20been%20proposed%20to%20accurately%20compute%20the%20robustness%20of%20state-of-the-art%20deep%20classifiers%20to%20such%20perturbations%20on%20large-scale%20datasets.%20In%20this%20paper%2C%20we%20fill%20this%20gap%20and%20propose%20the%20DeepFool%20algorithm%20to%20efficiently%20compute%20perturbations%20that%20fool%20deep%20networks%2C%20and%20thus%20reliably%20quantify%20the%20robustness%20of%20these%20classifiers.%20Extensive%20experimental%20results%20show%20that%20our%20approach%20outperforms%20recent%20methods%20in%20the%20task%20of%20computing%20adversarial%20perturbations%20and%20making%20classifiers%20more%20robust.%22%2C%22DOI%22%3A%2210.48550%2FarXiv.1511.04599%22%2C%22note%22%3A%22arXiv%3A1511.04599%20%5Bcs%5D%22%2C%22number%22%3A%22arXiv%3A1511.04599%22%2C%22publisher%22%3A%22arXiv%22%2C%22source%22%3A%22arXiv.org%22%2C%22title%22%3A%22DeepFool%3A%20a%20simple%20and%20accurate%20method%20to%20fool%20deep%20neural%20networks%22%2C%22title-short%22%3A%22DeepFool%22%2C%22URL%22%3A%22http%3A%2F%2Farxiv.org%2Fabs%2F1511.04599%22%2C%22author%22%3A%5B%7B%22family%22%3A%22Moosavi-Dezfooli%22%2C%22given%22%3A%22Seyed-Mohsen%22%7D%2C%7B%22family%22%3A%22Fawzi%22%2C%22given%22%3A%22Alhussein%22%7D%2C%7B%22family%22%3A%22Frossard%22%2C%22given%22%3A%22Pascal%22%7D%5D%2C%22accessed%22%3A%7B%22date-parts%22%3A%5B%5B%222023%22%2C3%2C15%5D%5D%7D%2C%22issued%22%3A%7B%22date-parts%22%3A%5B%5B%222016%22%2C7%2C4%5D%5D%7D%7D%7D%5D\" data-schema-version=\"9\"><p>motivation在于</p>\n<ol>\n<li>\n对抗样本安全性…\n</li>\n<li>\n目前算法的缺点\n</li>\n</ol>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FT2L744X2%22%2C%22annotationKey%22%3A%22547AAWEJ%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%221%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B239.192%2C493.102%2C286.365%2C501.69%5D%2C%5B50.112%2C481.146%2C286.365%2C489.734%5D%2C%5B50.112%2C469.191%2C286.365%2C477.779%5D%2C%5B50.112%2C457.236%2C176.866%2C465.824%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FH9NZQ58A%22%5D%2C%22locator%22%3A%221%22%7D%7D\">“no effective methods have been proposed to accurately compute the robustness of state-of-the-art deep classifiers to such perturbations on large-scale datasets”</span></p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FT2L744X2%22%2C%22annotationKey%22%3A%22VJLA723G%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%221%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B257.962%2C457.236%2C286.365%2C465.824%5D%2C%5B50.112%2C445.281%2C286.365%2C453.869%5D%2C%5B50.112%2C433.326%2C224.786%2C441.914%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FH9NZQ58A%22%5D%2C%22locator%22%3A%221%22%7D%7D\">“fill this gap and propose the DeepFool algorithm to efficiently compute perturbations that fool deep networks”</span></p>\n<p></p>\n<h1>intro</h1>\n<p>提到了这篇文章主要的贡献</p>\n<p><img alt=\"\" data-attachment-key=\"KLUY9RTX\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FT2L744X2%22%2C%22annotationKey%22%3A%22VY8EJQA7%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B41.538%2C182.192%2C293.654%2C405.462%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FH9NZQ58A%22%5D%2C%22locator%22%3A%222%22%7D%7D\" width=\"420\" height=\"372\"></p>\n<hr>\n<p>然后描述了其他方法的问题</p>\n<p>对于intriguing那篇文章的问题在于</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FT2L744X2%22%2C%22annotationKey%22%3A%22BAUQY5MK%22%2C%22color%22%3A%22%23a28ae5%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B202.032%2C90.803%2C286.365%2C99.71%5D%2C%5B50.112%2C78.848%2C195.038%2C87.755%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FH9NZQ58A%22%5D%2C%22locator%22%3A%222%22%7D%7D\">“optimization method employed in [18] is time-consuming”</span> </p>\n<p>然后对于FGSM的方法，缺点在于</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FT2L744X2%22%2C%22annotationKey%22%3A%22JDHCSLUL%22%2C%22color%22%3A%22%23a28ae5%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B323.009%2C564.423%2C545.115%2C573.33%5D%2C%5B308.862%2C552.468%2C545.115%2C561.375%5D%2C%5B308.862%2C540.513%2C349.011%2C549.42%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FH9NZQ58A%22%5D%2C%22locator%22%3A%222%22%7D%7D\">“introduced the “fast gradient sign” method, which computes the adversarial perturbations for a given classifier very efficiently”</span></p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FT2L744X2%22%2C%22annotationKey%22%3A%22WZK5A425%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B441.733%2C540.513%2C545.115%2C549.42%5D%2C%5B308.862%2C528.558%2C545.115%2C537.465%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FH9NZQ58A%22%5D%2C%22locator%22%3A%222%22%7D%7D\">“this method provides only a coarse approximation of the optimal perturbation vectors.”</span></p>\n<p></p>\n<hr>\n<p>第一节最后介绍了文章接下来的布局：</p>\n<p><img alt=\"\" data-attachment-key=\"VG3QL35K\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FT2L744X2%22%2C%22annotationKey%22%3A%22N363NKHE%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B296.538%2C242.769%2C561.346%2C333.346%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FH9NZQ58A%22%5D%2C%22locator%22%3A%222%22%7D%7D\" width=\"441\" height=\"151\"> 文章结构，第二节先讲述二分类问题，然后第三节扩展到多分类上。最后第四节通过实验来验证准确性</p>\n<p></p>\n<h1>2. deepfool for binary</h1>\n<hr>\n<p><img alt=\"\" data-attachment-key=\"R8PQIJ7B\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FT2L744X2%22%2C%22annotationKey%22%3A%226RBR3EPV%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B50.192%2C429.692%2C293.077%2C494.885%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FH9NZQ58A%22%5D%2C%22locator%22%3A%223%22%7D%7D\" width=\"405\" height=\"109\"> 这里公式的理解非常重要，对于一个点到超平面的距离为：<span class=\"math\">$\\frac{|wx+b|}{||w||}$</span> 这里的f不是sign函数而就是指的wx+b，注意对于超平面wx+b来说，向量w为这个超平面的法向量。</p>\n<p>这里之所以有<span class=\"math\">$\\frac{w}{||w||^2}$</span> 这种形式原因在于这里求的是r向量，带有方向，所以多乘了一个法向量除以范数最终得到这个形式</p>\n<p></p>\n<h1>3. deepfool for multiclass</h1>\n<p></p>\n<p></p>\n<p></p>\n<h1>4.Exp</h1>\n<p>提到了deepfool如何作为一个工具来分析一个模型的鲁棒性</p>\n<p><img alt=\"\" data-attachment-key=\"YKJ4ELSB\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FT2L744X2%22%2C%22annotationKey%22%3A%22U94AGIDY%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%225%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B289.038%2C530.077%2C561.346%2C647.769%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FH9NZQ58A%22%5D%2C%22locator%22%3A%225%22%7D%7D\" width=\"454\" height=\"196\"> 评估鲁棒性的方法，计算这个模型通过deepfool得出的平均的最小扰动又多小。<br>这个值越大也就代表越不容易被扰动。</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FT2L744X2%22%2C%22annotationKey%22%3A%22D9KSJJ7U%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%225%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B394.969%2C138.584%2C545.115%2C147.491%5D%2C%5B308.862%2C126.629%2C524.094%2C135.536%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FH9NZQ58A%22%5D%2C%22locator%22%3A%225%22%7D%7D\">“DeepFool can be used as a valuable tool to accurately assess the robustness of classifiers”</span></p>\n<p></p>\n<hr>\n<p>然后使用对抗样本fine tuning做了一些实验</p>\n<p>在这个过程中着重强调了不准确的对抗样本会让模型的准确性下降（比如FGSM）</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FT2L744X2%22%2C%22annotationKey%22%3A%22299UA5KI%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%227%22%2C%22position%22%3A%7B%22pageIndex%22%3A6%2C%22rects%22%3A%5B%5B157.449%2C114.713%2C286.365%2C123.62%5D%2C%5B50.112%2C102.758%2C286.363%2C111.665%5D%2C%5B50.112%2C90.803%2C130.022%2C99.71%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FH9NZQ58A%22%5D%2C%22locator%22%3A%227%22%7D%7D\">“Conversely, fine-tuning with the approach in [4] has led to a decrease of the test accuracy in all our experiments.”</span></p>\n<p></p>\n<h1>my comment</h1>\n<p>最终结果不仅更准确，而且生产还高效</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FT2L744X2%22%2C%22annotationKey%22%3A%22FC7HBLLP%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%226%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B50.112%2C434.909%2C286.365%2C443.816%5D%2C%5B50.112%2C422.954%2C286.365%2C431.861%5D%2C%5B50.112%2C410.999%2C164.532%2C419.906%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FH9NZQ58A%22%5D%2C%22locator%22%3A%226%22%7D%7D\">“the proposed approach reaches a more accurate perturbation vector compared to state-of-the-art methods, while being computationally efficient”</span></p>\n<p></p>\n<p>可以说达到了sota效果</p>\n<p>但我还是没有非常明确搞懂其生产对抗样本的方式和FGSM有什么本质不同</p>\n</div>",
          "tags": [],
          "relations": {},
          "dateAdded": "2023-03-15T10:42:16Z",
          "dateModified": "2023-03-20T11:56:06Z",
          "uri": "http://zotero.org/groups/4922950/items/TNZE3FU5"
        }
      ],
      "citationKey": "moosavi-dezfooliDeepFoolSimpleAccurate2016a",
      "itemKey": "H9NZQ58A",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/H9NZQ58A"
    },
    {
      "key": "JKC5CCM8",
      "version": 52,
      "itemType": "journalArticle",
      "title": "Defending against Adversarial Patches with Robust Self-Attention",
      "abstractNote": "We introduce a new defense against adversarial patch attacks based on our proposed Robust SelfAttention (RSA) layer. Robust Self-Attention replaces the outlier-sensitive weighted mean operation used by standard Self-Attention with a robust aggregation mechanism that detects and masks outlier tokens. Vision Transformer (ViT) models using our RSA layer achieve promising robust classiﬁcation accuracy, outperforming patch adversarial training as well as a prior provable defense, all with zero additional parameters or training. Additionally, we provide further evidence for the strength of simple patch adversarial training as a baseline defense.",
      "language": "en",
      "libraryCatalog": "Zotero",
      "creators": [
        {
          "firstName": "Norman",
          "lastName": "Mu",
          "creatorType": "author"
        },
        {
          "firstName": "David",
          "lastName": "Wagner",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": [],
      "dateAdded": "2023-01-24T11:29:39Z",
      "dateModified": "2023-01-24T11:29:39Z",
      "uri": "http://zotero.org/groups/4922950/items/JKC5CCM8",
      "itemID": 1780,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "Mu 和 Wagner - Defending against Adversarial Patches with Robust .pdf",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-01-24T11:29:38Z",
          "dateModified": "2023-01-24T11:29:40Z",
          "uri": "http://zotero.org/groups/4922950/items/TSVPMZU8",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\TSVPMZU8\\Mu 和 Wagner - Defending against Adversarial Patches with Robust .pdf",
          "select": "zotero://select/groups/4922950/items/TSVPMZU8"
        }
      ],
      "notes": [],
      "citationKey": "muDefendingAdversarialPatches",
      "itemKey": "JKC5CCM8",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/JKC5CCM8"
    },
    {
      "key": "P6LRJAQ2",
      "version": 72,
      "itemType": "bookSection",
      "title": "SLIP: Self-supervision Meets Language-Image Pre-training",
      "date": "2022",
      "language": "en",
      "shortTitle": "SLIP",
      "libraryCatalog": "DOI.org (Crossref)",
      "url": "https://link.springer.com/10.1007/978-3-031-19809-0_30",
      "accessDate": "2023-01-24T11:36:41Z",
      "extra": "Series Title: Lecture Notes in Computer Science\nDOI: 10.1007/978-3-031-19809-0_30",
      "volume": "13686",
      "place": "Cham",
      "publisher": "Springer Nature Switzerland",
      "ISBN": "978-3-031-19808-3 978-3-031-19809-0",
      "pages": "529-544",
      "bookTitle": "Computer Vision – ECCV 2022",
      "creators": [
        {
          "firstName": "Shai",
          "lastName": "Avidan",
          "creatorType": "editor"
        },
        {
          "firstName": "Gabriel",
          "lastName": "Brostow",
          "creatorType": "editor"
        },
        {
          "firstName": "Moustapha",
          "lastName": "Cissé",
          "creatorType": "editor"
        },
        {
          "firstName": "Giovanni Maria",
          "lastName": "Farinella",
          "creatorType": "editor"
        },
        {
          "firstName": "Tal",
          "lastName": "Hassner",
          "creatorType": "editor"
        },
        {
          "firstName": "Norman",
          "lastName": "Mu",
          "creatorType": "author"
        },
        {
          "firstName": "Alexander",
          "lastName": "Kirillov",
          "creatorType": "author"
        },
        {
          "firstName": "David",
          "lastName": "Wagner",
          "creatorType": "author"
        },
        {
          "firstName": "Saining",
          "lastName": "Xie",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": [],
      "dateAdded": "2023-01-24T11:36:41Z",
      "dateModified": "2023-01-24T11:36:41Z",
      "uri": "http://zotero.org/groups/4922950/items/P6LRJAQ2",
      "itemID": 1793,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "Mu 等 - 2022 - SLIP Self-supervision Meets Language-Image Pre-tr.pdf",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-01-24T11:36:37Z",
          "dateModified": "2023-01-24T11:36:41Z",
          "uri": "http://zotero.org/groups/4922950/items/Y3QPVK83",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\Y3QPVK83\\Mu 等 - 2022 - SLIP Self-supervision Meets Language-Image Pre-tr.pdf",
          "select": "zotero://select/groups/4922950/items/Y3QPVK83"
        }
      ],
      "notes": [],
      "citationKey": "muSLIPSelfsupervisionMeets2022",
      "itemKey": "P6LRJAQ2",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/P6LRJAQ2"
    },
    {
      "key": "ZL8KBQ3Z",
      "version": 186,
      "itemType": "note",
      "note": "<div data-citation-items=\"%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FQYM46RB3%22%5D%2C%22itemData%22%3A%7B%22id%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FQYM46RB3%22%2C%22type%22%3A%22article%22%2C%22abstract%22%3A%22Many%20machine%20learning%20models%20are%20vulnerable%20to%20adversarial%20examples%3A%20inputs%20that%20are%20specially%20crafted%20to%20cause%20a%20machine%20learning%20model%20to%20produce%20an%20incorrect%20output.%20Adversarial%20examples%20that%20affect%20one%20model%20often%20affect%20another%20model%2C%20even%20if%20the%20two%20models%20have%20different%20architectures%20or%20were%20trained%20on%20different%20training%20sets%2C%20so%20long%20as%20both%20models%20were%20trained%20to%20perform%20the%20same%20task.%20An%20attacker%20may%20therefore%20train%20their%20own%20substitute%20model%2C%20craft%20adversarial%20examples%20against%20the%20substitute%2C%20and%20transfer%20them%20to%20a%20victim%20model%2C%20with%20very%20little%20information%20about%20the%20victim.%20Recent%20work%20has%20further%20developed%20a%20technique%20that%20uses%20the%20victim%20model%20as%20an%20oracle%20to%20label%20a%20synthetic%20training%20set%20for%20the%20substitute%2C%20so%20the%20attacker%20need%20not%20even%20collect%20a%20training%20set%20to%20mount%20the%20attack.%20We%20extend%20these%20recent%20techniques%20using%20reservoir%20sampling%20to%20greatly%20enhance%20the%20efficiency%20of%20the%20training%20procedure%20for%20the%20substitute%20model.%20We%20introduce%20new%20transferability%20attacks%20between%20previously%20unexplored%20(substitute%2C%20victim)%20pairs%20of%20machine%20learning%20model%20classes%2C%20most%20notably%20SVMs%20and%20decision%20trees.%20We%20demonstrate%20our%20attacks%20on%20two%20commercial%20machine%20learning%20classification%20systems%20from%20Amazon%20(96.19%25%20misclassification%20rate)%20and%20Google%20(88.94%25)%20using%20only%20800%20queries%20of%20the%20victim%20model%2C%20thereby%20showing%20that%20existing%20machine%20learning%20approaches%20are%20in%20general%20vulnerable%20to%20systematic%20black-box%20attacks%20regardless%20of%20their%20structure.%22%2C%22note%22%3A%22arXiv%3A1605.07277%20%5Bcs%5D%22%2C%22number%22%3A%22arXiv%3A1605.07277%22%2C%22publisher%22%3A%22arXiv%22%2C%22source%22%3A%22arXiv.org%22%2C%22title%22%3A%22Transferability%20in%20Machine%20Learning%3A%20from%20Phenomena%20to%20Black-Box%20Attacks%20using%20Adversarial%20Samples%22%2C%22title-short%22%3A%22Transferability%20in%20Machine%20Learning%22%2C%22URL%22%3A%22http%3A%2F%2Farxiv.org%2Fabs%2F1605.07277%22%2C%22author%22%3A%5B%7B%22family%22%3A%22Papernot%22%2C%22given%22%3A%22Nicolas%22%7D%2C%7B%22family%22%3A%22McDaniel%22%2C%22given%22%3A%22Patrick%22%7D%2C%7B%22family%22%3A%22Goodfellow%22%2C%22given%22%3A%22Ian%22%7D%5D%2C%22accessed%22%3A%7B%22date-parts%22%3A%5B%5B%222023%22%2C2%2C1%5D%5D%7D%2C%22issued%22%3A%7B%22date-parts%22%3A%5B%5B%222016%22%2C5%2C23%5D%5D%7D%2C%22citation-key%22%3A%22papernotTransferabilityMachineLearning2016b%22%7D%7D%5D\" data-schema-version=\"8\"><p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FGU9DWZGF%22%2C%22annotationKey%22%3A%22XG9MQMT8%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%221%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B277.821%2C542.191%2C292.905%2C550.153%5D%2C%5B53.798%2C531.73%2C292.906%2C539.692%5D%2C%5B53.798%2C521.27%2C292.904%2C529.232%5D%2C%5B53.798%2C510.809%2C203.951%2C518.771%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FQYM46RB3%22%5D%2C%22locator%22%3A%221%22%7D%7D\">“Adversarial examples that affect one model often affect another model, even if the two models have different architectures or were trained on different training sets”</span></p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FGU9DWZGF%22%2C%22pageLabel%22%3A%221%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B53.798%2C427.123%2C292.91%2C435.085%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FQYM46RB3%22%5D%2C%22locator%22%3A%221%22%7D%7D\">“training set to mount the attack. We extend these recent”</span> <span class=\"citation\" data-citation=\"%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FQYM46RB3%22%5D%2C%22locator%22%3A%221%22%7D%5D%2C%22properties%22%3A%7B%7D%7D\">(<span class=\"citation-item\">Papernot 等, 2016, p. 1</span>)</span></p>\n</div>",
      "tags": [],
      "collections": [],
      "relations": {},
      "dateAdded": "2023-01-31T16:28:01Z",
      "dateModified": "2023-01-31T16:29:07Z",
      "uri": "http://zotero.org/groups/4922950/items/ZL8KBQ3Z",
      "itemID": 1867,
      "itemKey": "ZL8KBQ3Z",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/ZL8KBQ3Z"
    },
    {
      "key": "7GYBMIR3",
      "version": 1603,
      "itemType": "preprint",
      "title": "Protecting Intellectual Property of Generative Adversarial Networks from Ambiguity Attack",
      "abstractNote": "Ever since Machine Learning as a Service (MLaaS) emerges as a viable business that utilizes deep learning models to generate lucrative revenue, Intellectual Property Right (IPR) has become a major concern because these deep learning models can easily be replicated, shared, and re-distributed by any unauthorized third parties. To the best of our knowledge, one of the prominent deep learning models - Generative Adversarial Networks (GANs) which has been widely used to create photorealistic image are totally unprotected despite the existence of pioneering IPR protection methodology for Convolutional Neural Networks (CNNs). This paper therefore presents a complete protection framework in both black-box and white-box settings to enforce IPR protection on GANs. Empirically, we show that the proposed method does not compromise the original GANs performance (i.e. image generation, image super-resolution, style transfer), and at the same time, it is able to withstand both removal and ambiguity attacks against embedded watermarks.",
      "date": "2021-02-28",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/2102.04362",
      "accessDate": "2023-04-16T14:39:32Z",
      "extra": "arXiv:2102.04362 [cs]",
      "DOI": "10.48550/arXiv.2102.04362",
      "repository": "arXiv",
      "archiveID": "arXiv:2102.04362",
      "creators": [
        {
          "firstName": "Ding Sheng",
          "lastName": "Ong",
          "creatorType": "author"
        },
        {
          "firstName": "Chee Seng",
          "lastName": "Chan",
          "creatorType": "author"
        },
        {
          "firstName": "Kam Woh",
          "lastName": "Ng",
          "creatorType": "author"
        },
        {
          "firstName": "Lixin",
          "lastName": "Fan",
          "creatorType": "author"
        },
        {
          "firstName": "Qiang",
          "lastName": "Yang",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Cryptography and Security",
          "type": 1
        },
        {
          "tag": "Computer Science - Artificial Intelligence",
          "type": 1
        },
        {
          "tag": "Computer Science - Computer Vision and Pattern Recognition",
          "type": 1
        }
      ],
      "relations": [],
      "dateAdded": "2023-04-16T14:39:32Z",
      "dateModified": "2023-04-16T14:39:32Z",
      "uri": "http://zotero.org/groups/4922950/items/7GYBMIR3",
      "itemID": 3424,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "arXiv Fulltext PDF",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-04-16T14:39:45Z",
          "dateModified": "2023-04-16T14:39:45Z",
          "uri": "http://zotero.org/groups/4922950/items/PFIIZN58",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\PFIIZN58\\Ong 等 - 2021 - Protecting Intellectual Property of Generative Adv.pdf",
          "select": "zotero://select/groups/4922950/items/PFIIZN58"
        },
        {
          "itemType": "attachment",
          "title": "arXiv.org Snapshot",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-04-16T14:39:53Z",
          "dateModified": "2023-04-16T14:39:53Z",
          "uri": "http://zotero.org/groups/4922950/items/VBGTXEL8",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\VBGTXEL8\\2102.html",
          "select": "zotero://select/groups/4922950/items/VBGTXEL8"
        }
      ],
      "notes": [
        {
          "key": "3RSCG5JM",
          "version": 1602,
          "itemType": "note",
          "parentItem": "7GYBMIR3",
          "note": "Comment: Accepted at CVPR2021",
          "tags": [],
          "relations": {},
          "dateAdded": "2023-04-16T14:39:32Z",
          "dateModified": "2023-04-16T14:39:32Z",
          "uri": "http://zotero.org/groups/4922950/items/3RSCG5JM"
        }
      ],
      "citationKey": "ongProtectingIntellectualProperty2021",
      "itemKey": "7GYBMIR3",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/7GYBMIR3"
    },
    {
      "key": "TQKGH2UJ",
      "version": 493,
      "itemType": "preprint",
      "title": "The Limitations of Deep Learning in Adversarial Settings",
      "abstractNote": "Deep learning takes advantage of large datasets and computationally efficient training algorithms to outperform other approaches at various machine learning tasks. However, imperfections in the training phase of deep neural networks make them vulnerable to adversarial samples: inputs crafted by adversaries with the intent of causing deep neural networks to misclassify. In this work, we formalize the space of adversaries against deep neural networks (DNNs) and introduce a novel class of algorithms to craft adversarial samples based on a precise understanding of the mapping between inputs and outputs of DNNs. In an application to computer vision, we show that our algorithms can reliably produce samples correctly classified by human subjects but misclassified in specific targets by a DNN with a 97% adversarial success rate while only modifying on average 4.02% of the input features per sample. We then evaluate the vulnerability of different sample classes to adversarial perturbations by defining a hardness measure. Finally, we describe preliminary work outlining defenses against adversarial samples by defining a predictive measure of distance between a benign input and a target classification.",
      "date": "2015-11-23",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/1511.07528",
      "accessDate": "2023-03-11T04:56:36Z",
      "extra": "arXiv:1511.07528 [cs, stat]",
      "repository": "arXiv",
      "archiveID": "arXiv:1511.07528",
      "creators": [
        {
          "firstName": "Nicolas",
          "lastName": "Papernot",
          "creatorType": "author"
        },
        {
          "firstName": "Patrick",
          "lastName": "McDaniel",
          "creatorType": "author"
        },
        {
          "firstName": "Somesh",
          "lastName": "Jha",
          "creatorType": "author"
        },
        {
          "firstName": "Matt",
          "lastName": "Fredrikson",
          "creatorType": "author"
        },
        {
          "firstName": "Z. Berkay",
          "lastName": "Celik",
          "creatorType": "author"
        },
        {
          "firstName": "Ananthram",
          "lastName": "Swami",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Machine Learning",
          "type": 1
        },
        {
          "tag": "Statistics - Machine Learning",
          "type": 1
        },
        {
          "tag": "Computer Science - Cryptography and Security",
          "type": 1
        },
        {
          "tag": "Computer Science - Neural and Evolutionary Computing",
          "type": 1
        }
      ],
      "relations": [],
      "dateAdded": "2023-03-11T04:56:37Z",
      "dateModified": "2023-03-11T04:56:37Z",
      "uri": "http://zotero.org/groups/4922950/items/TQKGH2UJ",
      "itemID": 2457,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "arXiv Fulltext PDF",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-03-11T05:04:05Z",
          "dateModified": "2023-03-11T05:04:05Z",
          "uri": "http://zotero.org/groups/4922950/items/MZCC28YH",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\MZCC28YH\\Papernot 等 - 2015 - The Limitations of Deep Learning in Adversarial Se.pdf",
          "select": "zotero://select/groups/4922950/items/MZCC28YH"
        }
      ],
      "notes": [
        {
          "key": "QPPU4LWI",
          "version": 493,
          "itemType": "note",
          "parentItem": "TQKGH2UJ",
          "note": "Comment: Accepted to the 1st IEEE European Symposium on Security & Privacy, IEEE 2016. Saarbrucken, Germany",
          "tags": [],
          "relations": {},
          "dateAdded": "2023-03-11T04:56:37Z",
          "dateModified": "2023-03-11T04:56:37Z",
          "uri": "http://zotero.org/groups/4922950/items/QPPU4LWI"
        }
      ],
      "citationKey": "papernotLimitationsDeepLearning2015b",
      "itemKey": "TQKGH2UJ",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/TQKGH2UJ"
    },
    {
      "key": "QYM46RB3",
      "version": 173,
      "itemType": "preprint",
      "title": "Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples",
      "abstractNote": "Many machine learning models are vulnerable to adversarial examples: inputs that are specially crafted to cause a machine learning model to produce an incorrect output. Adversarial examples that affect one model often affect another model, even if the two models have different architectures or were trained on different training sets, so long as both models were trained to perform the same task. An attacker may therefore train their own substitute model, craft adversarial examples against the substitute, and transfer them to a victim model, with very little information about the victim. Recent work has further developed a technique that uses the victim model as an oracle to label a synthetic training set for the substitute, so the attacker need not even collect a training set to mount the attack. We extend these recent techniques using reservoir sampling to greatly enhance the efficiency of the training procedure for the substitute model. We introduce new transferability attacks between previously unexplored (substitute, victim) pairs of machine learning model classes, most notably SVMs and decision trees. We demonstrate our attacks on two commercial machine learning classification systems from Amazon (96.19% misclassification rate) and Google (88.94%) using only 800 queries of the victim model, thereby showing that existing machine learning approaches are in general vulnerable to systematic black-box attacks regardless of their structure.",
      "date": "2016-05-23",
      "shortTitle": "Transferability in Machine Learning",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/1605.07277",
      "accessDate": "2023-01-31T16:24:33Z",
      "extra": "arXiv:1605.07277 [cs]",
      "repository": "arXiv",
      "archiveID": "arXiv:1605.07277",
      "creators": [
        {
          "firstName": "Nicolas",
          "lastName": "Papernot",
          "creatorType": "author"
        },
        {
          "firstName": "Patrick",
          "lastName": "McDaniel",
          "creatorType": "author"
        },
        {
          "firstName": "Ian",
          "lastName": "Goodfellow",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Machine Learning",
          "type": 1
        },
        {
          "tag": "Computer Science - Cryptography and Security",
          "type": 1
        }
      ],
      "relations": [],
      "dateAdded": "2023-01-31T16:24:33Z",
      "dateModified": "2023-01-31T16:24:33Z",
      "uri": "http://zotero.org/groups/4922950/items/QYM46RB3",
      "itemID": 1859,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "arXiv Fulltext PDF",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-01-31T16:25:02Z",
          "dateModified": "2023-01-31T16:25:02Z",
          "uri": "http://zotero.org/groups/4922950/items/GU9DWZGF",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\GU9DWZGF\\Papernot 等 - 2016 - Transferability in Machine Learning from Phenomen.pdf",
          "select": "zotero://select/groups/4922950/items/GU9DWZGF"
        },
        {
          "itemType": "attachment",
          "title": "arXiv.org Snapshot",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-01-31T16:25:09Z",
          "dateModified": "2023-01-31T16:25:09Z",
          "uri": "http://zotero.org/groups/4922950/items/W9BXP8MQ",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\W9BXP8MQ\\1605.html",
          "select": "zotero://select/groups/4922950/items/W9BXP8MQ"
        }
      ],
      "notes": [],
      "citationKey": "papernotTransferabilityMachineLearning2016b",
      "itemKey": "QYM46RB3",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/QYM46RB3"
    },
    {
      "key": "KFW2APUH",
      "version": 1521,
      "itemType": "preprint",
      "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
      "abstractNote": "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .",
      "date": "2022-04-13",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/2112.10752",
      "accessDate": "2023-04-15T07:49:01Z",
      "extra": "arXiv:2112.10752 [cs]",
      "DOI": "10.48550/arXiv.2112.10752",
      "repository": "arXiv",
      "archiveID": "arXiv:2112.10752",
      "creators": [
        {
          "firstName": "Robin",
          "lastName": "Rombach",
          "creatorType": "author"
        },
        {
          "firstName": "Andreas",
          "lastName": "Blattmann",
          "creatorType": "author"
        },
        {
          "firstName": "Dominik",
          "lastName": "Lorenz",
          "creatorType": "author"
        },
        {
          "firstName": "Patrick",
          "lastName": "Esser",
          "creatorType": "author"
        },
        {
          "firstName": "Björn",
          "lastName": "Ommer",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Computer Vision and Pattern Recognition",
          "type": 1
        }
      ],
      "relations": [],
      "dateAdded": "2023-04-15T07:49:01Z",
      "dateModified": "2023-04-15T07:49:01Z",
      "uri": "http://zotero.org/groups/4922950/items/KFW2APUH",
      "itemID": 3353,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "arXiv Fulltext PDF",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-04-15T07:59:56Z",
          "dateModified": "2023-04-15T07:59:56Z",
          "uri": "http://zotero.org/groups/4922950/items/CHRH354Z",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\CHRH354Z\\Rombach 等 - 2022 - High-Resolution Image Synthesis with Latent Diffus.pdf",
          "select": "zotero://select/groups/4922950/items/CHRH354Z"
        },
        {
          "itemType": "attachment",
          "title": "arXiv.org Snapshot",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-04-15T08:00:25Z",
          "dateModified": "2023-04-15T08:00:25Z",
          "uri": "http://zotero.org/groups/4922950/items/X6K3I32F",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\X6K3I32F\\2112.html",
          "select": "zotero://select/groups/4922950/items/X6K3I32F"
        }
      ],
      "notes": [
        {
          "key": "4B9CYDQE",
          "version": 1521,
          "itemType": "note",
          "parentItem": "KFW2APUH",
          "note": "Comment: CVPR 2022",
          "tags": [],
          "relations": {},
          "dateAdded": "2023-04-15T07:49:01Z",
          "dateModified": "2023-04-15T07:49:01Z",
          "uri": "http://zotero.org/groups/4922950/items/4B9CYDQE"
        }
      ],
      "citationKey": "rombachHighResolutionImageSynthesis2022",
      "itemKey": "KFW2APUH",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/KFW2APUH"
    },
    {
      "key": "X5I3JAB4",
      "version": 106,
      "itemType": "preprint",
      "title": "Adversarial Training for Free!",
      "abstractNote": "Adversarial training, in which a network is trained on adversarial examples, is one of the few defenses against adversarial attacks that withstands strong attacks. Unfortunately, the high cost of generating strong adversarial examples makes standard adversarial training impractical on large-scale problems like ImageNet. We present an algorithm that eliminates the overhead cost of generating adversarial examples by recycling the gradient information computed when updating model parameters. Our \"free\" adversarial training algorithm achieves comparable robustness to PGD adversarial training on the CIFAR-10 and CIFAR-100 datasets at negligible additional cost compared to natural training, and can be 7 to 30 times faster than other strong adversarial training methods. Using a single workstation with 4 P100 GPUs and 2 days of runtime, we can train a robust model for the large-scale ImageNet classification task that maintains 40% accuracy against PGD attacks. The code is available at https://github.com/ashafahi/free_adv_train.",
      "date": "2019-11-20",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/1904.12843",
      "accessDate": "2023-01-29T14:42:06Z",
      "extra": "arXiv:1904.12843 [cs, stat]",
      "DOI": "10.48550/arXiv.1904.12843",
      "repository": "arXiv",
      "archiveID": "arXiv:1904.12843",
      "creators": [
        {
          "firstName": "Ali",
          "lastName": "Shafahi",
          "creatorType": "author"
        },
        {
          "firstName": "Mahyar",
          "lastName": "Najibi",
          "creatorType": "author"
        },
        {
          "firstName": "Amin",
          "lastName": "Ghiasi",
          "creatorType": "author"
        },
        {
          "firstName": "Zheng",
          "lastName": "Xu",
          "creatorType": "author"
        },
        {
          "firstName": "John",
          "lastName": "Dickerson",
          "creatorType": "author"
        },
        {
          "firstName": "Christoph",
          "lastName": "Studer",
          "creatorType": "author"
        },
        {
          "firstName": "Larry S.",
          "lastName": "Davis",
          "creatorType": "author"
        },
        {
          "firstName": "Gavin",
          "lastName": "Taylor",
          "creatorType": "author"
        },
        {
          "firstName": "Tom",
          "lastName": "Goldstein",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Machine Learning",
          "type": 1
        },
        {
          "tag": "Statistics - Machine Learning",
          "type": 1
        },
        {
          "tag": "Computer Science - Cryptography and Security",
          "type": 1
        },
        {
          "tag": "Computer Science - Computer Vision and Pattern Recognition",
          "type": 1
        }
      ],
      "relations": [],
      "dateAdded": "2023-01-29T14:42:06Z",
      "dateModified": "2023-01-29T14:42:06Z",
      "uri": "http://zotero.org/groups/4922950/items/X5I3JAB4",
      "itemID": 1799,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "arXiv Fulltext PDF",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-01-29T14:42:57Z",
          "dateModified": "2023-01-29T14:42:57Z",
          "uri": "http://zotero.org/groups/4922950/items/W5CAKKW8",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\W5CAKKW8\\Shafahi 等 - 2019 - Adversarial Training for Free!.pdf",
          "select": "zotero://select/groups/4922950/items/W5CAKKW8"
        },
        {
          "itemType": "attachment",
          "title": "arXiv.org Snapshot",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-01-29T14:43:06Z",
          "dateModified": "2023-01-29T14:43:06Z",
          "uri": "http://zotero.org/groups/4922950/items/7FNG5EYJ",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\7FNG5EYJ\\1904.html",
          "select": "zotero://select/groups/4922950/items/7FNG5EYJ"
        }
      ],
      "notes": [
        {
          "key": "8FE2Y892",
          "version": 88,
          "itemType": "note",
          "parentItem": "X5I3JAB4",
          "note": "Comment: Accepted to NeurIPS 2019",
          "tags": [],
          "relations": {},
          "dateAdded": "2023-01-29T14:42:06Z",
          "dateModified": "2023-01-29T14:42:06Z",
          "uri": "http://zotero.org/groups/4922950/items/8FE2Y892"
        }
      ],
      "citationKey": "shafahiAdversarialTrainingFree2019",
      "itemKey": "X5I3JAB4",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/X5I3JAB4"
    },
    {
      "key": "Q8WKH3MF",
      "version": 106,
      "itemType": "preprint",
      "title": "Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks",
      "abstractNote": "Data poisoning is an attack on machine learning models wherein the attacker adds examples to the training set to manipulate the behavior of the model at test time. This paper explores poisoning attacks on neural nets. The proposed attacks use \"clean-labels\"; they don't require the attacker to have any control over the labeling of training data. They are also targeted; they control the behavior of the classifier on a $\\textit{specific}$ test instance without degrading overall classifier performance. For example, an attacker could add a seemingly innocuous image (that is properly labeled) to a training set for a face recognition engine, and control the identity of a chosen person at test time. Because the attacker does not need to control the labeling function, poisons could be entered into the training set simply by leaving them on the web and waiting for them to be scraped by a data collection bot. We present an optimization-based method for crafting poisons, and show that just one single poison image can control classifier behavior when transfer learning is used. For full end-to-end training, we present a \"watermarking\" strategy that makes poisoning reliable using multiple ($\\approx$50) poisoned training instances. We demonstrate our method by generating poisoned frog images from the CIFAR dataset and using them to manipulate image classifiers.",
      "date": "2018-11-10",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/1804.00792",
      "accessDate": "2023-01-29T14:42:42Z",
      "extra": "arXiv:1804.00792 [cs, stat]",
      "DOI": "10.48550/arXiv.1804.00792",
      "repository": "arXiv",
      "archiveID": "arXiv:1804.00792",
      "creators": [
        {
          "firstName": "Ali",
          "lastName": "Shafahi",
          "creatorType": "author"
        },
        {
          "firstName": "W. Ronny",
          "lastName": "Huang",
          "creatorType": "author"
        },
        {
          "firstName": "Mahyar",
          "lastName": "Najibi",
          "creatorType": "author"
        },
        {
          "firstName": "Octavian",
          "lastName": "Suciu",
          "creatorType": "author"
        },
        {
          "firstName": "Christoph",
          "lastName": "Studer",
          "creatorType": "author"
        },
        {
          "firstName": "Tudor",
          "lastName": "Dumitras",
          "creatorType": "author"
        },
        {
          "firstName": "Tom",
          "lastName": "Goldstein",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Machine Learning",
          "type": 1
        },
        {
          "tag": "Statistics - Machine Learning",
          "type": 1
        },
        {
          "tag": "Computer Science - Cryptography and Security",
          "type": 1
        },
        {
          "tag": "Computer Science - Computer Vision and Pattern Recognition",
          "type": 1
        }
      ],
      "relations": [],
      "dateAdded": "2023-01-29T14:42:42Z",
      "dateModified": "2023-01-29T14:42:42Z",
      "uri": "http://zotero.org/groups/4922950/items/Q8WKH3MF",
      "itemID": 1801,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "arXiv Fulltext PDF",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-01-29T14:44:55Z",
          "dateModified": "2023-01-29T14:44:55Z",
          "uri": "http://zotero.org/groups/4922950/items/IRYAYF5K",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\IRYAYF5K\\Shafahi 等 - 2018 - Poison Frogs! Targeted Clean-Label Poisoning Attac.pdf",
          "select": "zotero://select/groups/4922950/items/IRYAYF5K"
        },
        {
          "itemType": "attachment",
          "title": "arXiv.org Snapshot",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-01-29T14:45:02Z",
          "dateModified": "2023-01-29T14:45:02Z",
          "uri": "http://zotero.org/groups/4922950/items/HSNRIWRB",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\HSNRIWRB\\1804.html",
          "select": "zotero://select/groups/4922950/items/HSNRIWRB"
        }
      ],
      "notes": [
        {
          "key": "MBGSEG4I",
          "version": 89,
          "itemType": "note",
          "parentItem": "Q8WKH3MF",
          "note": "Comment: Presented at the NIPS 2018 conference. 11 pages, 4 figures, with a supplementary section of 7 pages, 7 figures. First two authors contributed equally",
          "tags": [],
          "relations": {},
          "dateAdded": "2023-01-29T14:42:42Z",
          "dateModified": "2023-01-29T14:42:42Z",
          "uri": "http://zotero.org/groups/4922950/items/MBGSEG4I"
        }
      ],
      "citationKey": "shafahiPoisonFrogsTargeted2018",
      "itemKey": "Q8WKH3MF",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/Q8WKH3MF"
    },
    {
      "key": "35NYZ5XA",
      "version": 61,
      "itemType": "preprint",
      "title": "Adversarial Examples for $k$-Nearest Neighbor Classifiers Based on Higher-Order Voronoi Diagrams",
      "abstractNote": "Adversarial examples are a widely studied phenomenon in machine learning models. While most of the attention has been focused on neural networks, other practical models also suffer from this issue. In this work, we propose an algorithm for evaluating the adversarial robustness of k-nearest neighbor classiﬁcation, i.e., ﬁnding a minimum-norm adversarial example. Diverging from previous proposals, we propose the ﬁrst geometric approach by performing a search that expands outwards from a given input point. On a high level, the search radius expands to the nearby higher-order Voronoi cells until we ﬁnd a cell that classiﬁes differently from the input point. To scale the algorithm to a large k, we introduce approximation steps that ﬁnd perturbation with smaller norm, compared to the baselines, in a variety of datasets. Furthermore, we analyze the structural properties of a dataset where our approach outperforms the competition.",
      "date": "2021-11-01",
      "language": "en",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/2011.09719",
      "accessDate": "2023-01-24T11:30:43Z",
      "extra": "arXiv:2011.09719 [cs, stat]",
      "repository": "arXiv",
      "archiveID": "arXiv:2011.09719",
      "creators": [
        {
          "firstName": "Chawin",
          "lastName": "Sitawarin",
          "creatorType": "author"
        },
        {
          "firstName": "Evgenios M.",
          "lastName": "Kornaropoulos",
          "creatorType": "author"
        },
        {
          "firstName": "Dawn",
          "lastName": "Song",
          "creatorType": "author"
        },
        {
          "firstName": "David",
          "lastName": "Wagner",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Machine Learning",
          "type": 1
        },
        {
          "tag": "Statistics - Machine Learning",
          "type": 1
        },
        {
          "tag": "Computer Science - Cryptography and Security",
          "type": 1
        },
        {
          "tag": "Computer Science - Artificial Intelligence",
          "type": 1
        }
      ],
      "relations": [],
      "dateAdded": "2023-01-24T11:30:43Z",
      "dateModified": "2023-01-24T11:30:43Z",
      "uri": "http://zotero.org/groups/4922950/items/35NYZ5XA",
      "itemID": 1784,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "Sitawarin 等 - 2021 - Adversarial Examples for $k$-Nearest Neighbor Clas.pdf",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-01-24T11:30:38Z",
          "dateModified": "2023-01-24T11:30:44Z",
          "uri": "http://zotero.org/groups/4922950/items/B99N6B9W",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\B99N6B9W\\Sitawarin 等 - 2021 - Adversarial Examples for $k$-Nearest Neighbor Clas.pdf",
          "select": "zotero://select/groups/4922950/items/B99N6B9W"
        }
      ],
      "notes": [
        {
          "key": "C5APXHBB",
          "version": 61,
          "itemType": "note",
          "parentItem": "35NYZ5XA",
          "note": "Comment: Appears at NeurIPS 2021. Code is available at https://github.com/wagner-group/geoadex",
          "tags": [],
          "relations": {},
          "dateAdded": "2023-01-24T11:30:43Z",
          "dateModified": "2023-01-24T11:30:43Z",
          "uri": "http://zotero.org/groups/4922950/items/C5APXHBB"
        }
      ],
      "citationKey": "sitawarinAdversarialExamplesNearest2021",
      "itemKey": "35NYZ5XA",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/35NYZ5XA"
    },
    {
      "key": "SQE2Z267",
      "version": 69,
      "itemType": "journalArticle",
      "title": "Demystifying the Adversarial Robustness of Random Transformation Defenses",
      "abstractNote": "Neural networks’ lack of robustness against attacks raises concerns in security-sensitive settings such as autonomous vehicles. While many countermeasures may look promising, only a few withstand rigorous evaluation. Defenses using random transformations (RT) have shown impressive results, particularly BaRT (Raff et al., 2019) on ImageNet. However, this type of defense has not been rigorously evaluated, leaving its robustness properties poorly understood. Their stochastic properties make evaluation more challenging and render many proposed attacks on deterministic models inapplicable. First, we show that the BPDA attack (Athalye et al., 2018a) used in BaRT’s evaluation is ineffective and likely overestimates its robustness. We then attempt to construct the strongest possible RT defense through the informed selection of transformations and Bayesian optimization for tuning their parameters. Furthermore, we create the strongest possible attack to evaluate our RT defense. Our new attack vastly outperforms the baseline, reducing the accuracy by 83% compared to the 19% reduction by the commonly used EoT attack (4.3× improvement). Our result indicates that the RT defense on Imagenette dataset (a ten-class subset of ImageNet) is not robust against adversarial examples. Extending the study further, we use our new attack to adversarially train RT defense (called AdvRT), resulting in a large robustness gain. Code is available at https://github.com/wagnergroup/demystify-random-transform.",
      "language": "en",
      "libraryCatalog": "Zotero",
      "creators": [
        {
          "firstName": "Chawin",
          "lastName": "Sitawarin",
          "creatorType": "author"
        },
        {
          "firstName": "Zachary",
          "lastName": "Golan-Strieb",
          "creatorType": "author"
        },
        {
          "firstName": "David",
          "lastName": "Wagner",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": [],
      "dateAdded": "2023-01-24T11:36:22Z",
      "dateModified": "2023-01-24T11:36:22Z",
      "uri": "http://zotero.org/groups/4922950/items/SQE2Z267",
      "itemID": 1791,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "Sitawarin 等 - Demystifying the Adversarial Robustness of Random .pdf",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-01-24T11:36:20Z",
          "dateModified": "2023-01-24T11:36:23Z",
          "uri": "http://zotero.org/groups/4922950/items/SEIQXCYX",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\SEIQXCYX\\Sitawarin 等 - Demystifying the Adversarial Robustness of Random .pdf",
          "select": "zotero://select/groups/4922950/items/SEIQXCYX"
        }
      ],
      "notes": [],
      "citationKey": "sitawarinDemystifyingAdversarialRobustness",
      "itemKey": "SQE2Z267",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/SQE2Z267"
    },
    {
      "key": "EBH9U8XA",
      "version": 29,
      "itemType": "conferencePaper",
      "title": "Minimum-Norm Adversarial Examples on KNN and KNN based Models",
      "abstractNote": "We study the robustness against adversarial examples of kNN classiﬁers and classiﬁers that combine kNN with neural networks. The main difﬁculty lies in the fact that ﬁnding an optimal attack on kNN is intractable for typical datasets. In this work, we propose a gradient-based attack on kNN and kNNbased defenses, inspired by the previous work by Sitawarin & Wagner [1]. We demonstrate that our attack outperforms their method on all of the models we tested with only a minimal increase in the computation time. The attack also beats the stateof-the-art attack [2] on kNN when k > 1 using less than 1% of its running time. We hope that this attack can be used as a new baseline for evaluating the robustness of kNN and its variants.",
      "date": "5/2020",
      "language": "en",
      "libraryCatalog": "DOI.org (Crossref)",
      "url": "https://ieeexplore.ieee.org/document/9283888/",
      "accessDate": "2023-01-24T11:03:16Z",
      "place": "San Francisco, CA, USA",
      "publisher": "IEEE",
      "ISBN": "978-1-72819-346-5",
      "pages": "34-40",
      "proceedingsTitle": "2020 IEEE Security and Privacy Workshops (SPW)",
      "conferenceName": "2020 IEEE Security and Privacy Workshops (SPW)",
      "DOI": "10.1109/SPW50608.2020.00023",
      "creators": [
        {
          "firstName": "Chawin",
          "lastName": "Sitawarin",
          "creatorType": "author"
        },
        {
          "firstName": "David",
          "lastName": "Wagner",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": [],
      "dateAdded": "2023-01-24T11:03:16Z",
      "dateModified": "2023-01-24T11:03:16Z",
      "uri": "http://zotero.org/groups/4922950/items/EBH9U8XA",
      "itemID": 1766,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "Sitawarin 和 Wagner - 2020 - Minimum-Norm Adversarial Examples on KNN and KNN b.pdf",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-01-24T11:03:12Z",
          "dateModified": "2023-01-24T11:03:17Z",
          "uri": "http://zotero.org/groups/4922950/items/67K6ZXX9",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\67K6ZXX9\\Sitawarin 和 Wagner - 2020 - Minimum-Norm Adversarial Examples on KNN and KNN b.pdf",
          "select": "zotero://select/groups/4922950/items/67K6ZXX9"
        }
      ],
      "notes": [],
      "citationKey": "sitawarinMinimumNormAdversarialExamples2020",
      "itemKey": "EBH9U8XA",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/EBH9U8XA"
    },
    {
      "key": "369EWXK6",
      "version": 1225,
      "itemType": "preprint",
      "title": "Part-Based Models Improve Adversarial Robustness",
      "abstractNote": "We show that combining human prior knowledge with end-to-end learning can improve the robustness of deep neural networks by introducing a part-based model for object classification. We believe that the richer form of annotation helps guide neural networks to learn more robust features without requiring more samples or larger models. Our model combines a part segmentation model with a tiny classifier and is trained end-to-end to simultaneously segment objects into parts and then classify the segmented object. Empirically, our part-based models achieve both higher accuracy and higher adversarial robustness than a ResNet-50 baseline on all three datasets. For instance, the clean accuracy of our part models is up to 15 percentage points higher than the baseline's, given the same level of robustness. Our experiments indicate that these models also reduce texture bias and yield better robustness against common corruptions and spurious correlations. The code is publicly available at https://github.com/chawins/adv-part-model.",
      "date": "2023-03-08",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/2209.09117",
      "accessDate": "2023-04-11T04:07:26Z",
      "extra": "arXiv:2209.09117 [cs]",
      "DOI": "10.48550/arXiv.2209.09117",
      "repository": "arXiv",
      "archiveID": "arXiv:2209.09117",
      "creators": [
        {
          "firstName": "Chawin",
          "lastName": "Sitawarin",
          "creatorType": "author"
        },
        {
          "firstName": "Kornrapat",
          "lastName": "Pongmala",
          "creatorType": "author"
        },
        {
          "firstName": "Yizheng",
          "lastName": "Chen",
          "creatorType": "author"
        },
        {
          "firstName": "Nicholas",
          "lastName": "Carlini",
          "creatorType": "author"
        },
        {
          "firstName": "David",
          "lastName": "Wagner",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Machine Learning",
          "type": 1
        },
        {
          "tag": "Computer Science - Cryptography and Security",
          "type": 1
        },
        {
          "tag": "Computer Science - Computer Vision and Pattern Recognition",
          "type": 1
        }
      ],
      "relations": [],
      "dateAdded": "2023-04-11T04:07:26Z",
      "dateModified": "2023-04-11T04:07:34Z",
      "uri": "http://zotero.org/groups/4922950/items/369EWXK6",
      "itemID": 3056,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "arXiv Fulltext PDF",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-04-11T11:30:54Z",
          "dateModified": "2023-04-11T11:30:54Z",
          "uri": "http://zotero.org/groups/4922950/items/QN5N5P8N",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\QN5N5P8N\\Sitawarin 等 - 2023 - Part-Based Models Improve Adversarial Robustness.pdf",
          "select": "zotero://select/groups/4922950/items/QN5N5P8N"
        }
      ],
      "notes": [
        {
          "key": "DQAPFHB6",
          "version": 1225,
          "itemType": "note",
          "parentItem": "369EWXK6",
          "note": "Comment: Published in ICLR 2023 (poster). Code can be found at https://github.com/chawins/adv-part-model",
          "tags": [],
          "relations": {},
          "dateAdded": "2023-04-11T04:07:26Z",
          "dateModified": "2023-04-11T04:07:26Z",
          "uri": "http://zotero.org/groups/4922950/items/DQAPFHB6"
        }
      ],
      "citationKey": "sitawarinPartBasedModelsImprove2023",
      "itemKey": "369EWXK6",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/369EWXK6"
    },
    {
      "key": "CQZL8NPY",
      "version": 63,
      "itemType": "conferencePaper",
      "title": "SAT: Improving Adversarial Training via Curriculum-Based Loss Smoothing",
      "abstractNote": "Adversarial training (AT) has become a popular choice for training robust networks. However, it tends to sacrifice clean accuracy heavily in favor of robustness and suffers from a large generalization error. To address these concerns, we propose Smooth Adversarial Training (SAT), guided by our analysis on the eigenspectrum of the loss Hessian. We find that curriculum learning, a scheme that emphasizes on starting “easy” and gradually ramping up on the “difficulty” of training, smooths the adversarial loss landscape for a suitably chosen difficulty metric. We present a general formulation for curriculum learning in the adversarial setting and propose two difficulty metrics based on the maximal Hessian eigenvalue (H-SAT) and the softmax probability (P-SAT). We demonstrate that SAT stabilizes network training even for a large perturbation norm and allows the network to operate at a better clean accuracy versus robustness trade-off curve compared to AT. This leads to a significant improvement in both clean accuracy and robustness compared to AT, TRADES, and other baselines. To highlight a few results, our best model improves normal and robust accuracy by 6% and 1% on CIFAR-100 compared to AT, respectively. On Imagenette, a ten-class subset of ImageNet, our model outperforms AT by 23% and 3% on normal and robust accuracy respectively.",
      "date": "2021-11-15",
      "language": "en",
      "shortTitle": "SAT",
      "libraryCatalog": "DOI.org (Crossref)",
      "url": "https://dl.acm.org/doi/10.1145/3474369.3486878",
      "accessDate": "2023-01-24T11:31:02Z",
      "place": "Virtual Event Republic of Korea",
      "publisher": "ACM",
      "ISBN": "978-1-4503-8657-9",
      "pages": "25-36",
      "proceedingsTitle": "Proceedings of the 14th ACM Workshop on Artificial Intelligence and Security",
      "conferenceName": "CCS '21: 2021 ACM SIGSAC Conference on Computer and Communications Security",
      "DOI": "10.1145/3474369.3486878",
      "creators": [
        {
          "firstName": "Chawin",
          "lastName": "Sitawarin",
          "creatorType": "author"
        },
        {
          "firstName": "Supriyo",
          "lastName": "Chakraborty",
          "creatorType": "author"
        },
        {
          "firstName": "David",
          "lastName": "Wagner",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": [],
      "dateAdded": "2023-01-24T11:31:02Z",
      "dateModified": "2023-01-24T11:31:03Z",
      "uri": "http://zotero.org/groups/4922950/items/CQZL8NPY",
      "itemID": 1787,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "Sitawarin 等 - 2021 - SAT Improving Adversarial Training via Curriculum.pdf",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-01-24T11:30:59Z",
          "dateModified": "2023-01-24T11:31:03Z",
          "uri": "http://zotero.org/groups/4922950/items/I39HJHV9",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\I39HJHV9\\Sitawarin 等 - 2021 - SAT Improving Adversarial Training via Curriculum.pdf",
          "select": "zotero://select/groups/4922950/items/I39HJHV9"
        }
      ],
      "notes": [],
      "citationKey": "sitawarinSATImprovingAdversarial2021",
      "itemKey": "CQZL8NPY",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/CQZL8NPY"
    },
    {
      "key": "5X3MZQZ5",
      "version": 49,
      "itemType": "journalArticle",
      "title": "MITIGATING ADVERSARIAL TRAINING INSTABILITY WITH BATCH NORMALIZATION",
      "abstractNote": "The adversarial training paradigm has become the standard in training deep neural networks for robustness. Yet, it remains unstable, with the mechanisms driving this instability poorly understood. In this study, we discover that this instability is primarily driven by a non-smooth optimization landscape and an internal covariate shift phenomenon, and show that Batch Normalization (BN) can effectively mitigate both these issues. Further, we demonstrate that BN universally improves clean and robust performance across various defenses, datasets, and model types, with greater improvement on more difﬁcult tasks. Finally, we conﬁrm BN’s heterogeneous distribution issue with mixed-batch training and propose a solution.",
      "date": "2021",
      "language": "en",
      "libraryCatalog": "Zotero",
      "creators": [
        {
          "firstName": "Arvind P",
          "lastName": "Sridhar",
          "creatorType": "author"
        },
        {
          "firstName": "Chawin",
          "lastName": "Sitawarin",
          "creatorType": "author"
        },
        {
          "firstName": "David",
          "lastName": "Wagner",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": [],
      "dateAdded": "2023-01-24T11:29:22Z",
      "dateModified": "2023-01-24T11:29:22Z",
      "uri": "http://zotero.org/groups/4922950/items/5X3MZQZ5",
      "itemID": 1778,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "Sridhar 等 - 2021 - MITIGATING ADVERSARIAL TRAINING INSTABILITY WITH B.pdf",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-01-24T11:29:21Z",
          "dateModified": "2023-01-24T11:29:23Z",
          "uri": "http://zotero.org/groups/4922950/items/EXFNVIRT",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\EXFNVIRT\\Sridhar 等 - 2021 - MITIGATING ADVERSARIAL TRAINING INSTABILITY WITH B.pdf",
          "select": "zotero://select/groups/4922950/items/EXFNVIRT"
        }
      ],
      "notes": [],
      "citationKey": "sridharMITIGATINGADVERSARIALTRAINING2021",
      "itemKey": "5X3MZQZ5",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/5X3MZQZ5"
    },
    {
      "key": "U9LPNVZF",
      "version": 615,
      "itemType": "preprint",
      "title": "Intriguing properties of neural networks",
      "abstractNote": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",
      "date": "2014-02-19",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/1312.6199",
      "accessDate": "2023-01-31T12:16:06Z",
      "extra": "arXiv:1312.6199 [cs]",
      "DOI": "10.48550/arXiv.1312.6199",
      "repository": "arXiv",
      "archiveID": "arXiv:1312.6199",
      "creators": [
        {
          "firstName": "Christian",
          "lastName": "Szegedy",
          "creatorType": "author"
        },
        {
          "firstName": "Wojciech",
          "lastName": "Zaremba",
          "creatorType": "author"
        },
        {
          "firstName": "Ilya",
          "lastName": "Sutskever",
          "creatorType": "author"
        },
        {
          "firstName": "Joan",
          "lastName": "Bruna",
          "creatorType": "author"
        },
        {
          "firstName": "Dumitru",
          "lastName": "Erhan",
          "creatorType": "author"
        },
        {
          "firstName": "Ian",
          "lastName": "Goodfellow",
          "creatorType": "author"
        },
        {
          "firstName": "Rob",
          "lastName": "Fergus",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Machine Learning",
          "type": 1
        },
        {
          "tag": "Computer Science - Computer Vision and Pattern Recognition",
          "type": 1
        },
        {
          "tag": "Computer Science - Neural and Evolutionary Computing",
          "type": 1
        }
      ],
      "relations": [],
      "dateAdded": "2023-01-31T12:16:06Z",
      "dateModified": "2023-01-31T12:16:06Z",
      "uri": "http://zotero.org/groups/4922950/items/U9LPNVZF",
      "itemID": 1847,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "arXiv Fulltext PDF",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-01-31T12:16:45Z",
          "dateModified": "2023-01-31T12:16:45Z",
          "uri": "http://zotero.org/groups/4922950/items/RR9WPHI8",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\RR9WPHI8\\Szegedy 等 - 2014 - Intriguing properties of neural networks.pdf",
          "select": "zotero://select/groups/4922950/items/RR9WPHI8"
        },
        {
          "itemType": "attachment",
          "title": "arXiv.org Snapshot",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-01-31T12:17:02Z",
          "dateModified": "2023-01-31T12:17:02Z",
          "uri": "http://zotero.org/groups/4922950/items/W59TWUKS",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\W59TWUKS\\1312.html",
          "select": "zotero://select/groups/4922950/items/W59TWUKS"
        }
      ],
      "notes": [
        {
          "key": "Q7GUQH6G",
          "version": 360,
          "itemType": "note",
          "parentItem": "U9LPNVZF",
          "note": "<div data-citation-items=\"%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FU9LPNVZF%22%5D%2C%22itemData%22%3A%7B%22id%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FU9LPNVZF%22%2C%22type%22%3A%22article%22%2C%22abstract%22%3A%22Deep%20neural%20networks%20are%20highly%20expressive%20models%20that%20have%20recently%20achieved%20state%20of%20the%20art%20performance%20on%20speech%20and%20visual%20recognition%20tasks.%20While%20their%20expressiveness%20is%20the%20reason%20they%20succeed%2C%20it%20also%20causes%20them%20to%20learn%20uninterpretable%20solutions%20that%20could%20have%20counter-intuitive%20properties.%20In%20this%20paper%20we%20report%20two%20such%20properties.%20First%2C%20we%20find%20that%20there%20is%20no%20distinction%20between%20individual%20high%20level%20units%20and%20random%20linear%20combinations%20of%20high%20level%20units%2C%20according%20to%20various%20methods%20of%20unit%20analysis.%20It%20suggests%20that%20it%20is%20the%20space%2C%20rather%20than%20the%20individual%20units%2C%20that%20contains%20of%20the%20semantic%20information%20in%20the%20high%20layers%20of%20neural%20networks.%20Second%2C%20we%20find%20that%20deep%20neural%20networks%20learn%20input-output%20mappings%20that%20are%20fairly%20discontinuous%20to%20a%20significant%20extend.%20We%20can%20cause%20the%20network%20to%20misclassify%20an%20image%20by%20applying%20a%20certain%20imperceptible%20perturbation%2C%20which%20is%20found%20by%20maximizing%20the%20network's%20prediction%20error.%20In%20addition%2C%20the%20specific%20nature%20of%20these%20perturbations%20is%20not%20a%20random%20artifact%20of%20learning%3A%20the%20same%20perturbation%20can%20cause%20a%20different%20network%2C%20that%20was%20trained%20on%20a%20different%20subset%20of%20the%20dataset%2C%20to%20misclassify%20the%20same%20input.%22%2C%22DOI%22%3A%2210.48550%2FarXiv.1312.6199%22%2C%22note%22%3A%22arXiv%3A1312.6199%20%5Bcs%5D%22%2C%22number%22%3A%22arXiv%3A1312.6199%22%2C%22publisher%22%3A%22arXiv%22%2C%22source%22%3A%22arXiv.org%22%2C%22title%22%3A%22Intriguing%20properties%20of%20neural%20networks%22%2C%22URL%22%3A%22http%3A%2F%2Farxiv.org%2Fabs%2F1312.6199%22%2C%22author%22%3A%5B%7B%22family%22%3A%22Szegedy%22%2C%22given%22%3A%22Christian%22%7D%2C%7B%22family%22%3A%22Zaremba%22%2C%22given%22%3A%22Wojciech%22%7D%2C%7B%22family%22%3A%22Sutskever%22%2C%22given%22%3A%22Ilya%22%7D%2C%7B%22family%22%3A%22Bruna%22%2C%22given%22%3A%22Joan%22%7D%2C%7B%22family%22%3A%22Erhan%22%2C%22given%22%3A%22Dumitru%22%7D%2C%7B%22family%22%3A%22Goodfellow%22%2C%22given%22%3A%22Ian%22%7D%2C%7B%22family%22%3A%22Fergus%22%2C%22given%22%3A%22Rob%22%7D%5D%2C%22accessed%22%3A%7B%22date-parts%22%3A%5B%5B%222023%22%2C1%2C31%5D%5D%7D%2C%22issued%22%3A%7B%22date-parts%22%3A%5B%5B%222014%22%2C2%2C19%5D%5D%7D%7D%7D%5D\" data-schema-version=\"8\"><h1>Abstract</h1>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FRR9WPHI8%22%2C%22annotationKey%22%3A%225KFF9NUF%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%221%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B348.875%2C446.698%2C468.138%2C455.605%5D%2C%5B143.865%2C435.739%2C393.358%2C444.646%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FU9LPNVZF%22%5D%2C%22locator%22%3A%221%22%7D%7D\">“causes them to learn uninterpretable solutions that could have counter-intuitive properties”</span> <span style=\"background-color: #2ea8e580\">这篇论文更多的是从可解释性来讨论的？</span></p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FRR9WPHI8%22%2C%22annotationKey%22%3A%22KVZMVV5K%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%221%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B230.669%2C365.604%2C468.138%2C374.511%5D%2C%5B143.865%2C354.645%2C311.715%2C363.552%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FU9LPNVZF%22%5D%2C%22locator%22%3A%221%22%7D%7D\">“deep neural networks learn input-output mappings that are fairly discontinuous to a significant extent”</span> 深度神经网络学习的输入-输出映射在很大程度上是不连续的<br><br>而不连续的性质或许也是导致adversarial attack的原因？</p>\n<p>作者的一系列实验表明，很容易能够通过扰动来得到对抗样本，而这种扰动的可迁移性（<span style=\"background-color: #2ea8e580\">black box的理论基础？</span>）有体现了神经网络的共同特性</p>\n<p></p>\n<h1>1.intro</h1>\n<p><span style=\"background-color: #5fb23680\">开头介绍了一下深度神经网络的优点，然后又提到了可解释新的问题，以及一些反直觉的特性</span></p>\n<p>本文就聚焦于两个反直觉的特性</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FRR9WPHI8%22%2C%22annotationKey%22%3A%22R3BQ4GG7%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%221%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B108%2C186.512%2C264.752%2C195.419%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FU9LPNVZF%22%5D%2C%22locator%22%3A%221%22%7D%7D\">“discuss two counter-intuitive properties”</span></p>\n<p></p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FRR9WPHI8%22%2C%22annotationKey%22%3A%22SCIBSVK2%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%221%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B364.045%2C114.781%2C503.999%2C123.847%5D%2C%5B108%2C103.822%2C339.431%2C112.729%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FU9LPNVZF%22%5D%2C%22locator%22%3A%221%22%7D%7D\">“question the conjecture that neural networks disentangle variation factors across coordinates”</span></p>\n<p>第一个的意思是，神经网络中unit的结构并没有特定的语义信息</p>\n<p><span style=\"background-color: #2ea8e580\">那么特征向量的部分值是否有语义信息？</span>，<span style=\"background-color: #2ea8e580\">特别注意其中disentangle的说法，是否和李宏毅语音处理的文章有关</span></p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FRR9WPHI8%22%2C%22annotationKey%22%3A%22GXWCMAD6%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%221%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B446.977%2C103.822%2C504.003%2C112.729%5D%2C%5B108%2C92.863%2C504.003%2C101.77%5D%2C%5B108%2C81.904%2C125.992%2C90.811%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FU9LPNVZF%22%5D%2C%22locator%22%3A%221%22%7D%7D\">“it is the entire space of activations, rather than the individual units, that contains the bulk of the semantic information.”</span></p>\n<p></p>\n<hr>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FRR9WPHI8%22%2C%22annotationKey%22%3A%22P5UVTYQH%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B126.74%2C670.128%2C504.003%2C679.035%5D%2C%5B108%2C659.169%2C207.208%2C668.076%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FU9LPNVZF%22%5D%2C%22locator%22%3A%222%22%7D%7D\">“second property is concerned with the stability of neural networks with respect to small perturbations to their inputs”</span></p>\n<p>第二个的意思是神经网络对于扰动的稳定性</p>\n<p>发现可以设计出非常小的扰动来让神经网络出错。跟关键的是，这种对抗样本还有很强的迁移能力和鲁棒性</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FRR9WPHI8%22%2C%22annotationKey%22%3A%22RGI4YMA8%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B108%2C565.52%2C345.419%2C574.427%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FU9LPNVZF%22%5D%2C%22locator%22%3A%222%22%7D%7D\">“Yet, we found that adversarial examples are relatively robust”</span></p>\n<p>就算神经网络的层数，激活函数乃至是训练样本都不一样（<span style=\"background-color: #5fb23680\">这里样本不一样只是不同还是样本的概率分布都不同</span>）等等设定</p>\n<p><img alt=\"\" data-attachment-key=\"G9XWBCLU\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FRR9WPHI8%22%2C%22annotationKey%22%3A%22EH88L9TN%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B102.5%2C518%2C507%2C577%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FU9LPNVZF%22%5D%2C%22locator%22%3A%222%22%7D%7D\" width=\"674\" height=\"98\"></p>\n<h1>2.framework</h1>\n<h1>3.Units</h1>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FRR9WPHI8%22%2C%22annotationKey%22%3A%22HHRLB5G8%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B108%2C180.117%2C504.003%2C189.024%5D%2C%5B108%2C169.159%2C140.09%2C178.066%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FU9LPNVZF%22%5D%2C%22locator%22%3A%222%22%7D%7D\">“Traditional computer vision systems rely on feature extraction: often a single feature is easily interpretable”</span></p>\n<p>相比之下，深度神经网络并没有这个性质</p>\n<p><img alt=\"\" data-attachment-key=\"9E48VC75\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FRR9WPHI8%22%2C%22annotationKey%22%3A%2295WF6KDW%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B252%2C58.5%2C361.5%2C83%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FU9LPNVZF%22%5D%2C%22locator%22%3A%222%22%7D%7D\" width=\"183\" height=\"41\"></p>\n<p>实验表明随机的参数和训练的参数都对<span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FRR9WPHI8%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B118.816%2C308.165%2C138.194%2C317.231%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FU9LPNVZF%22%5D%2C%22locator%22%3A%223%22%7D%7D\">“φ(x)”</span>没什么不同的反应</p>\n<p><span style=\"background-color: #ff666680\">还是不太明白 </span><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FRR9WPHI8%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B118.816%2C308.165%2C138.194%2C317.231%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FU9LPNVZF%22%5D%2C%22locator%22%3A%223%22%7D%7D\"><span style=\"background-color: #ff666680\">“φ(x)”</span></span><span style=\"background-color: #ff666680\">到底指的是什么，以及basic vector</span></p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FRR9WPHI8%22%2C%22annotationKey%22%3A%22IBFFU93E%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B201.228%2C308.165%2C504.002%2C317.231%5D%2C%5B108%2C297.206%2C153.928%2C306.113%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FU9LPNVZF%22%5D%2C%22locator%22%3A%223%22%7D%7D\">“question the notion that neural networks disentangle variation factors across coordinates”</span></p>\n<p></p>\n<h1>4.</h1>\n<p>关于对抗样本的话题</p>\n<p><img alt=\"\" data-attachment-key=\"WSMI3HLK\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FRR9WPHI8%22%2C%22annotationKey%22%3A%22AZFVUAU9%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B102.692%2C168.34599999999995%2C515.192%2C237.1153916579026%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FU9LPNVZF%22%5D%2C%22locator%22%3A%224%22%7D%7D\" width=\"688\" height=\"115\"></p>\n<p>首先对于小的扰动，一般来说不太会造成错误的分类（由于连续性的性质）</p>\n<p>但对于深度神经网络来说，可能并不连续</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FRR9WPHI8%22%2C%22annotationKey%22%3A%22AX3K9UZI%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B203.193%2C153.635%2C504.003%2C162.542%5D%2C%5B108%2C142.676%2C225.021%2C151.583%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FU9LPNVZF%22%5D%2C%22locator%22%3A%224%22%7D%7D\">“for deep neural networks, the smoothness assumption that underlies many kernel methods does not hold”</span></p>\n<p></p>\n<p>作者是通过优化（梯度？）的方式来找对抗性的样本。现在有些工作使用deformation的方法（<span style=\"background-color: #2ea8e580\">像是加一层GAN什么的？</span>）来提高鲁棒性</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FRR9WPHI8%22%2C%22annotationKey%22%3A%22SCGH98X9%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%225%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B108%2C698.024%2C366.589%2C706.931%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FU9LPNVZF%22%5D%2C%22locator%22%3A%225%22%7D%7D\">“increasing the robustness and convergence speed of the models”</span> 提高鲁棒性可以理解，为啥可以提高收敛速度</p>\n<p>不过这种方法从统计学上来说，非常的低效。</p>\n<p><span style=\"background-color: #2ea8e580\">作者想说的应该是，通过找对抗样本，也相当于是找统计中非常少出现的样本，从数据的角度来讲是一种更高效的提高鲁棒性的方式？</span></p>\n<p><span style=\"background-color: #ff666680\">Formal description 没有太看懂</span></p>\n<p></p>\n<p>Experiment results，就像之前说的一样，非常小的扰动就可以实现攻击，而且这种恶意样本在不同深度模型上，不同数据集上都可以迁移。</p>\n<p><span style=\"background-color: #5fb23680\">根据这些结果，作者总结</span></p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FRR9WPHI8%22%2C%22annotationKey%22%3A%22BGZXNMLJ%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%225%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B256.891%2C125.74%2C504.003%2C134.647%5D%2C%5B108%2C114.781%2C458.126%2C123.688%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FU9LPNVZF%22%5D%2C%22locator%22%3A%225%22%7D%7D\">“adversarial examples are somewhat universal and not just the results of overfitting to a particular model or to the specific selection of the training set”</span></p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FRR9WPHI8%22%2C%22annotationKey%22%3A%22GL7R6VU9%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%225%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B158.71%2C103.822%2C504.003%2C112.729%5D%2C%5B108%2C92.863%2C166.68%2C101.77%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FU9LPNVZF%22%5D%2C%22locator%22%3A%225%22%7D%7D\">“back-feeding adversarial examples to training might improve generalization of the resulting models”</span></p>\n<p></p>\n<hr>\n<p>注意实验细节，作者是争对每一层来构造对抗样本来使之学习的</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FRR9WPHI8%22%2C%22annotationKey%22%3A%227CH5F7AT%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%226%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B389.54%2C312.645%2C504.001%2C321.711%5D%2C%5B108%2C301.686%2C504.003%2C310.593%5D%2C%5B108%2C290.727%2C260.627%2C299.634%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FU9LPNVZF%22%5D%2C%22locator%22%3A%226%22%7D%7D\">“A subtle, but essential detail is that we only got improvements by generating adversarial examples for each layer outputs which were used to train all the layers above.”</span></p>\n<p>然后作者发现</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FRR9WPHI8%22%2C%22annotationKey%22%3A%22YFP84REM%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%226%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B307.292%2C268.809%2C504.003%2C277.716%5D%2C%5B108%2C257.851%2C387.132%2C266.758%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FU9LPNVZF%22%5D%2C%22locator%22%3A%226%22%7D%7D\">“adversarial examples for the higher layers seemed to be significantly more useful than those on the input or lower layers”</span></p>\n<p></p>\n<p><span style=\"background-color: #2ea8e580\">这和之前讨论的深度神经网络是否有什么联系</span></p>\n<p></p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FRR9WPHI8%22%2C%22annotationKey%22%3A%227T5HMKST%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%227%22%2C%22position%22%3A%7B%22pageIndex%22%3A6%2C%22rects%22%3A%5B%5B316.567%2C120.758%2C504.003%2C129.665%5D%2C%5B108%2C109.799%2C395.172%2C118.706%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FU9LPNVZF%22%5D%2C%22locator%22%3A%227%22%7D%7D\">“general conclusion is that adversarial examples tend to stay hard even for models trained with different hyperparameters”</span></p>\n<p>似乎auto encoder机制的神经网络表现的好一些</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FRR9WPHI8%22%2C%22annotationKey%22%3A%22TJKTUT25%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%227%22%2C%22position%22%3A%7B%22pageIndex%22%3A6%2C%22rects%22%3A%5B%5B455.316%2C109.799%2C504.003%2C118.706%5D%2C%5B108%2C98.841%2C458.424%2C107.748%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FU9LPNVZF%22%5D%2C%22locator%22%3A%227%22%7D%7D\">“autoencoder based version seems most resilient to adversarial examples, it is not fully immune either”</span></p>\n<p></p>\n<p><span style=\"background-color: #5fb23680\">然后，作者讨论了不同数据集上的问题</span></p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FRR9WPHI8%22%2C%22annotationKey%22%3A%22RH4N2I32%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%228%22%2C%22position%22%3A%7B%22pageIndex%22%3A7%2C%22rects%22%3A%5B%5B390.49%2C326.774%2C504.003%2C335.681%5D%2C%5B108%2C315.815%2C504.003%2C324.722%5D%2C%5B108%2C304.856%2C276.009%2C313.763%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FU9LPNVZF%22%5D%2C%22locator%22%3A%228%22%7D%7D\">“The intriguing conclusion is that the adversarial examples remain hard for models trained even on a disjoint training set, although their effectiveness decreases considerably.”</span></p>\n</div>",
          "tags": [],
          "relations": {},
          "dateAdded": "2023-03-07T10:47:38Z",
          "dateModified": "2023-03-08T08:06:15Z",
          "uri": "http://zotero.org/groups/4922950/items/Q7GUQH6G"
        }
      ],
      "citationKey": "szegedyIntriguingPropertiesNeural2014",
      "itemKey": "U9LPNVZF",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/U9LPNVZF"
    },
    {
      "key": "5GU8IVYT",
      "version": 1193,
      "itemType": "webpage",
      "title": "Diversity can be Transferred: Output Diversification for White- and Black-box Attacks",
      "abstractNote": "Adversarial attacks often involve random perturbations of the inputs drawn from uniform or Gaussian distributions, e.g., to initialize optimization-based white-box attacks or generate update directions in black-box attacks. These simple perturbations, however, could be sub-optimal as they are agnostic to the model being attacked. To improve the efficiency of these attacks, we propose Output Diversified Sampling (ODS), a novel sampling strategy that attempts to maximize diversity in the target model's outputs among the generated samples. While ODS is a gradient-based strategy, the diversity offered by ODS is transferable and can be helpful for both white-box and black-box attacks via surrogate models. Empirically, we demonstrate that ODS significantly improves the performance of existing white-box and black-box attacks. In particular, ODS reduces the number of queries needed for state-of-the-art black-box attacks on ImageNet by a factor of two.",
      "date": "2020/03/15",
      "language": "en",
      "shortTitle": "Diversity can be Transferred",
      "url": "https://arxiv.org/abs/2003.06878v3",
      "accessDate": "2023-04-10T09:01:36Z",
      "websiteTitle": "arXiv.org",
      "creators": [
        {
          "firstName": "Yusuke",
          "lastName": "Tashiro",
          "creatorType": "author"
        },
        {
          "firstName": "Yang",
          "lastName": "Song",
          "creatorType": "author"
        },
        {
          "firstName": "Stefano",
          "lastName": "Ermon",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": [],
      "dateAdded": "2023-04-10T09:01:36Z",
      "dateModified": "2023-04-10T09:01:41Z",
      "uri": "http://zotero.org/groups/4922950/items/5GU8IVYT",
      "itemID": 3062,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "全文",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-04-10T09:38:39Z",
          "dateModified": "2023-04-10T09:38:39Z",
          "uri": "http://zotero.org/groups/4922950/items/BDQVAUCC",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\BDQVAUCC\\Tashiro 等 - 2020 - Diversity can be Transferred Output Diversificati.pdf",
          "select": "zotero://select/groups/4922950/items/BDQVAUCC"
        }
      ],
      "notes": [],
      "citationKey": "tashiroDiversityCanBe2020",
      "itemKey": "5GU8IVYT",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/5GU8IVYT"
    },
    {
      "key": "3HR3XXMR",
      "version": 1758,
      "itemType": "conferencePaper",
      "title": "Embedding Watermarks into Deep Neural Networks",
      "abstractNote": "Deep neural networks have recently achieved significant progress. Sharing trained models of these deep neural networks is very important in the rapid progress of researching or developing deep neural network systems. At the same time, it is necessary to protect the rights of shared trained models. To this end, we propose to use a digital watermarking technology to protect intellectual property or detect intellectual property infringement of trained models. Firstly, we formulate a new problem: embedding watermarks into deep neural networks. We also define requirements, embedding situations, and attack types for watermarking to deep neural networks. Secondly, we propose a general framework to embed a watermark into model parameters using a parameter regularizer. Our approach does not hurt the performance of networks into which a watermark is embedded. Finally, we perform comprehensive experiments to reveal the potential of watermarking to deep neural networks as a basis of this new problem. We show that our framework can embed a watermark in the situations of training a network from scratch, fine-tuning, and distilling without hurting the performance of a deep neural network. The embedded watermark does not disappear even after fine-tuning or parameter pruning; the watermark completely remains even after removing 65% of parameters were pruned. The implementation of this research is: https://github.com/yu4u/dnn-watermark",
      "date": "2017-06-06",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/1701.04082",
      "accessDate": "2023-04-19T15:01:58Z",
      "extra": "arXiv:1701.04082 [cs]",
      "pages": "269-277",
      "proceedingsTitle": "Proceedings of the 2017 ACM on International Conference on Multimedia Retrieval",
      "DOI": "10.1145/3078971.3078974",
      "creators": [
        {
          "firstName": "Yusuke",
          "lastName": "Uchida",
          "creatorType": "author"
        },
        {
          "firstName": "Yuki",
          "lastName": "Nagai",
          "creatorType": "author"
        },
        {
          "firstName": "Shigeyuki",
          "lastName": "Sakazawa",
          "creatorType": "author"
        },
        {
          "firstName": "Shin'ichi",
          "lastName": "Satoh",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Computer Vision and Pattern Recognition",
          "type": 1
        }
      ],
      "relations": [],
      "dateAdded": "2023-04-19T15:01:58Z",
      "dateModified": "2023-04-19T15:01:58Z",
      "uri": "http://zotero.org/groups/4922950/items/3HR3XXMR",
      "itemID": 3566,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "arXiv Fulltext PDF",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-04-19T15:02:41Z",
          "dateModified": "2023-04-19T15:02:41Z",
          "uri": "http://zotero.org/groups/4922950/items/9URSJM3Y",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\9URSJM3Y\\Uchida 等 - 2017 - Embedding Watermarks into Deep Neural Networks.pdf",
          "select": "zotero://select/groups/4922950/items/9URSJM3Y"
        },
        {
          "itemType": "attachment",
          "title": "arXiv.org Snapshot",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-04-19T15:02:53Z",
          "dateModified": "2023-04-19T15:02:53Z",
          "uri": "http://zotero.org/groups/4922950/items/9DXQ6VL9",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\9DXQ6VL9\\1701.html",
          "select": "zotero://select/groups/4922950/items/9DXQ6VL9"
        }
      ],
      "notes": [],
      "citationKey": "uchidaEmbeddingWatermarksDeep2017",
      "itemKey": "3HR3XXMR",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/3HR3XXMR"
    },
    {
      "key": "ZDUSPMSB",
      "version": 921,
      "itemType": "conferencePaper",
      "title": "PRNet: A Progressive Recovery Network for Revealing Perceptually Encrypted Images",
      "abstractNote": "Perceptual encryption is an efficient way of protecting image content by only selectively encrypting a portion of significant data in plain images. Existing security analysis of perceptual encryption usually resorts to traditional cryptanalysis techniques, which require heavy manual work and strict prior knowledge of encryption schemes. In this paper, we introduce a new end-to-end method of analyzing the visual security of perceptually encrypted images, without any manual work or knowing any prior knowledge of the encryption scheme. Specifically, by leveraging convolutional neural networks (CNNs), we propose a progressive recovery network (PRNet) to recover visual content from perceptually encrypted images. Our PRNet is stacked with several dense attention recovery blocks (DARBs), where each DARB contains two branches: feature extraction branch and image recovery branch. These two branches cooperate to rehabilitate more detailed visual information and generate efficient feature representation via densely connected structure and dual-saliency mechanism. We conduct extensive experiments to demonstrate that PRNet works on different perceptual encryption schemes with different settings, and the results show that PRNet significantly outperforms the state-of-the-art CNN-based image restoration methods.",
      "date": "十月 17, 2021",
      "shortTitle": "PRNet",
      "libraryCatalog": "ACM Digital Library",
      "url": "https://dl.acm.org/doi/10.1145/3474085.3475517",
      "accessDate": "2023-04-01",
      "place": "New York, NY, USA",
      "publisher": "Association for Computing Machinery",
      "ISBN": "978-1-4503-8651-7",
      "pages": "3537–3545",
      "series": "MM '21",
      "proceedingsTitle": "Proceedings of the 29th ACM International Conference on Multimedia",
      "DOI": "10.1145/3474085.3475517",
      "creators": [
        {
          "firstName": "Tao",
          "lastName": "Xiang",
          "creatorType": "author"
        },
        {
          "firstName": "Ying",
          "lastName": "Yang",
          "creatorType": "author"
        },
        {
          "firstName": "Shangwei",
          "lastName": "Guo",
          "creatorType": "author"
        },
        {
          "firstName": "Hangcheng",
          "lastName": "Liu",
          "creatorType": "author"
        },
        {
          "firstName": "Hantao",
          "lastName": "Liu",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "dual-saliency mechanism",
          "type": 1
        },
        {
          "tag": "image restoration",
          "type": 1
        },
        {
          "tag": "perceptual image encryption",
          "type": 1
        },
        {
          "tag": "visual security",
          "type": 1
        }
      ],
      "relations": [],
      "dateAdded": "2023-04-01T23:51:31Z",
      "dateModified": "2023-04-01T23:51:31Z",
      "uri": "http://zotero.org/groups/4922950/items/ZDUSPMSB",
      "itemID": 2867,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "Full Text PDF",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-04-01T23:51:34Z",
          "dateModified": "2023-04-01T23:51:34Z",
          "uri": "http://zotero.org/groups/4922950/items/E4E5MF8V",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\E4E5MF8V\\Xiang 等 - 2021 - PRNet A Progressive Recovery Network for Revealin.pdf",
          "select": "zotero://select/groups/4922950/items/E4E5MF8V"
        }
      ],
      "notes": [],
      "citationKey": "xiangPRNetProgressiveRecovery2021",
      "itemKey": "ZDUSPMSB",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/ZDUSPMSB"
    },
    {
      "key": "XJXU7WWA",
      "version": 254,
      "itemType": "conferencePaper",
      "title": "Text's Armor: Optimized Local Adversarial Perturbation Against Scene Text Editing Attacks",
      "abstractNote": "Deep neural networks (DNNs) have shown their powerful capability in scene text editing (STE). With carefully designed DNNs, one can alter texts in a source image with other ones while maintaining their realistic look. However, such editing tools provide a great convenience for criminals to falsify documents or modify texts without authorization. In this paper, we propose to actively defeat text editing attacks by designing invisible “armors” for texts in the scene. We turn the adversarial vulnerability of DNN-based STE into strength and design local perturbations (i.e., “armors”) specifically for texts using an optimized normalization strategy. Such local perturbations can effectively mislead STE attacks without affecting the perceptibility of scene background. To strengthen our defense capabilities, we systemically analyze and model STE attacks and provide a precise defense method to defeat attacks on different editing stages. We conduct both subjective and objective experiments to show the superior of our optimized local adversarial perturbation against state-of-the-art STE attacks. We also evaluate the portrait and landscape transferability of our perturbations.",
      "date": "2022-10-10",
      "language": "en",
      "shortTitle": "Text's Armor",
      "libraryCatalog": "DOI.org (Crossref)",
      "url": "https://dl.acm.org/doi/10.1145/3503161.3548103",
      "accessDate": "2023-03-02T12:27:23Z",
      "place": "Lisboa Portugal",
      "publisher": "ACM",
      "ISBN": "978-1-4503-9203-7",
      "pages": "2777-2785",
      "proceedingsTitle": "Proceedings of the 30th ACM International Conference on Multimedia",
      "conferenceName": "MM '22: The 30th ACM International Conference on Multimedia",
      "DOI": "10.1145/3503161.3548103",
      "creators": [
        {
          "firstName": "Tao",
          "lastName": "Xiang",
          "creatorType": "author"
        },
        {
          "firstName": "Hangcheng",
          "lastName": "Liu",
          "creatorType": "author"
        },
        {
          "firstName": "Shangwei",
          "lastName": "Guo",
          "creatorType": "author"
        },
        {
          "firstName": "Hantao",
          "lastName": "Liu",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": [],
      "dateAdded": "2023-03-02T12:28:24Z",
      "dateModified": "2023-03-07T08:42:54Z",
      "uri": "http://zotero.org/groups/4922950/items/XJXU7WWA",
      "itemID": 2232,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "Xiang 等 - 2022 - Text's Armor Optimized Local Adversarial Perturba.pdf",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-03-02T12:28:24Z",
          "dateModified": "2023-03-02T12:28:24Z",
          "uri": "http://zotero.org/groups/4922950/items/CBQKYRMV",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\CBQKYRMV\\Xiang 等 - 2022 - Text's Armor Optimized Local Adversarial Perturba.pdf",
          "select": "zotero://select/groups/4922950/items/CBQKYRMV"
        }
      ],
      "notes": [
        {
          "key": "774LNJ5Z",
          "version": 283,
          "itemType": "note",
          "parentItem": "XJXU7WWA",
          "note": "<div data-citation-items=\"%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FXJXU7WWA%22%5D%2C%22itemData%22%3A%7B%22id%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FXJXU7WWA%22%2C%22type%22%3A%22paper-conference%22%2C%22abstract%22%3A%22Deep%20neural%20networks%20(DNNs)%20have%20shown%20their%20powerful%20capability%20in%20scene%20text%20editing%20(STE).%20With%20carefully%20designed%20DNNs%2C%20one%20can%20alter%20texts%20in%20a%20source%20image%20with%20other%20ones%20while%20maintaining%20their%20realistic%20look.%20However%2C%20such%20editing%20tools%20provide%20a%20great%20convenience%20for%20criminals%20to%20falsify%20documents%20or%20modify%20texts%20without%20authorization.%20In%20this%20paper%2C%20we%20propose%20to%20actively%20defeat%20text%20editing%20attacks%20by%20designing%20invisible%20%E2%80%9Carmors%E2%80%9D%20for%20texts%20in%20the%20scene.%20We%20turn%20the%20adversarial%20vulnerability%20of%20DNN-based%20STE%20into%20strength%20and%20design%20local%20perturbations%20(i.e.%2C%20%E2%80%9Carmors%E2%80%9D)%20specifically%20for%20texts%20using%20an%20optimized%20normalization%20strategy.%20Such%20local%20perturbations%20can%20effectively%20mislead%20STE%20attacks%20without%20affecting%20the%20perceptibility%20of%20scene%20background.%20To%20strengthen%20our%20defense%20capabilities%2C%20we%20systemically%20analyze%20and%20model%20STE%20attacks%20and%20provide%20a%20precise%20defense%20method%20to%20defeat%20attacks%20on%20different%20editing%20stages.%20We%20conduct%20both%20subjective%20and%20objective%20experiments%20to%20show%20the%20superior%20of%20our%20optimized%20local%20adversarial%20perturbation%20against%20state-of-the-art%20STE%20attacks.%20We%20also%20evaluate%20the%20portrait%20and%20landscape%20transferability%20of%20our%20perturbations.%22%2C%22container-title%22%3A%22Proceedings%20of%20the%2030th%20ACM%20International%20Conference%20on%20Multimedia%22%2C%22DOI%22%3A%2210.1145%2F3503161.3548103%22%2C%22event-place%22%3A%22Lisboa%20Portugal%22%2C%22event-title%22%3A%22MM%20'22%3A%20The%2030th%20ACM%20International%20Conference%20on%20Multimedia%22%2C%22ISBN%22%3A%22978-1-4503-9203-7%22%2C%22language%22%3A%22en%22%2C%22page%22%3A%222777-2785%22%2C%22publisher%22%3A%22ACM%22%2C%22publisher-place%22%3A%22Lisboa%20Portugal%22%2C%22source%22%3A%22DOI.org%20(Crossref)%22%2C%22title%22%3A%22Text's%20Armor%3A%20Optimized%20Local%20Adversarial%20Perturbation%20Against%20Scene%20Text%20Editing%20Attacks%22%2C%22title-short%22%3A%22Text's%20Armor%22%2C%22URL%22%3A%22https%3A%2F%2Fdl.acm.org%2Fdoi%2F10.1145%2F3503161.3548103%22%2C%22author%22%3A%5B%7B%22family%22%3A%22Xiang%22%2C%22given%22%3A%22Tao%22%7D%2C%7B%22family%22%3A%22Liu%22%2C%22given%22%3A%22Hangcheng%22%7D%2C%7B%22family%22%3A%22Guo%22%2C%22given%22%3A%22Shangwei%22%7D%2C%7B%22family%22%3A%22Liu%22%2C%22given%22%3A%22Hantao%22%7D%5D%2C%22accessed%22%3A%7B%22date-parts%22%3A%5B%5B%222023%22%2C3%2C2%5D%5D%7D%2C%22issued%22%3A%7B%22date-parts%22%3A%5B%5B%222022%22%2C10%2C10%5D%5D%7D%7D%7D%5D\" data-schema-version=\"9\"><h1>1.Abstract</h1>\n<p>深度神经网络可以运用于背景文字编辑STE，而有的时候不希望这种功能对图片进行编辑</p>\n<p><span style=\"background-color: #2ea8e580\">看是那种类型的STE，其实和普通的视觉模型有什么区别吗</span></p>\n<p>本文的核心思想是，通过加入不可见的像素（就像实现恶意攻击的方式一样）然后来实现对图片的保护</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FCBQKYRMV%22%2C%22annotationKey%22%3A%22DW2CG9U4%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%222777%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B241.213%2C469.335%2C294.035%2C473.424%5D%2C%5B53.798%2C458.376%2C294.036%2C462.465%5D%2C%5B53.798%2C447.417%2C89.565%2C451.506%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FXJXU7WWA%22%5D%2C%22locator%22%3A%222777%22%7D%7D\">“actively defeat text editing attacks by designing invisible “armors” for texts in the scene”</span></p>\n<p>采用的是局部的扰动变换</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FCBQKYRMV%22%2C%22annotationKey%22%3A%22V8B5B853%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%222777%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B139.773%2C436.458%2C294.576%2C440.547%5D%2C%5B53.798%2C425.499%2C295.417%2C429.588%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FXJXU7WWA%22%5D%2C%22locator%22%3A%222777%22%7D%7D\">“design local perturbations (i.e., “armors”) specifically for texts using an optimized normalization strategy.”</span></p>\n<p><span style=\"background-color: #2ea8e580\">比起通过改动样本来让模型识别错误，这种方式的设计其实本质是一模一样的，但是是从安全的角度出发，防止机器学习模型的恶意，这种思考方式也很有参考价值</span></p>\n<p>同时，这种保护机制的限制在于要让人看不出来</p>\n<p><span style=\"background-color: #2ea8e580\">这种博弈模式让人想到了广告的例子，通过加扰动，让广告内容不被隐藏，或者让算法错误的把其他东西隐藏</span></p>\n<p></p>\n<p>对各种STE算法都能处理</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FCBQKYRMV%22%2C%22annotationKey%22%3A%22W75JFFI6%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%222777%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B231.985%2C381.664%2C294.037%2C385.753%5D%2C%5B53.798%2C370.705%2C137.597%2C374.794%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FXJXU7WWA%22%5D%2C%22locator%22%3A%222777%22%7D%7D\">“defeat attacks on different editing stages”</span></p>\n<p>这应该是攻击（虽然说是保护式的设计，但本质是攻击STE模型）的一个很重要的点，就是对这个领域的各种算法都能够实现攻击</p>\n<p></p>\n<p></p>\n<h1>2.Intro</h1>\n<p><span style=\"background-color: #5fb23680\">先简单介绍了一下STE，以及安全性工作的意义</span></p>\n<p><img alt=\"\" data-attachment-key=\"44V99RPI\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FCBQKYRMV%22%2C%22annotationKey%22%3A%22RAPG8P69%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%222777%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B314.559%2C345.088%2C566.029%2C533.029%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FXJXU7WWA%22%5D%2C%22locator%22%3A%222777%22%7D%7D\" width=\"419\" height=\"313\"></p>\n<p><span style=\"background-color: #5fb23680\">然后将前人工作的问题写了一下</span></p>\n<p>之前方法主要是消极的被动的检测图片是否被修改过，这并不能直接阻止修改，一般的用户也看不出来。而且目前大多检测方案都是对某种特定的模式，对于未知的攻击很难起到效果（<span style=\"background-color: #ff666680\">未知是指新方法，还是不确定的方法</span>）</p>\n<hr>\n<p>对于主动的对STE进行恶意攻击，现存方法的问题在于</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FCBQKYRMV%22%2C%22annotationKey%22%3A%22LJAXQTWA%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%222777%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B481.991%2C173.444%2C558.426%2C177.533%5D%2C%5B317.955%2C162.485%2C558.193%2C166.574%5D%2C%5B317.955%2C151.527%2C343.884%2C155.616%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FXJXU7WWA%22%5D%2C%22locator%22%3A%222777%22%7D%7D\">“straightforward way to generate adversarial perturbations is using existing methods such as”</span></p>\n<p>1.这些方式对于图片的改变太大了，影响使用（约束出问题）而且原来是全局的改动，这篇文章是局部的改动（<span style=\"background-color: #2ea8e580\">这有这么重要吗？</span>）</p>\n<p>2.STE是一个多步骤的复杂的系统，而恶意扰动对这种机制的攻击还没有被探索</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FCBQKYRMV%22%2C%22annotationKey%22%3A%2266TTNGND%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%222777%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B359.575%2C96.732%2C558.193%2C100.821%5D%2C%5B317.955%2C85.773%2C415.261%2C89.862%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FXJXU7WWA%22%5D%2C%22locator%22%3A%222777%22%7D%7D\">“breaking STE attacks using adversarial perturbations has not been explored yet”</span></p>\n<p><span style=\"background-color: #5fb23680\">然后基于这些问题提出自己的工作</span></p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FCBQKYRMV%22%2C%22annotationKey%22%3A%22DRZA3KS3%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%222778%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B250.981%2C643.539%2C294.035%2C647.628%5D%2C%5B53.798%2C632.58%2C295.548%2C636.669%5D%2C%5B53.798%2C621.621%2C163.324%2C625.71%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FXJXU7WWA%22%5D%2C%22locator%22%3A%222778%22%7D%7D\">“design local adversarial perturbations as texts’ armors without affecting the visual quality of the background”</span></p>\n<p>然后提出了这个工作的关键核心</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FCBQKYRMV%22%2C%22annotationKey%22%3A%22VZJYXN4N%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%222778%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B195.975%2C621.621%2C294.035%2C625.71%5D%2C%5B53.798%2C610.662%2C294.035%2C614.751%5D%2C%5B53.798%2C599.703%2C294.035%2C603.792%5D%2C%5B53.798%2C588.744%2C183.186%2C592.833%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FXJXU7WWA%22%5D%2C%22locator%22%3A%222778%22%7D%7D\">“defense performance of the local armors may be unsatisfactory when using the sign function to determine the update direction because it sets all partial derivatives in the gradient to {-1, 0, 1} roughly”</span></p>\n<p>梯度更新的方法，通过快速的梯度更新，局部的扰动可能并不能起到很好的效果</p>\n<p>所以说新提出了一种恶意攻击中样本扰动的梯度更新方法，这种方法既可以达到比较好的效果，而且效率也不错。最终还通过理论证明了这种梯度更新方式跟接近真正的梯度</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FCBQKYRMV%22%2C%22annotationKey%22%3A%22NQEVBE9Q%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%222778%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B209.515%2C566.826%2C295.545%2C570.915%5D%2C%5B53.798%2C555.867%2C199.288%2C559.956%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FXJXU7WWA%22%5D%2C%22locator%22%3A%222778%22%7D%7D\">“novel gradient normalization strategy to reduce such deviation”</span></p>\n<p></p>\n<p>然后根据STE的性质，分析了四个阶段</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FCBQKYRMV%22%2C%22annotationKey%22%3A%225W67ZLHI%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%222778%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B234.805%2C490.114%2C294.033%2C494.203%5D%2C%5B53.798%2C479.155%2C294.263%2C483.244%5D%2C%5B53.798%2C468.196%2C76.056%2C472.285%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FXJXU7WWA%22%5D%2C%22locator%22%3A%222778%22%7D%7D\">“analyze existing STE attacks and model the text editing process with four necessary stages”</span></p>\n<p></p>\n<hr>\n<p>最终效果很好</p>\n<p></p>\n<h1>3.Related work</h1>\n<p>简单介绍了一下背景，没啥意思</p>\n<p></p>\n<h1>4.Problem Statement</h1>\n<p>建立了一个模型，核心观点还是老一套，不能改变太多，以及能够让STE出错</p>\n<hr>\n<p>接着重新强调了一下工作的创新点，并且额外强调了一下之前的adversarial perturbation不能够很好的处理STE的复杂机制<span style=\"background-color: #2ea8e580\">（之前的做法效果到底有多差，而且对于复杂的STE系统，难道不是鲁棒性更差，更加容易出现问题吗）</span></p>\n<p></p>\n<h1>5.Methodology</h1>\n<p>承接上文，继续介绍工作，核心不同在于两点，一是局部的扰动，二是梯度算法</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FCBQKYRMV%22%2C%22annotationKey%22%3A%22KGN53TIM%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%222779%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B128.004%2C159.267%2C294.031%2C167.605%5D%2C%5B53.798%2C150.388%2C156.79%2C154.477%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FXJXU7WWA%22%5D%2C%22locator%22%3A%222779%22%7D%7D\">“design local perturbations specifically for the text areas of a source image”</span></p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FCBQKYRMV%22%2C%22annotationKey%22%3A%228V38X355%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%222779%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B141.876%2C104.472%2C295.553%2C112.81%5D%2C%5B53.798%2C95.593%2C259.908%2C99.682%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FXJXU7WWA%22%5D%2C%22locator%22%3A%222779%22%7D%7D\">“optimize the iteration process of perturbation generation to obtain more precise update directions”</span></p>\n<p></p>\n<hr>\n<p>接着讲到STE作为一个复杂系统，攻击他的取决于其最脆弱的地方，这篇文章也是通过这个思路来攻击的（<span style=\"background-color: #2ea8e580\">回答了上一个问题？</span>）</p>\n<hr>\n<p><img alt=\"\" data-attachment-key=\"NI8D99WF\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FCBQKYRMV%22%2C%22annotationKey%22%3A%22PK6NY6DF%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%222779%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B366.176%2C433.765%2C512.206%2C471.265%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FXJXU7WWA%22%5D%2C%22locator%22%3A%222779%22%7D%7D\" width=\"243\" height=\"62\"></p>\n<p>算法公式，意思是找到一个小的扰动<span class=\"math\">$\\eta$</span> &nbsp; 来最大化STE函数f下图片i的损失函数。需要注意的是这个损失函数是针对STE系统的多过程来设计的</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FCBQKYRMV%22%2C%22annotationKey%22%3A%22CNM2LRE6%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%222779%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B356.473%2C414.288%2C558.359%2C418.44%5D%2C%5B317.955%2C402%2C363.201%2C409.749%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FXJXU7WWA%22%5D%2C%22locator%22%3A%222779%22%7D%7D\">“denotes one of the four necessary editing stages or their combination”</span></p>\n<p>然后对于这个梯度的优化方法，又分析了一下以往梯度方法实现攻击的问题</p>\n<p><span class=\"highlight\" data-annotation=\"%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FCBQKYRMV%22%2C%22annotationKey%22%3A%22M2YGWR6F%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%222779%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B317.955%2C310.212%2C532.111%2C314.301%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fgroups%2F4922950%2Fitems%2FXJXU7WWA%22%5D%2C%22locator%22%3A%222779%22%7D%7D\">“However, these gradient-based methods have two problems”</span></p>\n<p></p>\n</div>",
          "tags": [],
          "relations": {},
          "dateAdded": "2023-03-07T01:09:59Z",
          "dateModified": "2023-03-07T09:27:30Z",
          "uri": "http://zotero.org/groups/4922950/items/774LNJ5Z"
        }
      ],
      "citationKey": "xiangTextArmorOptimized2022",
      "itemKey": "XJXU7WWA",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/XJXU7WWA"
    },
    {
      "key": "8WCISN7W",
      "version": 47,
      "itemType": "conferencePaper",
      "title": "Model-Agnostic Defense for Lane Detection against Adversarial Attack",
      "abstractNote": "Susceptibility of neural networks to adversarial attack prompts serious safety concerns for lane detection efforts, a domain where such models have been widely applied. Recent work on adversarial road patches have successfully induced perception of lane lines with arbitrary form, presenting an avenue for rogue control of vehicle behavior. In this paper, we propose a modular lane veriﬁcation system that can catch such threats before the autonomous driving system is misled while remaining agnostic to the particular lane detection model. Our experiments show that implementing the system with a simple convolutional neural network (CNN) can defend against a wide gamut of attacks on lane detection models. With a 10% impact to inference time, we can detect 96% of bounded non-adaptive attacks, 90% of bounded adaptive attacks, and 98% of patch attacks while preserving accurate identiﬁcation at least 95% of true lanes, indicating that our proposed veriﬁcation system is effective at mitigating lane detection security risks with minimal overhead.",
      "date": "2021",
      "language": "en",
      "libraryCatalog": "DOI.org (Crossref)",
      "url": "https://www.ndss-symposium.org/wp-content/uploads/autosec2021_23032_paper.pdf",
      "accessDate": "2023-01-24T11:29:00Z",
      "place": "Virtual",
      "publisher": "Internet Society",
      "ISBN": "978-1-891562-68-6",
      "proceedingsTitle": "Proceedings Third International Workshop on Automotive and Autonomous Vehicle Security",
      "conferenceName": "Third International Workshop on Automotive and Autonomous Vehicle Security",
      "DOI": "10.14722/autosec.2021.23032",
      "creators": [
        {
          "firstName": "Henry",
          "lastName": "Xu",
          "creatorType": "author"
        },
        {
          "firstName": "An",
          "lastName": "Ju",
          "creatorType": "author"
        },
        {
          "firstName": "David",
          "lastName": "Wagner",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": [],
      "dateAdded": "2023-01-24T11:29:00Z",
      "dateModified": "2023-01-24T11:29:00Z",
      "uri": "http://zotero.org/groups/4922950/items/8WCISN7W",
      "itemID": 1776,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "Xu 等 - 2021 - Model-Agnostic Defense for Lane Detection against .pdf",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-01-24T11:28:57Z",
          "dateModified": "2023-01-24T11:29:01Z",
          "uri": "http://zotero.org/groups/4922950/items/V5JSV6MJ",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\V5JSV6MJ\\Xu 等 - 2021 - Model-Agnostic Defense for Lane Detection against .pdf",
          "select": "zotero://select/groups/4922950/items/V5JSV6MJ"
        }
      ],
      "notes": [],
      "citationKey": "xuModelAgnosticDefenseLane2021",
      "itemKey": "8WCISN7W",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/8WCISN7W"
    },
    {
      "key": "VJFK3WLT",
      "version": 1480,
      "itemType": "preprint",
      "title": "Diffusion Models: A Comprehensive Survey of Methods and Applications",
      "abstractNote": "Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative models for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer vision, natural language generation, temporal data modeling, to interdisciplinary applications in other scientific disciplines. This survey aims to provide a contextualized, in-depth look at the state of diffusion models, identifying the key areas of focus and pointing to potential areas for further exploration. Github: https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy.",
      "date": "2023-03-23",
      "language": "en",
      "shortTitle": "Diffusion Models",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/2209.00796",
      "accessDate": "2023-04-14T08:34:04Z",
      "extra": "arXiv:2209.00796 [cs]",
      "repository": "arXiv",
      "archiveID": "arXiv:2209.00796",
      "creators": [
        {
          "firstName": "Ling",
          "lastName": "Yang",
          "creatorType": "author"
        },
        {
          "firstName": "Zhilong",
          "lastName": "Zhang",
          "creatorType": "author"
        },
        {
          "firstName": "Yang",
          "lastName": "Song",
          "creatorType": "author"
        },
        {
          "firstName": "Shenda",
          "lastName": "Hong",
          "creatorType": "author"
        },
        {
          "firstName": "Runsheng",
          "lastName": "Xu",
          "creatorType": "author"
        },
        {
          "firstName": "Yue",
          "lastName": "Zhao",
          "creatorType": "author"
        },
        {
          "firstName": "Wentao",
          "lastName": "Zhang",
          "creatorType": "author"
        },
        {
          "firstName": "Bin",
          "lastName": "Cui",
          "creatorType": "author"
        },
        {
          "firstName": "Ming-Hsuan",
          "lastName": "Yang",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Machine Learning",
          "type": 1
        },
        {
          "tag": "Computer Science - Artificial Intelligence",
          "type": 1
        },
        {
          "tag": "Computer Science - Computer Vision and Pattern Recognition",
          "type": 1
        }
      ],
      "relations": [],
      "dateAdded": "2023-04-14T08:34:04Z",
      "dateModified": "2023-04-14T08:34:04Z",
      "uri": "http://zotero.org/groups/4922950/items/VJFK3WLT",
      "itemID": 3315,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "Yang 等 - 2023 - Diffusion Models A Comprehensive Survey of Method.pdf",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-04-14T08:33:56Z",
          "dateModified": "2023-04-14T08:34:05Z",
          "uri": "http://zotero.org/groups/4922950/items/YG23RWZ8",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\YG23RWZ8\\2209.00796.pdf",
          "select": "zotero://select/groups/4922950/items/YG23RWZ8"
        }
      ],
      "notes": [
        {
          "key": "DIJFD7WQ",
          "version": 1480,
          "itemType": "note",
          "parentItem": "VJFK3WLT",
          "note": "Comment: 49 pages, 17 figures, citing 337 (up-to-date) papers, project: https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy",
          "tags": [],
          "relations": {},
          "dateAdded": "2023-04-14T08:34:04Z",
          "dateModified": "2023-04-14T08:34:04Z",
          "uri": "http://zotero.org/groups/4922950/items/DIJFD7WQ"
        }
      ],
      "citationKey": "yangDiffusionModelsComprehensive2023",
      "itemKey": "VJFK3WLT",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/VJFK3WLT"
    },
    {
      "key": "BBG25LJ6",
      "version": 112,
      "itemType": "preprint",
      "title": "Gradient Obfuscation Gives a False Sense of Security in Federated Learning",
      "abstractNote": "Federated learning has been proposed as a privacy-preserving machine learning framework that enables multiple clients to collaborate without sharing raw data. However, client privacy protection is not guaranteed by design in this framework. Prior work has shown that the gradient sharing strategies in federated learning can be vulnerable to data reconstruction attacks. In practice, though, clients may not transmit raw gradients considering the high communication cost or due to privacy enhancement requirements. Empirical studies have demonstrated that gradient obfuscation, including intentional obfuscation via gradient noise injection and unintentional obfuscation via gradient compression, can provide more privacy protection against reconstruction attacks. In this work, we present a new data reconstruction attack framework targeting the image classification task in federated learning. We show that commonly adopted gradient postprocessing procedures, such as gradient quantization, gradient sparsification, and gradient perturbation, may give a false sense of security in federated learning. Contrary to prior studies, we argue that privacy enhancement should not be treated as a byproduct of gradient compression. Additionally, we design a new method under the proposed framework to reconstruct the image at the semantic level. We quantify the semantic privacy leakage and compare with conventional based on image similarity scores. Our comparisons challenge the image data leakage evaluation schemes in the literature. The results emphasize the importance of revisiting and redesigning the privacy protection mechanisms for client data in existing federated learning algorithms.",
      "date": "2022-10-13",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/2206.04055",
      "accessDate": "2023-01-29T15:02:36Z",
      "extra": "arXiv:2206.04055 [cs]",
      "DOI": "10.48550/arXiv.2206.04055",
      "repository": "arXiv",
      "archiveID": "arXiv:2206.04055",
      "creators": [
        {
          "firstName": "Kai",
          "lastName": "Yue",
          "creatorType": "author"
        },
        {
          "firstName": "Richeng",
          "lastName": "Jin",
          "creatorType": "author"
        },
        {
          "firstName": "Chau-Wai",
          "lastName": "Wong",
          "creatorType": "author"
        },
        {
          "firstName": "Dror",
          "lastName": "Baron",
          "creatorType": "author"
        },
        {
          "firstName": "Huaiyu",
          "lastName": "Dai",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Machine Learning",
          "type": 1
        },
        {
          "tag": "Computer Science - Cryptography and Security",
          "type": 1
        },
        {
          "tag": "Computer Science - Artificial Intelligence",
          "type": 1
        },
        {
          "tag": "Computer Science - Distributed, Parallel, and Cluster Computing",
          "type": 1
        }
      ],
      "relations": [],
      "dateAdded": "2023-01-29T15:02:36Z",
      "dateModified": "2023-01-29T15:02:39Z",
      "uri": "http://zotero.org/groups/4922950/items/BBG25LJ6",
      "itemID": 1816,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "arXiv Fulltext PDF",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-01-29T15:07:35Z",
          "dateModified": "2023-01-29T15:07:35Z",
          "uri": "http://zotero.org/groups/4922950/items/6UIAHQCL",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\6UIAHQCL\\Yue 等 - 2022 - Gradient Obfuscation Gives a False Sense of Securi.pdf",
          "select": "zotero://select/groups/4922950/items/6UIAHQCL"
        },
        {
          "itemType": "attachment",
          "title": "arXiv.org Snapshot",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-01-29T15:07:43Z",
          "dateModified": "2023-01-29T15:07:43Z",
          "uri": "http://zotero.org/groups/4922950/items/3ZVHDZ56",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\3ZVHDZ56\\2206.html",
          "select": "zotero://select/groups/4922950/items/3ZVHDZ56"
        }
      ],
      "notes": [
        {
          "key": "EN76Z3KT",
          "version": 110,
          "itemType": "note",
          "parentItem": "BBG25LJ6",
          "note": "Comment: Accepted by USENIX Security 2023",
          "tags": [],
          "relations": {},
          "dateAdded": "2023-01-29T15:02:36Z",
          "dateModified": "2023-01-29T15:02:36Z",
          "uri": "http://zotero.org/groups/4922950/items/EN76Z3KT"
        }
      ],
      "citationKey": "yueGradientObfuscationGives2022",
      "itemKey": "BBG25LJ6",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/BBG25LJ6"
    },
    {
      "key": "SKJ7P3KF",
      "version": 1605,
      "itemType": "preprint",
      "title": "Responsible Disclosure of Generative Models Using Scalable Fingerprinting",
      "abstractNote": "Over the past years, deep generative models have achieved a new level of performance. Generated data has become difficult, if not impossible, to be distinguished from real data. While there are plenty of use cases that benefit from this technology, there are also strong concerns on how this new technology can be misused to generate deep fakes and enable misinformation at scale. Unfortunately, current deep fake detection methods are not sustainable, as the gap between real and fake continues to close. In contrast, our work enables a responsible disclosure of such state-of-the-art generative models, that allows model inventors to fingerprint their models, so that the generated samples containing a fingerprint can be accurately detected and attributed to a source. Our technique achieves this by an efficient and scalable ad-hoc generation of a large population of models with distinct fingerprints. Our recommended operation point uses a 128-bit fingerprint which in principle results in more than $10^{38}$ identifiable models. Experiments show that our method fulfills key properties of a fingerprinting mechanism and achieves effectiveness in deep fake detection and attribution. Code and models are available at https://github.com/ningyu1991/ScalableGANFingerprints .",
      "date": "2022-03-17",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/2012.08726",
      "accessDate": "2023-04-16T14:39:54Z",
      "extra": "arXiv:2012.08726 [cs]",
      "DOI": "10.48550/arXiv.2012.08726",
      "repository": "arXiv",
      "archiveID": "arXiv:2012.08726",
      "creators": [
        {
          "firstName": "Ning",
          "lastName": "Yu",
          "creatorType": "author"
        },
        {
          "firstName": "Vladislav",
          "lastName": "Skripniuk",
          "creatorType": "author"
        },
        {
          "firstName": "Dingfan",
          "lastName": "Chen",
          "creatorType": "author"
        },
        {
          "firstName": "Larry",
          "lastName": "Davis",
          "creatorType": "author"
        },
        {
          "firstName": "Mario",
          "lastName": "Fritz",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Machine Learning",
          "type": 1
        },
        {
          "tag": "Computer Science - Cryptography and Security",
          "type": 1
        },
        {
          "tag": "Computer Science - Computer Vision and Pattern Recognition",
          "type": 1
        },
        {
          "tag": "Computer Science - Computers and Society",
          "type": 1
        },
        {
          "tag": "Computer Science - Graphics",
          "type": 1
        }
      ],
      "relations": [],
      "dateAdded": "2023-04-16T14:39:54Z",
      "dateModified": "2023-04-16T14:39:54Z",
      "uri": "http://zotero.org/groups/4922950/items/SKJ7P3KF",
      "itemID": 3428,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "arXiv Fulltext PDF",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-04-16T14:40:13Z",
          "dateModified": "2023-04-16T14:40:13Z",
          "uri": "http://zotero.org/groups/4922950/items/AY2UZ7QP",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\AY2UZ7QP\\Yu 等 - 2022 - Responsible Disclosure of Generative Models Using .pdf",
          "select": "zotero://select/groups/4922950/items/AY2UZ7QP"
        },
        {
          "itemType": "attachment",
          "title": "arXiv.org Snapshot",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-04-16T14:40:23Z",
          "dateModified": "2023-04-16T14:40:23Z",
          "uri": "http://zotero.org/groups/4922950/items/RWAYWI2K",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\RWAYWI2K\\2012.html",
          "select": "zotero://select/groups/4922950/items/RWAYWI2K"
        }
      ],
      "notes": [
        {
          "key": "INI2SYWB",
          "version": 1605,
          "itemType": "note",
          "parentItem": "SKJ7P3KF",
          "note": "Comment: Accepted to ICLR'22 as Spotlight",
          "tags": [],
          "relations": {},
          "dateAdded": "2023-04-16T14:39:54Z",
          "dateModified": "2023-04-16T14:39:54Z",
          "uri": "http://zotero.org/groups/4922950/items/INI2SYWB"
        }
      ],
      "citationKey": "yuResponsibleDisclosureGenerative2022",
      "itemKey": "SKJ7P3KF",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/SKJ7P3KF"
    },
    {
      "key": "ETZAM2D6",
      "version": 138,
      "itemType": "preprint",
      "title": "Attacks Which Do Not Kill Training Make Adversarial Learning Stronger",
      "abstractNote": "Adversarial training based on the minimax formulation is necessary for obtaining adversarial robustness of trained models. However, it is conservative or even pessimistic so that it sometimes hurts the natural generalization. In this paper, we raise a fundamental question---do we have to trade off natural generalization for adversarial robustness? We argue that adversarial training is to employ confident adversarial data for updating the current model. We propose a novel approach of friendly adversarial training (FAT): rather than employing most adversarial data maximizing the loss, we search for least adversarial (i.e., friendly adversarial) data minimizing the loss, among the adversarial data that are confidently misclassified. Our novel formulation is easy to implement by just stopping the most adversarial data searching algorithms such as PGD (projected gradient descent) early, which we call early-stopped PGD. Theoretically, FAT is justified by an upper bound of the adversarial risk. Empirically, early-stopped PGD allows us to answer the earlier question negatively---adversarial robustness can indeed be achieved without compromising the natural generalization.",
      "date": "2020-09-05",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/2002.11242",
      "accessDate": "2023-01-31T11:36:29Z",
      "extra": "arXiv:2002.11242 [cs, stat]",
      "DOI": "10.48550/arXiv.2002.11242",
      "repository": "arXiv",
      "archiveID": "arXiv:2002.11242",
      "creators": [
        {
          "firstName": "Jingfeng",
          "lastName": "Zhang",
          "creatorType": "author"
        },
        {
          "firstName": "Xilie",
          "lastName": "Xu",
          "creatorType": "author"
        },
        {
          "firstName": "Bo",
          "lastName": "Han",
          "creatorType": "author"
        },
        {
          "firstName": "Gang",
          "lastName": "Niu",
          "creatorType": "author"
        },
        {
          "firstName": "Lizhen",
          "lastName": "Cui",
          "creatorType": "author"
        },
        {
          "firstName": "Masashi",
          "lastName": "Sugiyama",
          "creatorType": "author"
        },
        {
          "firstName": "Mohan",
          "lastName": "Kankanhalli",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Machine Learning",
          "type": 1
        },
        {
          "tag": "Statistics - Machine Learning",
          "type": 1
        }
      ],
      "relations": [],
      "dateAdded": "2023-01-31T11:36:29Z",
      "dateModified": "2023-01-31T11:36:30Z",
      "uri": "http://zotero.org/groups/4922950/items/ETZAM2D6",
      "itemID": 1835,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "arXiv Fulltext PDF",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-01-31T11:36:52Z",
          "dateModified": "2023-01-31T11:36:52Z",
          "uri": "http://zotero.org/groups/4922950/items/VUQWAA5J",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\VUQWAA5J\\Zhang 等 - 2020 - Attacks Which Do Not Kill Training Make Adversaria.pdf",
          "select": "zotero://select/groups/4922950/items/VUQWAA5J"
        },
        {
          "itemType": "attachment",
          "title": "arXiv.org Snapshot",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-01-31T11:37:03Z",
          "dateModified": "2023-01-31T11:37:03Z",
          "uri": "http://zotero.org/groups/4922950/items/Q5G5X7NG",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\Q5G5X7NG\\2002.html",
          "select": "zotero://select/groups/4922950/items/Q5G5X7NG"
        }
      ],
      "notes": [
        {
          "key": "EJJBYFXG",
          "version": 138,
          "itemType": "note",
          "parentItem": "ETZAM2D6",
          "note": "Comment: Thirty-seventh International Conference on Machine Learning (ICML 2020)",
          "tags": [],
          "relations": {},
          "dateAdded": "2023-01-31T11:36:29Z",
          "dateModified": "2023-01-31T11:36:29Z",
          "uri": "http://zotero.org/groups/4922950/items/EJJBYFXG"
        }
      ],
      "citationKey": "zhangAttacksWhichNot2020",
      "itemKey": "ETZAM2D6",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/ETZAM2D6"
    },
    {
      "key": "2FURDLUJ",
      "version": 34,
      "itemType": "conferencePaper",
      "title": "Clipped BagNet: Defending Against Sticker Attacks with Clipped Bag-of-features",
      "abstractNote": "Many works have demonstrated that neural networks are vulnerable to adversarial examples. We examine the adversarial sticker attack, where the attacker places a sticker somewhere on an image to induce it to be misclassiﬁed. We take a ﬁrst step towards defending against such attacks using clipped BagNet, which bounds the inﬂuence that any limitedsize sticker can have on the ﬁnal classiﬁcation. We evaluate our scheme on ImageNet and show that it provides strong security against targeted PGD attacks and gradient-free attacks, and yields certiﬁed security for a 95% of images against a targeted 20 × 20 pixel attack.",
      "date": "5/2020",
      "language": "en",
      "shortTitle": "Clipped BagNet",
      "libraryCatalog": "DOI.org (Crossref)",
      "url": "https://ieeexplore.ieee.org/document/9283860/",
      "accessDate": "2023-01-24T11:03:34Z",
      "place": "San Francisco, CA, USA",
      "publisher": "IEEE",
      "ISBN": "978-1-72819-346-5",
      "pages": "55-61",
      "proceedingsTitle": "2020 IEEE Security and Privacy Workshops (SPW)",
      "conferenceName": "2020 IEEE Security and Privacy Workshops (SPW)",
      "DOI": "10.1109/SPW50608.2020.00026",
      "creators": [
        {
          "firstName": "Zhanyuan",
          "lastName": "Zhang",
          "creatorType": "author"
        },
        {
          "firstName": "Benson",
          "lastName": "Yuan",
          "creatorType": "author"
        },
        {
          "firstName": "Michael",
          "lastName": "McCoyd",
          "creatorType": "author"
        },
        {
          "firstName": "David",
          "lastName": "Wagner",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": [],
      "dateAdded": "2023-01-24T11:03:34Z",
      "dateModified": "2023-01-24T11:03:34Z",
      "uri": "http://zotero.org/groups/4922950/items/2FURDLUJ",
      "itemID": 1768,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "bagnet-dls20.pdf",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-01-24T11:03:29Z",
          "dateModified": "2023-01-24T11:03:34Z",
          "uri": "http://zotero.org/groups/4922950/items/JRTL62IV",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\JRTL62IV\\bagnet-dls20.pdf",
          "select": "zotero://select/groups/4922950/items/JRTL62IV"
        }
      ],
      "notes": [],
      "citationKey": "zhangClippedBagNetDefending2020",
      "itemKey": "2FURDLUJ",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/2FURDLUJ"
    },
    {
      "key": "4WHREF2H",
      "version": 856,
      "itemType": "preprint",
      "title": "Self-Attention Generative Adversarial Networks",
      "abstractNote": "In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other. Furthermore, recent work has shown that generator conditioning affects GAN performance. Leveraging this insight, we apply spectral normalization to the GAN generator and find that this improves training dynamics. The proposed SAGAN achieves the state-of-the-art results, boosting the best published Inception score from 36.8 to 52.52 and reducing Frechet Inception distance from 27.62 to 18.65 on the challenging ImageNet dataset. Visualization of the attention layers shows that the generator leverages neighborhoods that correspond to object shapes rather than local regions of fixed shape.",
      "date": "2019-06-14",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/1805.08318",
      "accessDate": "2023-03-29T02:06:51Z",
      "extra": "arXiv:1805.08318 [cs, stat]",
      "DOI": "10.48550/arXiv.1805.08318",
      "repository": "arXiv",
      "archiveID": "arXiv:1805.08318",
      "creators": [
        {
          "firstName": "Han",
          "lastName": "Zhang",
          "creatorType": "author"
        },
        {
          "firstName": "Ian",
          "lastName": "Goodfellow",
          "creatorType": "author"
        },
        {
          "firstName": "Dimitris",
          "lastName": "Metaxas",
          "creatorType": "author"
        },
        {
          "firstName": "Augustus",
          "lastName": "Odena",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Machine Learning",
          "type": 1
        },
        {
          "tag": "Statistics - Machine Learning",
          "type": 1
        }
      ],
      "relations": [],
      "dateAdded": "2023-03-29T02:06:51Z",
      "dateModified": "2023-03-29T02:06:51Z",
      "uri": "http://zotero.org/groups/4922950/items/4WHREF2H",
      "itemID": 2471,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "arXiv Fulltext PDF",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-03-29T02:07:00Z",
          "dateModified": "2023-03-29T02:07:00Z",
          "uri": "http://zotero.org/groups/4922950/items/6URKYX62",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\6URKYX62\\Zhang 等 - 2019 - Self-Attention Generative Adversarial Networks.pdf",
          "select": "zotero://select/groups/4922950/items/6URKYX62"
        },
        {
          "itemType": "attachment",
          "title": "arXiv.org Snapshot",
          "tags": [],
          "relations": [],
          "dateAdded": "2023-03-29T02:07:13Z",
          "dateModified": "2023-03-29T02:07:13Z",
          "uri": "http://zotero.org/groups/4922950/items/PWXNIY2U",
          "path": "C:\\Users\\AKK87\\Zotero\\storage\\PWXNIY2U\\1805.html",
          "select": "zotero://select/groups/4922950/items/PWXNIY2U"
        }
      ],
      "notes": [],
      "citationKey": "zhangSelfAttentionGenerativeAdversarial2019",
      "itemKey": "4WHREF2H",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/4WHREF2H"
    },
    {
      "key": "6LWCWHUV",
      "version": 230,
      "itemType": "conferencePaper",
      "title": "Adversarial Attacks on Neural Networks for Graph Data",
      "abstractNote": "Deep learning models for graphs have achieved strong performance for the task of node classification. Despite their proliferation, currently there is no study of their robustness to adversarial attacks. Yet, in domains where they are likely to be used, e.g. the web, adversaries are common. Can deep learning models for graphs be easily fooled? In this work, we introduce the first study of adversarial attacks on attributed graphs, specifically focusing on models exploiting ideas of graph convolutions. In addition to attacks at test time, we tackle the more challenging class of poisoning/causative attacks, which focus on the training phase of a machine learning model. We generate adversarial perturbations targeting the node's features and the graph structure, thus, taking the dependencies between instances in account. Moreover, we ensure that the perturbations remain unnoticeable by preserving important data characteristics. To cope with the underlying discrete domain we propose an efficient algorithm Nettack exploiting incremental computations. Our experimental study shows that accuracy of node classification significantly drops even when performing only few perturbations. Even more, our attacks are transferable: the learned attacks generalize to other state-of-the-art node classification models and unsupervised approaches, and likewise are successful even when only limited knowledge about the graph is given.",
      "date": "2018-07-19",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/1805.07984",
      "accessDate": "2023-03-04T05:41:19Z",
      "extra": "arXiv:1805.07984 [cs, stat]",
      "pages": "2847-2856",
      "proceedingsTitle": "Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining",
      "DOI": "10.1145/3219819.3220078",
      "creators": [
        {
          "firstName": "Daniel",
          "lastName": "Zügner",
          "creatorType": "author"
        },
        {
          "firstName": "Amir",
          "lastName": "Akbarnejad",
          "creatorType": "author"
        },
        {
          "firstName": "Stephan",
          "lastName": "Günnemann",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Machine Learning",
          "type": 1
        },
        {
          "tag": "Statistics - Machine Learning",
          "type": 1
        },
        {
          "tag": "Computer Science - Cryptography and Security",
          "type": 1
        }
      ],
      "relations": [],
      "dateAdded": "2023-03-04T05:41:20Z",
      "dateModified": "2023-03-04T05:41:20Z",
      "uri": "http://zotero.org/groups/4922950/items/6LWCWHUV",
      "itemID": 2267,
      "attachments": [],
      "notes": [
        {
          "key": "6IMF56WH",
          "version": 230,
          "itemType": "note",
          "parentItem": "6LWCWHUV",
          "note": "Comment: Accepted as a full paper at KDD 2018 on May 6, 2018",
          "tags": [],
          "relations": {},
          "dateAdded": "2023-03-04T05:41:20Z",
          "dateModified": "2023-03-04T05:41:20Z",
          "uri": "http://zotero.org/groups/4922950/items/6IMF56WH"
        }
      ],
      "citationKey": "zugnerAdversarialAttacksNeural2018",
      "itemKey": "6LWCWHUV",
      "libraryID": 4,
      "select": "zotero://select/groups/4922950/items/6LWCWHUV"
    }
  ]
}